StartDate,EndDate,Status,IPAddress,Progress,Duration (in seconds),Finished,RecordedDate,ResponseId,RecipientLastName,RecipientFirstName,RecipientEmail,ExternalReference,LocationLatitude,LocationLongitude,DistributionChannel,UserLanguage,Q2,Q3,Q5_2,Q5_3,Q5_4,Q6,Q8,Q9,Q10,Q11,Q12,Q13,Q14,Q15
Start Date,End Date,Response Type,IP Address,Progress,Duration (in seconds),Finished,Recorded Date,Response ID,Recipient Last Name,Recipient First Name,Recipient Email,External Data Reference,Location Latitude,Location Longitude,Distribution Channel,User Language,"Your team name (i.e., the fantasy animal)",The team name (fantasy animal) for the analysis you are reviewing.,"Please rate the following aspects of the study under review with regard to how well they answer the research question (""Do speakers acoustically modify utterances to signal atypical word combinations in referring expressions?"") with the available data. - Rate the phonetic analysis","Please rate the following aspects of the study under review with regard to how well they answer the research question (""Do speakers acoustically modify utterances to signal atypical word combinations in referring expressions?"") with the available data. - Rate the statistical analysis","Please rate the following aspects of the study under review with regard to how well they answer the research question (""Do speakers acoustically modify utterances to signal atypical word combinations in referring expressions?"") with the available data. - Provide an overall rating",Would the analytical methods presented produce an analysis that is:,Please explain your ratings of this analysis.,Please evaluate the choice of statistical analysis type.,Please evaluate the process of choosing variables for and structuring the statistical model.,Please evaluate the suitability of the variables included in (or excluded from) the statistical model.,Please evaluate the suitability of the structure of the statistical model.,Please evaluate choices to exclude or not exclude subsets of the data.,"Please evaluate any choices to transform data (or, if there were no transformations, but you think there should have been, please discuss that choice).",Please use this space for any additional comment you may have at this stage.
"{""ImportId"":""startDate"",""timeZone"":""America/Denver""}","{""ImportId"":""endDate"",""timeZone"":""America/Denver""}","{""ImportId"":""status""}","{""ImportId"":""ipAddress""}","{""ImportId"":""progress""}","{""ImportId"":""duration""}","{""ImportId"":""finished""}","{""ImportId"":""recordedDate"",""timeZone"":""America/Denver""}","{""ImportId"":""_recordId""}","{""ImportId"":""recipientLastName""}","{""ImportId"":""recipientFirstName""}","{""ImportId"":""recipientEmail""}","{""ImportId"":""externalDataReference""}","{""ImportId"":""locationLatitude""}","{""ImportId"":""locationLongitude""}","{""ImportId"":""distributionChannel""}","{""ImportId"":""userLanguage""}","{""ImportId"":""QID2""}","{""ImportId"":""QID4""}","{""ImportId"":""QID6_2""}","{""ImportId"":""QID6_3""}","{""ImportId"":""QID6_4""}","{""ImportId"":""QID7""}","{""ImportId"":""QID9_TEXT""}","{""ImportId"":""QID10_TEXT""}","{""ImportId"":""QID11_TEXT""}","{""ImportId"":""QID12_TEXT""}","{""ImportId"":""QID13_TEXT""}","{""ImportId"":""QID14_TEXT""}","{""ImportId"":""QID15_TEXT""}","{""ImportId"":""QID16_TEXT""}"
2022-04-29 15:08:51,2022-04-29 15:25:45,IP Address,153.104.35.64,100,1013,True,2022-04-29 15:25:46,R_1NsJgK4gcS9kRAJ,,,,,40.0428,-75.3506,anonymous,EN,polymetme_brevirostrum,eriphia_laterispinis,90,100,98,publishable as is.,"My ratings were based on replicability, motivation of the study from previous research, appropriateness of the analysis given the data and hypotheses, and interpretation of results.",The analysis type was appropriate for the data,"Variable selection was appropriate and well motivated. Model specification was nearly flawless; my only question is why such long chains were used when sampling the posterior. If not warranted by the data, doing so uses more time and resources to arrive to the same conclusion.","Use of the average values across the sentence, while easier than isolating average values within regions of interest, may not index the intended phenomenon. Several out-of-the-box tools currently exist to automate the process of alignment, so this could have easily been done. ",The model structure is gorgeous. The fitting procedure is excellent. The assessment of fit is spot-on. Every piece of the analysis is accountable and replicable (up to random noise that could have been avoided by setting the random seed).,"The others did not exclude errored trials, which may have biased their findings. ",The choice not to transform data was justified.,The statistics are beautifully done; this could be a gold standard for peer review.
2022-04-29 15:26:29,2022-04-30 09:02:51,IP Address,209.93.143.84,100,63381,True,2022-04-30 09:02:52,R_2VxO57A52A5PIPz,,,,,52.5297,-1.8543,anonymous,EN,psittacula_scabriculus,nestor_idahoensis,70,80,75,publishable with minor revision.,"The analysis conducted was very detailed and thorough, I had a few recommendations but I would include this as a publishable code that should be shared with students. ","The choice of statistical analysis was conducted well and was explained clearly, concisely and thoroughly. ","The variables were driven by the literature, as opposed to including them as controls to purify the variable, allowing type I error to be reduced (Shears & Zumbo, 2013; Spector & Brannick, 2011;  Westfall & Yarkoni, 2016). However, in the future, it is recommended that the variables are centred and scaled.  The structure of statistical model was very clear, concise and provided enough researcher to reproduce the analysis.","The variables were driven by the literature, as opposed to including them as controls to purify the variable, allowing type I error to be reduced (Shears & Zumbo, 2013; Spector & Brannick, 2011;  Westfall & Yarkoni, 2016). However, in the future, it is recommended that the variables are centred and scaled. ","The structure of statistical model was very clear, concise and provided enough researcher to reproduce the analysis.","Although they excluded Force-aligned phone intervals shorter than 50 ms, it would be nice to explain why this was done. Much outlier exclusion or data removal is arbitrary, potentially leading to type I error.  However, if there was an explanation provided as to why this choice was conducted, it would allow the thinking to be more reproducible. ","The choice to use raw data was adequate. However, there is a need to include statistical assumptions to check if the correct technique used. Bayesian is an approach that can apply to any statistical technique, but it is unclear if each assumption has been fulfilled.  If the residuals were not normally distributed, there is a need for transformation (e.g. log, inverse) to ensure the normal distribution was normally distributed.",
2022-05-02 02:59:32,2022-05-02 03:23:00,IP Address,77.140.136.4,100,1408,True,2022-05-02 03:23:01,R_3soe745I7v042Gm,,,,,43.2951,5.3861,anonymous,EN,eriphia_laterispinis,genyonemus_evotis,90,80,85,publishable with minor revision.,"The analysis seems sensible, but I should disclose that I am not familiar with the kind of analysis performed by the authors. This is reflected in my review below, which should be taken cautiously.","The statistical model per se (i.e., the binomial regression) seems like a sensible choice, although a discussion of issues related to multiple comparisons or over-fitting is missing. Indeed, the authors included a large number of predictors in the GLM in a theory-blind fashion, and report the significant predictors (i.e., the predictors with a p-value lower than .05). However, even if none of the predictor is truly (i.e., in the population) associated with typicality (the outcome), it is expected that some predictors will get a significant p-value, just because of random sampling. A more appropriate alternative would have been to fit several theoretically-sound models and to compare them (e.g., using information criteria).",This issue has been discussed just above: I am worried that some significant associations may just be false positives.,This issue has been discussed just above: I am worried that some significant associations may just be false positives.,"The likelihood choie seems sensible, but I am worried that some significant associations may just be false positives.",I am not competent to evaluate the pre-processing.,I am not competent to evaluate the pre-processing.,"As mentioned previously, the phonetic analysis seems sensible, but I should disclose that I am not familiar with the kind of analysis performed by the authors. This is reflected in my review below, which should be taken cautiously."
2022-05-02 15:03:55,2022-05-02 15:30:05,IP Address,129.108.202.181,100,1569,True,2022-05-02 15:30:05,R_3BFxxJjSNyrEkEh,,,,,31.7703,-106.3006,anonymous,EN,paralichthys_undulatus,hoplostethus_macrosteus,90,100,95,publishable as is.,"They chose two relevant acoustic measures, they labelled and measured them accurately and presented the results clearly. ","Linear regressions were run on each acoustic measure, after comparing models. This is a sound approach to using this type of analysis. ","The dependent variables were duration and intensity - the only issue here is that the instructions were to only include one in the writeup. The independent variables were typicality, color and observation number. Including color is useful because all of the colors were repeated in each typicality condition. ","They used model comparison based on AIC values and also included participant as a random effect. Also including ""observation"" (i.e., experiment order) as a possible effect is interesting to see if where the utterance came in the experiment may have an effect.","Using a linear regression model for each of the acoustic variables is a valid approach to answer this research question. ","They examined the NF condition, only the adjectives. For the sentences that were marked as errors, the authors only excluded them if they affected the target adjective or noun. They also excluded any tokens whos measurements were excluded any values that were +/- 2 standard deviations from the mean, which is logical. ","The authors note that they ""normalized the duration and the intensity measures"" but they do not provide any detail on how they actually did this. ","This analysis is well done - the only drawbacks being that they reported on two instead of just one, and also that they did not provide any detail on how they normalised duration and intensity. "
2022-05-03 10:24:21,2022-05-03 11:21:33,IP Address,73.14.34.31,100,3431,True,2022-05-03 11:21:34,R_22wUBi7RA0v29tk,,,,,40.0373,-105.279,anonymous,EN,clione_dorsalis,sphyrna_ellioti,40,40,40,publishable with major revision.,"Both the phonetic analysis and the statistical analysis are difficult to interpret. Our main issues were that the effect of typicality is buried in interactions with focus and condition, and the focus of the analysis seems to be on the focus condition rather than the typicality condition; the authors did not make predictions for their models, and it's unclear how to interpret interactions between part of speech and focus condition effects on the prosodic features used as dependent variables. As a more minor issue, using three measures of F0 at different points in a word is also difficult to interpret considering the different word lengths and word structures. ","The liner mixed-effects model is a reasonable approach to this analysis. ","Overall, the choice of variables was sufficient. The independent variables were all necessary, but it is unclear what the predictions are regarding nouns and focus conditions other than adjective focus. The interactions between part of speech, focus condition, and typicality are difficult to interpret and seem to highlight the focus effects rather than the typicality effects. The focus conditions were unbalanced and it's unclear how that affected the results. 
 ","The variables were suitable to include in a model investigating the effect of typicality on prosodic realization.  ","As stated previously, the structure of the model placed the effect of the variable of interest (typicality) inside a two- or three-way interaction and made interpretation difficult. ","The choice to include nouns as well as adjectives was interesting, but the design of the analysis given that the focus conditions weren't balanced across groups made interpretation difficult. ",Log transformation of word duration was appropriate,"We had reservations about interpreting complex interactions in the linear mixed-effects model, especially when those complex interactions included the main variable addressing the research question. The unbalanced nature of the focus condition indicated to us that the experiment was designed to test the adjective-focused condition, and we're not clear whether using an unbalanced data set had an effect on the model results.  We question the validity of the interpretation of the results using data from multiple conditions that was collected during an experiment designed to investigate a specific condition. "
2022-05-04 11:59:59,2022-05-04 12:35:37,IP Address,96.242.236.104,100,2138,True,2022-05-04 12:35:38,R_3HRcUfJLljgU9Lu,,,,,40.5921,-74.6238,anonymous,EN,procambarus_mahogoni,pervagor_meeki,80,80,80,publishable with minor revision.,"We question the decision to slow down the one speaker's rate of speech and wonder what the criteria was that went behind that decision. Specifically, why it was slowed down by a factor of 1.5",We agree that a series of linear mixed effects models are a good choice for this data set.,"The variables (duration, pitch, intensity) answer the question that was posed in the project. ","We question the use of duration of the whole word, especially in the nouns, as the nouns used are not present in every typicality condition, and are very different in length. Additionally, typicality was used as a categorical variable instead of a continuous one, which might have yielded different results.",The structure of the model is suitable for the data set. The authors are aware of their analyses's limitations with regard to including word as a random effect.,"The authors marked sentences for speech error and noise or hesitation and excluded them from the analysis. We wonder why the ""fast speaker"" was slowed down instead of just excluded as other data were.","As mentioned above, we would like a more detailed reasoning or criteria for slowing down the rate of one of the speakers by a factor of 1.5. ",N/A
2022-05-04 22:10:07,2022-05-04 22:29:53,IP Address,115.128.19.253,100,1185,True,2022-05-04 22:29:54,R_bIqRDALh5Lpt8Zj,,,,,-33.8715,151.2006,anonymous,EN,dunkleosteus_inscriptus,pervagor_adscensionis,75,60,68,publishable with major revision.,"It is not clear how the final models were chosen, other than for reasons involving convergence issues with more elaborated models. The analysis of adjectives, but not nouns or noun phrases is not motivated.","Linear Mixed Effects models are suitable, but Bayesian Mixed Models might have been more informative and a more appropriate approach, given the exploratory nature of the analysis. In particular, since the team state at the outset that ""we had no strong hypothesis about the results we expected to find"", and since no hypotheses are developed over the course of the analysis, the choice of NHST does not seem to be well-motivated.",The reasons for the choice of variables in the final model are not clearly explained. The main factor informing model structuring and refinement appears to data sparsity and model convergence issues. This would be OK if more information had been provided to motivate the random variables selected in the final model.,"The main effects of typicality and the choice of continuous variables are well chosen and clearly motivated, but it is not clear why random intercepts are included only for adjectives.","It is not actually clear which final model was used: 'MFCC_all' is reported as having good residuals in 'stats.R', but earlier, the model is defined as follows:
MFCC_all = lmer(mfcc_dist ~ typ_diff + (1|adjA) + (1|subjA) + (1|noun_pair), all) # singular fit if 1|noun_pair is included

... so what was the final model run for the analysis? Presumably, the formula was modified to drop the final random effect, but the analysis is not reproducible from the scripts provided.",It appears that all numbered trials in the dataset provided were included in this analysis.,"The use of MFCCs, GMMs, and z-scored KL-divergences is insightful; these are well-established methods of data parameterization for speaker ID analysis and utterance comparisons of this type. This team have extensive experience in the application of these methods, and the data processing transformation and analysis are appropriate to the dataset and research question.","This team have used a novel approach to examine differences between utterances, and the application of methods more commonly used in forensic analysis and speaker recognition potentially offers important insights into the phonetic properties of the dataset. Unfortunately, it is difficult to interpret this approach because the tools are not deployed in service of a hypothesis, and the discussion is not sufficiently detailed to explain exactly what was found and what it might mean. In particular, the sub-division of the utterance, and the choice of models and how they related to these sub-intervals is not explained. For example, why are the MFCCs of adjectives produced in different typicality conditions examined, but not the entire target NP, the noun itself, or other constituents associated with the referent? The division of utterances into ‘carrier-pre’, ‘article’, ‘noun’, ‘adjective’, and ‘carrier-post’ seems like a good approach, but then it is not clear how the interval of interest (between ‘carrier-pre’ and ‘carrier-post’) is analyzed and modeled, other than the adjective. From the script 'stats.R', it appears that convergence issues are the main reason the linear models didn't include the noun, but if so, why include random intercepts only for the adjective rather than the actual target noun? Is this purely a model fit/data sparsity issue, or is there a motivation for including random effects of adjective, but not nouns? What about the determiner, and the whole DP/NP?

Another issue not explained is how timing differences were dealt with in this analysis: are duration differences captured/bypassed by the Gaussians/MFCCs, or were the data time-normalized, or were temporal differences disregarded? Presumably the Kullback-Leibler metrics were used to compare utterances differing in the number of frames/dimensionality of the vectors, but how does this affect the quantified divergence, or doesn't this matter for this analysis? Figures illustrating the intervals of analysis would help to explain these issues, as would a more detailed report and more complete documentation of the statistical modelling used.

Materials on OSF repository should be anonymized if blind review was required, but scripts and code have all been signed by this team."
2022-05-05 02:15:31,2022-05-05 02:20:44,IP Address,195.221.243.131,100,313,True,2022-05-05 02:20:45,R_339yMtEwTp2YAyR,,,,,48.0014,0.1901,anonymous,EN,ceratophrys_elephantotus,trachyphyllia_lappa,95,100,97,publishable as is.,"Solid methods, even if manual corrections of the TG are not easily replicable.","Linear mixed effects models that were put into competition against each other. ","Well chosen and explained in a clear matter. ","Very good. ","Very good. ","Well documented and argued. Exclusions were based on the annotators experience of the data sets. ","No data transformation. ",
2022-05-05 08:20:23,2022-05-05 08:48:21,IP Address,144.32.240.17,100,1678,True,2022-05-05 08:48:22,R_0O2o75YvmeCcO65,,,,,53.9573,-1.0837,anonymous,EN,stygobromus_tyraica,pervagor_adscensionis,95,75,80,publishable with minor revision.,"We didn't find any issues with the overall methods, but found the results difficult to interpret",The use of discriminative techniques means that the question is answered in a more indirect way - so acoustic effects are inferred from classification rates.,"Choice of variables seemed good, but structure of statistical model doesn't directly answer the research question (but does address it in an interesting way).","A very wide range of acoustic features included, but unclear what each of the features actually represented. We also found it difficult to interpret the final result when referring only to DCT coefficients.",We're not familiar enough with this type of statistical modelling to comment on the specific structure of the model,Appropriate,"Lots of transformations applied to the data which in principle make sense, but in practice make it difficult to interpret the results",Beautiful R Markdown write up
2022-05-04 17:22:12,2022-05-05 15:29:00,IP Address,192.145.118.198,100,79607,True,2022-05-05 15:29:00,R_8fe60cv7ws3AAAF,,,,,37.3931,-121.962,anonymous,EN,pervagor_meeki,aracana_bitatawa,80,75,77,publishable with major revision.,"Positives: 
- Addressed unbalanced data
- Transforming pitch
- Considered speech rate
- Considered controlling for other variables
- Visualization of the factors (typicality / adjective + nouns) in tables
- Time-normalized f0 in the QGAM

Negatives
- Specifications of QGAMs needed more detail
- Motivation for using QGAMs for duration was unclear
- Pipeline for determining which fixed effects are included was unclear
- No effect sizes
- Unclear why the noun’s acoustic properties were not analyzed
- Needed more detail about the hand-correctors and the criteria used to annotate duration of the sentence
","1. The authors could have provided more motivation for the choice to use QGAMs as opposed to other analysis methods. In particular, why QGAMs were appropriate to analyze duration as it doesn’t vary across time.

2. The team motivated the use of QGAMs under the assumption that residuals would not be normally distributed, but didn’t include a check showing that this is the case.

3. Based on the Fasiolo et al. (2020) paper cited by the team, QGAMs are a Bayesian method. The team should address what the “p” value means in a Bayesian context.

4. Effect sizes were not discussed in the report. The effect size mentioned in the survey responses is only for duration, not for pitch.
","1. The pipeline for including or excluding fixed effects was not well described. Was the choice to include or not a fixed effect based on model comparison? They note that they use a “confirmatory analysis when testing typicality, which is why we did not compare the following model to simpler models on the basis of ML-scores or AIC” (p. 5), but whether they used maximum likelihood/AIC for log_freq, speech rate, etc. is not clear. 

2. It was unclear why the team only analyzed the adjective and not the nouns, considering that the noun focus condition was the only condition that included trials across all three typicality levels. The team raised an important issue in the study design by noting that adjectives, but not nouns, were counterbalanced across typicality levels; however, since the team controlled for the noun’s # of syllables, it would have been reasonable to look at how duration and pitch varied for the nouns as well.

3. There could have been more details as to how the syllable count used to compute speech rate was acquired.

4. The team could have also looked at intensity, a variable that consistently varies as a function of focus.

5. It was unclear how pitch was extracted over the course of the adjectives and over how many measurements.
","See comments to question 11. 
","See comments to question 11. 
","We agree with the team’s choice to exclude error trials. However it was unclear whether only trials marked as containing an error were excluded, or also trials marked as containing a hesitation, pause or structure error. Also, the survey responses indicated that all observations were included in the analyses, in contrast to what was described in the report.","We agree with the team’s choice to transform f0. However, the parametric table output (e.g., with Intercept = 184.95) suggests that the analysis was not conducted on z-scored f0 values. 
","1. It would have been nice to have more information about the 3 hand-correctors:   what criteria did they use for segmenting the words? How consistent were they? Did 1 hand-corrector do the entire textgrid for the subject? Similarly, it would be useful to know whether the Rapp force aligner consistently led to errors, and if so, we might suggest the use of a different force alignment method.

2. In the QGAMs, what was “err” defined as? Did the team use the default 0.05?
"
2022-05-06 11:02:15,2022-05-06 11:39:34,IP Address,77.182.180.85,100,2238,True,2022-05-06 11:39:35,R_2CP7WCnONoo4S85,,,,,50.1143,8.6641,anonymous,EN,aracana_bitatawa,alosa_atun,100,100,100,publishable as is.,"The analysis is performed very thorough. Each step of preprocessing and of the analysis are well described. The analyses are described in detail in the supplementary data. 

Regarding preprocessing: the automatic forced alignment was complemented with manual segmention (I would have wished to see how segmenters dealt with problematic cases of the [e] and [@]  boundary, but this is just complaining at a high level).

Regarding the analysis: The authors pre-registered their analysis and reported the results of the preregistered analysis. However, since the preregistered model would not converge, they opted for a more exploratory, backwards fitting procedure. This step is fully justified. ","The authors used lmer which, given the nature of the  dependent variable (point-wise measurements of formants and F0, durations) and predictors (numeric predictor of typicality, categorical predictor of focus) is fully justified. ","Choosing dependent variables: 
The authors chose to investigate the direct article preceding the adjective+noun phrase. Given the large phonetic variability in the phrase, this was a very elegant move to controll for phonetic variability . 
Accordingly, the authors opted to investigate literally all possible acoustic measures (formant values, pitch, word duration, speaking rate). Given the task at hand, this is justified. 

Choosing predictors: The authors chose good control variables. As for the predictors of interest, it was a good move to chose average typicality ratings from the rating experiment to get a broader distribution of the ""typicality"" predictor (instead of using a categorical predictor).",All variables are suitable. See above,"The  structure of the final, presented model, is fully sensible. However, I am not surprised that the preregistered model with a three-way interaction in the fixed effects and a three-way interaction of slopes with three (!!!!)  random intercepts did not work. This Barr et al. approach surpasses overfitting. ","Given that the authors have decided to use numerical typicality predictors, it is acceptable that they did not exclude the ""adjective focus"" condition. 
Regarding the exclusion of ""den"": this is totally acceptable, given that the [@] woud systematically differ in the [n] context (and that it was not part of the target phrases).
The exclusion of formants is completely sensible. ","The authors have not reported any transformations. It is very likely that the speaking rate predictor and the word duration variable would have needed a transformation. But I do not think that this would have changed the results. ","I am very impressed with the thoroughness of the analysis. One minor point that I have is the separation of [e] and [@] in ""der"". In my experience, finding a boundary between these two may be very hard. The authors could have used a non-linear method to investigate the entire formant trajectory of the entire vowel complex [e@] or what ever there is. 

Another aspect concerns the interpretation of the statistical analysis of F1. The authors made predictions for all the nine analyzed measures. However, only one turned out to yield a significant effect with a very low t-value (2.5). Under normal circumstances, I would be very careful to state that there is a significant effect of typicality. 

But again, these two points do not diminuish the accomplishments and thoroughness of the authors."
2022-05-08 12:06:50,2022-05-08 12:14:36,IP Address,67.1.31.162,100,466,True,2022-05-08 12:14:37,R_3Je0fLOUP9UMqQE,,,,,32.246,-110.9177,anonymous,EN,dermatolepis_aculeatus,neosilurus_omanensis,95,95,95,publishable with minor revision.,The team needs to elaborate clearly on the outliers issue. The criteria of deleting the outliers was not clear enough for readers. Is every false response or error an outlier?,The two models are good. But the authors have to clarify the reason behind the effect size's formula used in their analysis.,Very good.,The authors need to clarify the differences between errors found in the original data and the outliers.,Very good.,Good,Good,
2022-05-01 20:18:27,2022-05-01 23:56:40,IP Address,184.64.221.169,20,13093,False,2022-05-08 23:56:43,R_1P5KubktZHYZeQz,,,,,,,anonymous,EN,pseudopleuronectes_assasi,procambarus_maculosus,,,,,,,,,,,,
2022-05-09 12:20:10,2022-05-09 13:10:16,IP Address,134.69.55.186,100,3006,True,2022-05-09 13:10:17,R_20V5OlA7o6OtsU1,,,,,34.1325,-118.2076,anonymous,EN,hoplostethus_macrosteus,petauroides_fistulator,50,85,70,publishable with minor revision.,"Regarding the phonetic analysis: according to the instructions provided by the project creators, the WORD tiers were not aligned with each German word. However, it is not clear from the authors’ write-up whether they aligned the WORD-level boundaries prior to extracting the words and saving them as separate files. This has the potential to greatly affect the outcome of the results (if, in fact, the F0 trajectories are NOT from the Nouns, but other words in the utterance).  
","This seems reasonable to us; as F0 varies over time, it makes sense to model with GAMMs. ","The choice to include all of the variables from the spreadsheet in the model is not clearly motivated. ",Why were the distractor names and colors included? We are not clear on the authors' motivations.,"As researchers who are marginally familiar with the GAMM analysis, we have some questions. First, why not use “typicality” as a by-variable smooth term, since it is the variable of interest? Additionally, why were all the other variables included as random smooths, rather than parametric terms?  
Overall, we would have benefited from more detail about the setup of the model and motivations for the specific selections made by the authors. ","No data were excluded, according to this write-up. No mention was made of false starts, hesitations, or other errors that were enumerated in the textgrids, which we find to be an oversight that may have affected the outcome. ","The authors transformed the F0 measurements into semitones, essentially normalizing by speaker to account for inherent vocal tract size differences. We find this to be an appropriate way to account for these differences.",
2022-05-10 04:27:46,2022-05-10 04:52:46,IP Address,213.55.226.71,100,1500,True,2022-05-10 04:52:47,R_3ELJUGgvfiNYODQ,,,,,47.3857,8.5348,anonymous,EN,aratinga_lugubris,procambarus_mahogoni,69,37,51,publishable with major revision.,"'-We didn’t manage to find annotationTextGrids;
- We don’t think that it is reasonable to analyze acoustic correlates (the team chose vowel duration, pitch and intensity of the stressed vowel in adjectives) without taking into account inter-speaker differences, a speaker’s gender, vowel quality and syllable structure.
- We don’t think that it is reasonable to use a variable of interest as a predictor, rather than as a predicted variable (the team decided to predict normalized duration/intensity/f0 by typicality not the other way around).
- In the mixed-effects model typicality is both a predictor and a random intercept. Possibly this is why two out of three models had singular fit.","Linear mixed effects models are suitable for this type of data, but we have identified flaws in the application of the models and their structure.","In general the choice of variables is reasonable, but
-The authors use normalized acoustic features, but they normalize them across all observations without grouping by speaker (we’ve found this in the code, not the pdf)
-The authors didn’t take into account the gender of the speakers.
-The authors predict acoustic features using typicality and not the other way around, hence it should be a multinomial regression, not a linear one.

Also the authors did not take into account the fact that each combination Color+Noun was produced twice and the production could be different from each other.
"," The choice of variables is reasonable and is backed by literature, but the way they were used is flawed (see the comments above).","Since two of the three models had singular fit, it means that their structure was too complex for the provided inputs. Also the authors used typicality as both a predictor and a random intercept.

Also, please see comments in ""Please evaluate the process of choosing variables for and structuring the statistical model.""",The authors do not mention whether any data was excluded. Based on the variables used we could assume that for each token all of the three variables were available so there was no need to exclude any of them.,"We think that FPCA transformation of the data and making its result the main predicted variable in the statistical analysis is not justified by the researchers.
The authors use normalized acoustic features, but they normalize them across all observations without grouping by speaker.
",
2022-05-10 06:06:15,2022-05-11 04:32:00,IP Address,88.10.224.231,100,80745,True,2022-05-11 04:32:01,R_p5vUOmtYu9hLFZv,,,,,43.3126,-1.9745,anonymous,EN,gymnothorax_spinulosus,anomalocaris_ornata,88,56,67,publishable with minor revision.,"The use of mixed model is good. The choice of the fixed effects and random effects as well.
But they only analyze the NF condition. The other condition (AF and ANF) are simply excluded with no specific explanations in the report.
We have to note that they did not follow the guideline to choose only one dependent variable (instead they analyzed pitch, duration and intensity separately) - which to me is a good thing for a paper and something we would probably all do, but just did not respect the guidelines.
","The choice of mixed models is a good one for this type of dataset, in order to control for variability coming from the participants and the stimuli.","Overall the process is not much explained.
They did not choose a particular dependent variable because they used many of them, so it’s hard to evaluate this selection process. 
The good things are: Model residuals were checked properly. There was one transformation because one distribution was skewed.
They did not explain why they choose to use the typicality factor as a categorical variable (rather than the continuous scale typ_mean or typ_median). 
","There are good things and bad things.
To me the choice of using the typicality factor with 3 modality (rather than the continuous scale typ_mean or typ_median) is not a very good choice because they lose some information by reducing a continuous scale down to a 4-modalities factor. (and it kind of forces them to just discard the AF and ANF conditions)
We believe all the conditions (AF, NF, ANF) should have been included in the model instead of removing all the data from the AF and ANF conditions.
The use of col_obj factor (2 modalities: noun vs adjective) is a good idea to contrast the prosody in the color adjective vs the noun (instead of using absolute values or normalizing relatively to the whole sentence) and keeping everything in the model.
","Given the variables they have chosen, the structure of the statistical model is very good.","They excluded all data from NF and ANF condition which we disagree with. 
They also exclude outliers rather properly, although the way they exclude the outliers vary from one dependent variable to another.
","The choice to transform data seemed good, at least well-justified and useful.",
2022-05-11 05:33:33,2022-05-11 07:09:03,IP Address,138.246.3.45,100,5729,True,2022-05-11 07:09:04,R_3kEUp6k0mMm2AmR,,,,,48.3943,11.7319,anonymous,EN,gnathosaurus_canadensis,cromileptes_saxatilis,100,75,88,publishable as is.,"The analysis is well done and well explained, the methods are appropriate considering data and research question. However, it is not well argued why only the typical and atypical conditions were compared as opposed to all three conditions.",The choice of Bayesian mixed effects lognormal regression seems appropriate based on my research into the model.,"The fact that adjective duration was chosen as the main dependent variable was backed up by other literature, and thus seemed reasonable. The other variables (speaker, utterance, degree of typicality) were more or less given by the research question and the way the data was set up.","All included variables are suitable for the model, as already mentioned, I don't understand why medium typical combinations were not included.",Since I have never used the model myself (or have much knowledge of Bayesian statistics) I cannot rate the structure of the model.,"As far as I can tell, including medium typical combinations would not have made the model more difficult to interpret and thus I do not really understand why they were excluded. 
Exclusion of utterances marked as erroneous, as well as missing data points based on  the duration extraction method seems reasonable. 
Why the conditions other than the target condition (noun focus) were included was not explained.","no obvious data transformation was done (at least none that was described), but since only one dependent variable was analyzed, I wouldn't expect much data transformation",I really liked seeing an analysis that focused mainly on one dependent variable instead of a multifactoral approach.
2022-05-11 08:58:54,2022-05-11 09:06:43,IP Address,128.187.116.12,100,469,True,2022-05-11 09:06:44,R_vV7xsvy2lz9ZvDb,,,,,40.2584,-111.6591,anonymous,EN,trapezia_cantonensis,arapaima_modularis,95,100,98,publishable as is.,This was a relatively straightforward but thoroughly documented analysis. No red flags anywhere in the analysis pipeline or in the statistics.,ANOVA was a good choice for what this team was doing. My one complaint is that adding speaker as a random effect might have been helpful.,"There were two variables, pitch and treatment. Very straightforward and justified in a brief review in the beginning. ","The model itself was fine, though it would have been perhaps better to include speaker as a random effect. ","The model's structure was fine. ","This team excluded creaky voice, which is understandable given that they were focused on pitch. But, I recall seeing a lot of creaky voice, so I wonder if there was a lot of data excluded. ",No transformations as far as I could tell.,"I appreciate the thorough documentation of how the data was collected, the author's confessions of limited German and how that affected the data collection, and the detailed statistical output."
2022-05-11 09:50:04,2022-05-11 09:53:56,IP Address,67.218.235.188,100,231,True,2022-05-11 09:53:56,R_2wyjvv3U6w6megc,,,,,42.8087,-1.6628,anonymous,EN,pseudodax_euryzona,anthracoceros_coronata,50,60,55,publishable with major revision.,"This analysis reported test results on a combination of three metrics, duration, maximum intensity and maximum pitch, and two words within each sentence, the adjective and the noun. This resulted in six different variables. Each dependent variable was tested separately per 3 conditions of typicality, resulting in 36 models. There were 6 lm models without the condition, 18 lm models for each condition, 6 with the condition as a fixed effect, and 6 with the condition as a fixed effect plus the interaction.
The choice for separating noun and adjective was not explained. The hypothesis behind this rationale is not explicitly provided. The choice for duration, intensity and pitch was also not discussed. Moreover, the analyses on intensity and pitch were based on the maximum point within the word, and did not take the within-word variation into account. In addition, random factors had to be removed from the analyses due to convergence / singularity issues, and some aspects such as speech rate (which directly affects duration measures) and inter-speaker variation (as random effects in the lm statistical model) were not controlled for.","The originally planned statistical analysis type (LMER) seems correctly selected, but it was changed to (LM) later. Without further normalization, LMs do not seem appropriate (e.g. because the models do not take inter-speaker variation into account).","The metrics chosen could be relevant to the dataset, but the analysis seems to be carried out with elimination in mind, rather than choosing a specific metric. Specifically, the motivation behind using  a maximum intensity point and a maximum pitch point is not strong enough. Many nonlinear models could be used to account for intensity and pitch variations within the word. Thus, fine-grained changes in intensity and pitch within the word could be discerned. 
The number of tests, 36, makes  the analysis feel a bit like a “fishing expedition”, with multiple selected metrics and multiple testings of them. Bonferroni correction is applied to establish the alpha level, but it is calculated with the number of response variables (6) as a basis instead of the performed tests (36), although I do not think the results would have been too different with the alpha set at 0.001.","The authors chose to report duration, intensity and pitch as response variables. Mean and maximum intensity and pitch were extracted, but mean values were discarded afterwards. Choosing the maximum intensity within the word as the dependent variable can be problematic, as the maximum peak can be achieved in different positions within the word, and this would cause variation that would either obfuscate results or create patterns that do not reflect the real effect of the typicality predictor. Especially in the case of F0, modeling curves (e.g. with GAMMs, DCT transformation or fPCAs) would be more appropriate as much information is lost when using single data points as a dependent variable.
Lot of focus is put on the predictor “condition”, but the focus condition is not part of the research question. In addition, only one of the focus condition levels (NF) covers the whole typ_mean spectrum: ANF lacks values below 25 and AF lacks values over 70 (both having only two typ_mean clusters, as opposed to NF, which has three). Thus, the three levels of typicality (or typ_mean value clusters) are only represented in the NF level of “condition”.","The models are structured as lm(value ~ independent variable, data = .), where value represents the dependent variable. The authors explain previously, 6 lmer models were carried out with a random intercept for speaker, but that models had singularity issues, so speaker was removed and lm models were carried out. 
For the independent variable, the authors explain that they had 3 conditions, that resulted in 36 models: 
Separate models (with combined) condition: value ~ typ_mean (6 models)
Separate models per condition: value ~ typ_mean (6 models)
Condition as a factor but not interaction: value ~ typ_mean + condition (18 models)
Condition as a factor, with interaction: value ~ typ_mean * condition (6 models)
The rationale behind the inclusion of the variable condition or the interaction is not provided, and this choice is not explicitly justified by the research question.","Outliers from each metric were removed following a 2-standard deviation cutout. Mean and maximum intensity and pitch were extracted, but mean values were discarded due to their distribution not fitting gaussian. Aside from these and the sentences including notes in the original textgrids, there is no mention of further exclusions. This may not be enough. Our team had to mark extra sentences as errors or hesitation breaks in addition to the ones already marked as such in the provided textgrids. Errors and hesitation breaks likely affected the measurements (e.g. duration) in these sentences.","Duration values were log-transformed. All variables were z-transformed and centered. There were no further transformations and thus speech rate, inter-speaker and inter-utterance variation were not controlled for.",The script “data_cleaning.R” was mentioned in the report but not included in the data files.
2022-05-11 14:07:23,2022-05-11 15:50:54,IP Address,209.170.228.242,100,6210,True,2022-05-11 15:50:55,R_12FOyGi0w9SWx1i,,,,,34.0544,-118.244,anonymous,EN,eosipterus_pytyopsittacus,clione_dorsalis,40,60,60,publishable with major revision.,"The authors were interested in possible differences in duration and vowel space measurements as conditioned by typicality condition. They ran three sets of models (which required extra work to evaluate), one examining duration of the full color word. The second set respectively examined Vowel Space Area, and the third set examined F1 and F2 at different points. This results in several total models, each with their own set of limitations. As we describe below, the choice to use linear mixed effects regression models was fine, as were the questions related to vowel formants/space and duration as conditioned by typicality. However, we have a number of concerns related to data measurement and analysis. First, vowel boundaries should not have been done by hand, as it is an unreliable method. A forced-aligner would have been more useful and saved the authors significant time. The duration models use the entire word, causing problems related to the lack of control of non-stressed vowels and consonants, which makes it difficult to ascertain the true effects of stressed vowel length. Finally, comparing data from 30 different speakers without doing any sort of vowel normalization to control for vocal tract length differences by speaker, and including random effects of speaker does not address this limitation since the effect is not random. In short, our main concerns had to do with the variables: the lack of vowel normalization and the selection of model variables (detailed below).","The use of linear mixed effects regressions for duration, VSA, and F1/F2 data is appropriate for attempting to understand whether the categorical variable of interest (typicality) alongside other categorical variables (gender, vowel identity) had an effect on the dependent continuous variables of word duration, VSA, and F1/F2. ","In all models, the variable of interest (typicality) is included as an independent variable; this is appropriate.

In the model of duration, the researchers chose to model word duration as the dependent variable, even though their questions suggested that they were most interested in vowel duration. Indeed, they say that they only measured word duration because of slippery issues of segmentation for vowels with a neighboring sonorant. While word duration is in principle comparable across conditions (given that the same words are used in all conditions), there is an issue that the words each have different properties. To some extent, it seems the researchers attempted to control for this by including the identity of the stressed vowels as a variable (given different inherent vowel lengths), but problematically each word also had its own unique set of other segments with their own inherent lengths. This would have been an issue that could have been avoided had the dependent variable been vowel length instead of word length. To achieve this, the researchers could have used forced-aligner software (so that there would have been stable principles for identifying phone boundaries, even in the context of sonorants) to allow vowel duration to be directly measured (rather than indirectly measured through word duration).  Still in the duration model, it is unclear why gender is included in the model: are there previously identified effects of gender on issues related to duration? If not, it seems like an extraneous variable that should not be included. Lastly, the filename (which appears to be a stand-in for speaker) is included as a random effect; this is appropriate for this measure. What is missing in this model, however, is the inclusion of an independent variable that maps onto word identity (given that this is a measure of word length).

For the model of vowel space area, there are no variables considered besides typicality and speaker as a random effect. First, it seems to us that speaker should not be a random effect: different speakers have different physical sizes which has a non-random effect on vocal-tract length (and thereby vowel space area). Instead, the researchers ought to have included some variable(s) that get at the physical size of the speaker. One measure they could have included is an induced vocal tract length measure (e.g., by running measures on a schwa vowel). Similarly, since VSA depends on vocal tract lengths which (in most, but not all) cluster according to gender, this model would be one where inclusion of gender would be appropriate. (This appears to be something the researchers knew to do, given the inclusion of gender in the F1/F2 models, but they may have forgotten that their VSA measures depend on F1/F2 measures.)

Finally, for the four F1/F2 models (F1 at the 2nd measure, F1 at the 4th measure, F2 at the 2nd measure, F2 at the 4th measure), the dependent variables were F1 measures or F2 measures (in raw Hz), and the independent variables were typicality, vowel identity, and gender, with filename (speaker) as a random effect. The choice of vowel identity is a good one. Similarly, gender is a well-selected variable, for reasons discussed above for the VSA model. However, as with the VSA model, it may have been appropriate to include a measure of vocal tract length (or to have normalized the F1/F2 measures, as described below) so as to better model the relevant factor (i.e. directly, without gender as a proxy for both body size and socialization). Finally, speaker should not have been used as a random effect (for the same reasons as those described for the VSA model).
","The model examining duration has several limitations. First, the choice to hand-measure the vowels and then only include duration of the entire word introduces a lot of variability into the results. The authors should have force-aligned the data so that they would have consistent vowel measures. They could have then excluded vowels in the environment of sonorants, or included that as a variable in their model. The inclusion of only stressed vowel as a fixed effect in the model masks the possible effects of the durations of other vowels in the same word, which is the chief issue with testing the duration of the entire word. Additionally, there is no justification given for including gender in this duration model, and no hypothesis for why duration might differ by gender. 

The models for F1 and F2 are unusual in that they examined timepoints 2,4, and 5, without motivating the choice of those points, or including the effects of preceding and following phonological context. (e.g., we expect F2 effects for a vowel followed by [t] to be different from the F2 effects for a vowel followed by [k]; this model did not account for that.). The biggest issue with these models however, is the lack of any sort of normalization. With 30 speakers of two genders, there is massive variation in vocal tract size, which the authors acknowledge in their comment about gender. As a result, direct comparisons between non-normalized Hz values for different speakers are not appropriate, as they may mask these predictable physiological differences between speakers. The inclusion of speaker as a random effect does not solve this problem, as speaker is not a truly random effect for non-normalized data. 

The models examining Vowel Space Area also have several limitations, similar to those for the F1 and F2 models. The authors have data from 30 speakers, in 3 typicality conditions, at two time points, leading to 180 VSA observations. However, again, there is no normalization for speaker, leading to the types of issues discussed above for the F1 and F2 models. VSA is essentially based on a combination of F1 and F2 values, so speakers with a smaller vocal tract, for example, would be more likely to have a smaller VSA compared to speakers with larger ones. This model also does not include gender, though it is likely to be predictor as well and was included in the F1 and F2 models. A priori vowel normalization would have addressed several of these issues. 
","The authors include 7 models in their analyses, all of which are linear mixed effects regression. This is an appropriate choice of model type for their analyses. However the models have several limitations. As discussed above, the inclusion and exclusion of gender for some models and not others, without clear hypotheses, is not appropriate. The fact that the duration model includes gender is bizarre. This model should also have a random effect of both word identity in addition to filename (which here is a stand-in for speaker). The vowel models should not include speaker as a random effect, for the reasons discussed above. ","The authors excluded observations labeled NA for typicality, which is appropriate given that that they were evaluating the effects of typicality",We think the authors should have normalized for F1 and F2 to control for physical characteristics of speakers (and constraints on how much variability each speaker can physically produce in vowels space).,"The authors may want to consider focus condition along with typicality, as we expect that focus would also influence vowel space and hyperarticulation (not to mention vowel duration).
The R has some errors (likely due to changing variable names) – we suggest that the researchers re-run the R from scratch in an empty R environment to debug to ensure that the code can be run successively line-by-line without error.
"
2022-05-12 06:05:13,2022-05-12 06:21:12,IP Address,109.164.199.99,100,959,True,2022-05-12 06:21:13,R_86xkxlkthmBVXW1,,,,,46.36,6.2046,anonymous,EN,naso_cassivellaunos,gnathosaurus_canadensis,80,55,66,publishable with major revision.,"For phonetic analysis, the author measured the f0 of the stressed syllables only and analysed the f0 difference between the target adjective and noun, produced by the same speaker. The use of the semitone scale is appropriate for comparing results from different speakers. The methods were appropriate for controlling between-speaker variation and they are justifiable given the experimental design. 

However, we believe that it is a drawback that only one variable was examined, as this approach has ruled out the potential use of other phonetic cues. Further, we did not find evidence of exploratory analysis. We would have given a higher overall score if plots showing speaker- and item-related variation were created. 

We think we can conceive of this one-man approach (as it seems) as the first step in a larger team effort. As such, it’s very respectable — a good start and clearly written up. However, in-depth exploration and/or statistical modelling and/or the inclusion of more measurements would be required to appropriately address the research question.

The combination of a quite simple analysis with frequentist statistical analysis is unfortunate. Pure exploration would be preferable in this situation (the results from the statistical analysis are likely to be misleading in such cases, even though there is only a null effect. 

","For statistical analysis, the author used a Friedman test and the choice was well-justified. However, the data reduction process (the f0 difference between the adjective and the noun was averaged across items to allow each speaker to contribute one data point for each ‘typicality’ condition) may have averaged out variations worth reporting. In the reduced data used in the present analysis, the author found two speakers being outliers but in the present form, it is not clear what caused them to be outliers (e.g., particular items or particular ‘typicality’ condition). It would have been helpful if the author presented some figures showing the distribution of the raw data.   

While the statistical approach is justified, it is not up to commonly used current standards. Speakers (and items) are not included as random factors in this rather simple analysis. Multi-level/mixed models, of the frequentist or Bayesian flavour, would have been really helpful here, and indeed, might be expected from such an analysis in our field at this point.

A purely descriptive/exploratory approach might have been completely justified too, but then we would have wanted to see lots of plots and in-depth analysis by speaker, item etc. 
","
The choice of the variables (dependent variable as the f0 difference between the adjective and the noun in the NF condition; independent variable as ‘typicality’) is appropriate. 

Please see our comments about the statistical analysis. The methodological choice was justifiable, but now speech researchers are reluctant to opt for the data reduction as used in the present study. 
","The choice of the variable was justified, and therefore it is not unsuitable, but taking only a single dependent variable rules out the possibility to examine other phonetic effects (e.g., duration). If the author wished to examine only the f0, then still a few more dimensions could have been added (e.g., pitch range).
","
The Friedman test is an acceptable choice for simple analysis, but definitely, it is not of the state-of-the-art techniques and it is potentially misleading.
","Only data for the NF condition were analysed, excluding the tokens with an error. This is a simple but appropriate approach. For the given analysis this choice was fine.
",N/A,
2022-05-12 08:16:50,2022-05-12 08:25:09,IP Address,108.162.129.213,100,499,True,2022-05-12 08:25:10,R_2yeb3FeQp6etmJa,,,,,45.3881,-75.6538,anonymous,EN,trachurus_riukiuensis,polymetme_brevirostrum,50,60,55,publishable with minor revision.,"We have concerns about both the phonetic analysis and the statistical analysis, both of which could be revised to make this analysis publishable.  

First, the phonetic analysis seems to use entirely automatic methods both for determining where the adjective is and for taking measurements from it, without requiring the researcher to ever listen to the recordings or look at the spectrograms (although the authors may, of course, have done so!).  We understand that this is both more expedient and more replicable than methods that require hand-labeling, however, we think completely automatic methods may lead to missing crucial things about the speech that would inform the choice of dependent variable and choice of exclusions.  The main issue in this case is creaky voice, explained below. 

Second, for the statistical analysis, we like the approach, but are concerned about the choice to include the AF and ANF conditions alongside the NF condition (see Q12).  Also, although typicality for the AF and ANF conditions were coded as NA in the ratings file, the authors recoded this using the binning procedure discussed in the pdf of the methodological details, however, they used the median typicality ratings, where it appears that the original researchers used the mean for the binning (NOTE: in the methodological details it does not specify that the mean was used, but this appears to be the case based on inspection of the participants’ trials files). This leads to 4 noun phrases being categorized as ‘atypical’ because their median typicality rating is below 25, while their mean typicality rating is above 25.  Because of this miscategorization, the ANF condition appears to have all 3 levels of Typicality, when it should only have 2 (medium and typical). Ultimately, we don’t think the experiment was designed to test the effect of Typicality in the AF and ANF conditions. 
","This method seems fine. A minor note that the R code read in two of the three data files provided, but there was a missing file (or the third one was mislabeled). We *think* we figured it out, but we aren’t 100% sure.","We are not very familiar with the process of using ELPD values to determine which random effects to remove from the model, but it seems ok.  ","Gaps in experimental design: 
We are surprised by the choice to include the AF and ANF conditions in the analysis, and to include both the Condition (AF, ANF, NF) factor and an additional factor Category (FOOD/NON-FOOD).  The AF and ANF conditions do not seem to be designed to test the question about Typicality, so the AF and ANF items are not well-distributed among the Typicality conditions.  There are AF items with atypical and medium Typicality, but not with typical Typicality, while there are ANF items with medium and typical Typicality, but not with atypical.  Only the NF condition has data in all three levels of Typicality.  It follows that only the NF condition has each adjective appearing with each of the three Typicality levels (matching of the adjectives).  That is, the cells AF-typical and ANF-atypical have no data.  It gets worse when Category (FOOD/NON-FOOD) is included:  The AF condition has only food items in the atypical condition and only non-food in the medium typicality condition.  The ANF condition is populated exclusively by food items.  So out of the 18 possible combinations of levels, the 7 marked 'x' are the only ones represented:
	     Typ	   Med	Atyp
AF
  food			           x
  non-f.		      x

ANF
  food	 x	      x
  non-f.

NF
  food	 x	      x	   x
  non-f.

We think the authors' analysis is creative in attempting to expand the test of the Typicality question to the AF and ANF conditions and to the non-food items, but with so many gaps in the distribution of items among combinations of conditions, we think the model will be rank deficient and wonder whether it will be able to provide an accurate reflection of the effects of any factor, let alone the interactions.  It seems like the experimental design wasn't built to test anything other than the NF condition, and trying to do so is likely to lead to inaccurate estimates of effects.

There are also problems with the Category factor (food/nonfood).  First, the category was not correctly defined, with 2 of the food items not included in the food list (trauben and paprika were coded as non-foods).  Second, determining the effect of Category will be problematic, especially when considering its interaction with other factors. For example, among the non-food items, almost all of the Category*Typicality cells are empty (8/9). This seems like the kind of situation where even if the computer will return a mathematical solution for what the interactions of Category x Condition and Category x typicality and Category x Condition x Typicality are, one should not ask the computer for a solution to this model, because the information about interaction of Category with either other factor, and even about the main effect of Category, simply isn't there in the data.

Dependent variable: 
NOTE: the following discussion is based on the idea that the authors used f0 range within the adjective as the dependent variable as stated in their analysis summary, but according to the R code (bluebanana.R), in fact, adjective duration was used as the dependent variable.

The authors (maybe) use f0 range within the adjective (max-min) as the dependent variable, over the span of the entire adjective, defined by automatic alignment.  We like the authors' effort at checking the literature on German intonation to determine what patterns one might expect on these adjectives, and in principle, the choice of f0 range on the adjective seems fine.  Since the adjectives are matched across typicality conditions (within NF), the adjective is a good choice for the dependent variable.  However, we noticed substantial creaky voice in many of the tokens.  It is especially prevalent in vowel-initial adjectives, especially after vowel-final ""der"", as in ""der orangen"", but can also occur in ""den orangen"". Also, a few speakers just seem to use a lot of creaky voice in many locations.  The pitch tracker is not likely to do well with creak and will often give an extremely low value for f0 during creak (pitch halving).  Since usually part of the adjective is in modal voice, this will give an extremely large f0 range for most tokens that contain creak.  The authors' automatic alignment and measurement method doesn't seem to have led to noticing the creak or finding a way to deal with it.

Secondly, the authors seem to have left the pitch analysis range at Praat's default of 75 Hz - 600 Hz.  When the max is 600 Hz, the pitch tracker is extremely likely to have some pitch tracker errors that give very high ""pitch"" during voiceless aspiration noise or frication noise, where there is just enough of a repeating pattern for a brief bit for the pitch tracker to think there's voicing.  A method of automatically taking max - min over the span of a whole word is almost sure to get quite a few of the pitch maxima falling in these pitch tracker errors.  Restricting the determination of max pitch to voiced portions at least 30 ms from the onset or offset of voicing would help with this, or hand measurement of max and min would, or setting a narrower range for pitch analysis, perhaps with the max determined for each speaker's actual pitch range.  A process of checking outliers could also help.

We did not see any information in the analysis write up or R code about a verification step to ensure that the f0 ranges were reasonable.
","As noted above, we are surprised by the inclusion of the Condition (ANF/AF/NF) factor and the Category (FOOD/NONFOOD) factor, and concerned about the large number of completely missing cells.  Beyond that, the structure of the statistical model seems fine. 

However, we were surprised to find no statistical model for adjective f0 range in the R file (bluebanana.R).
","We agree with the decision to exclude items with labels filled in on the notes/error tier, as most of these contain production errors. However, we would like to note that we don’t think that information about errors/hesitation breaks/etc. were annotated in the AF and ANF conditions. For example, in JW_3, who had 9/30 NF trials marked as an error/hesitation break, none of the AF or ANF trials for JW_3 were so marked.  It seems unlikely to us that JW_3 only produced errors on the NF trials.  This introduces an inconsistency in how the different conditions were treated with respect to errors and points to the problems with including the AF and ANF conditions.

One might also want to exclude speaker JW_3 entirely, as this speaker had a very high proportion of stimuli marked as errors and seems to have struggled with the task.  However, since the actual error tokens were excluded, this seems like more a matter of preference, it could be fine either way.

We would also look for some method of excluding creaky voice from the f0 range measurements.  Hand-placement of measurement points could avoid this, or one could use a measure other than range, to avoid including f0 minima that reflect creak. 
","There are none, and we're fine with that.","The many blank cells in the design (as analyzed by these authors) and the possible inclusion of very large pitch ranges reflecting doubling from creak to modal voice are large concerns that make us worry about whether the findings are reliable, although we’d like to note again that the results do not specify if they are talking about adjective duration or f0 range.  Based on the contents of the R code (bluebanana.R), we think the results must only pertain to duration, which is surprising, given the explanation in the Analysis_summary.txt file, which focuses on pitch excursions and does not mention duration.

In addition, the problems with the categorization of the food/non-foods and with the assigning of typicality bins based on median instead of mean results in several differences between how we determine the data to be distributed among the cells and how the authors did.  Whereas we find this:

	     Typ	   Med	Atyp
AF
  food			          x
  non-f.		       x

ANF
  food	  x	       x
  non-f.

NF
  food	  x	       x	   x
  non-f.

The authors found this (which is incorrect):
	    Typ	  Med	Atyp
AF
  food			          x
  non-f.		     x	          x

ANF
  food	x	     x	          x
  non-f.

NF
  food	x	     x	          x
  non-f.			          x"
2022-05-12 12:46:03,2022-05-12 12:56:12,IP Address,184.64.221.169,100,609,True,2022-05-12 12:56:13,R_6QiDf1CyK69pmDL,,,,,51.0406,-114.0764,anonymous,EN,pseudopleuronectes_assasi,procambarus_maculosus,88,92,90,publishable with minor revision.,"The report is extremely well and clearly written.

It answers the research question.

It offers an adequate phonetic analysis. 

The choice and the execution of the statistical analysis largely support the phonetic analysis and the overall goal of the study.","The authors decided to analyze the data by submitting the data points to Functional Principal Component  Analysis (FCPA). The authors carry out a Linear Mixed Effect Regression (LMER) analysis of the received principal components. The LME models developed are well-formed and properly  motivated given the data structure.

The only problematic part of selecting the LME model out of the models received by using the backward elimination was that the models were not actually compared by using the anova() function in R. The authors trusted the step() function in R without checking the fit of each model against each other.","The authors decided to submit the timely-normalized F0 data points to FCPA. 
The received PC1 and PC2 were made dependent variables, while the typicality and sex were the independent variables. The two models also had by-speaker and by-word effects included in the models. 

The variables were very well suited for the LME models as the models support the type of exploration the authors intended to carry out.  ","The variables fit the models extremely well. First, the typicality is the variable whose effect upon the principal component the authors wanted to investigate and therefore it was an independent variable. In addition, it is well known that the speaker’s sex influences the F0 levels, so the two (typicality and sex) were the two independent variables explored. 
","The statistical models were very well structured and executed. ","All the choices to exclude or not exclude certain subsets of data were well motivated.

First, the authors excluded 2 files because of the error in the pronunciation of the target item and the loud noise. 
Second, the authors excluded all the ANF and AF trials. The authors did not explain why they excluded these trials, but we understand that there was an unequal number of tokens in each condition.The authors excluded the adjectives in the NF trials from the analysis because they expected the greatest F0 modulations in the nouns per typicality.
Third, the authors excluded the items with fewer than 5 F0 data points after the MAUS segmentation, which yielded a sufficient number of data points for  the analysis.
","All the F0s were timed normalized. F0 normalization is a standard procedure in many acoustic analyses, so in general the authors’ decisions on the transformation of the data was on target.","Very succinct and precise analysis corroborated with previous research.
Excellently written report, well-formed statistical analysis and well-motivated phonetic
analysis.

The study was not anonymized! 

Certain parts of the analysis were not sufficiently described or even not thoroughly carried out, such as the FPCA, and the pruning of the models.

The authors could have explained why they did not include in their phonetic analysis other acoustic properties such as duration and intensity."
2022-05-12 13:49:25,2022-05-12 14:06:16,IP Address,129.107.80.47,100,1011,True,2022-05-12 14:06:16,R_3lW8MoANmG1x1Oy,,,,,32.7185,-97.1432,anonymous,EN,varanus_eulophotes,saron_pictus,90,90,90,publishable with minor revision.,They selected one dependent variable a priori and the models tested whether it differed across typicality conditions. Analysis was clear.,"Models were appropriate for testing the dv, though additional justification for hierarchical components of model structure would have been nice. ","Predictors were appropriately chosen for the dv and research question. ","Variables included in model were appropriate. ","Would have liked to have some additional prose justification for hierarchical components of the model structure. ","trials that had ""some sort of hesitation or error"" were excluded. We think this is appropriate, however, cannot evaluate it as these criteria are not defined. What counts as hesitation? What counts as an error? Where these equally distributed across conditions? etc.","The models was fit using log normal likelihood, which better fit the data. However, the pattern of results did not change based on this transformation. ",
2022-05-12 15:35:54,2022-05-12 15:40:45,IP Address,132.181.212.226,100,291,True,2022-05-12 15:40:45,R_9REV16BKIPFZf45,,,,,-43.5379,172.6151,anonymous,EN,alosa_atun,paralichthys_undulatus,75,60,55,deeply flawed and unpublishable.,The author(s) didn't control for word length and there were no interactions or slopes in their model. They treated the word types as independent in the analysis which means there's nothing in the model that links the adjectives to the noun.,We believe LMER is appropriate for this analysis.,We found the choice of variables suprising as there was nothing in the model that links the adjectives to the noun.,"The choice of investigating semitones within the word as opposed the relationship between adjective and noun was surprising. This method could be justified; however, it is hard to assess this based on the information provided. ",There were no slopes or interactions in the model.,"We believe subsetting the data for the NF condition only was appropriate. While hesitations and pauses were removed, the lack of further outlier removal was inappropriate.",There were no transformations with the data.,"The report and code was straightforward and easy to tunderstand. However, it seemed as though the author(s) did not understand the way the typicality rankings were based on the relationship between the noun and adjective, and treated the typicality rankings as applying separately to both words and adjectives (for example, rather than ""blue banana"" being given an ""atypical"" rank, and ""yellow banana"" a ""typical"" rank, the author(s) analysis gave all instances of ""blue"" an ""atypical"" rank, all instances of ""yellow"" a ""typical"" rank, and ranked some instanes of ""banana"" as ""atypical"" and others as ""typical"" without reference to the adjective."
2022-05-13 00:06:57,2022-05-13 00:08:48,IP Address,83.135.242.64,100,111,True,2022-05-13 00:08:49,R_2dmql8FVEz3W5op,,,,,50.973,7.1754,anonymous,EN,petauroides_fistulator,epinephelus_aztecus,90,60,75,publishable with minor revision.,"We find that concerning the phonetic analysis and the resulting F0 values, one is commonly well-advised to use semitones instead of Hz values – especially when working on data of multiple speakers. However, this is the only minor comment we have on the phonetic analysis part.
Concerning the statistical analysis, we have little experience with running path models ourselves. Considering relevant sources, we find that the statistical analysis appears to be conducted in a meaningful manner. We wish to note that the description of modelling the fit indices including the modification of the same could (and likely should) be formulated in a more straightforward way. That is, with the current version, for laypersons (such as us) the relevant paragraph sounds a lot like hacking your way towards a well-fit model, even though the procedure described is well-grounded in statistical theory.
In a similar vein, we find that the amount of post hoc testing appears to be excessive at first. After the initial path model is fitted, several other models are fitted to account for speaker-individual differences and data that is rather out-of-scope of the overall research question (as given by the MSA coordinators), among other things.
As to our understanding, in all their models the authors do not include any random effects (or random effect like) structures for item (= the word for which the adjective was (un)typical), nor for adjective, nor for a combination of the two. We find that different combinations of adjectives and nouns might all be rated (un)typical; however, certain combinations might be more (un)typical than others. This is also reflected in the typicality measures provided by the MSA coordinators – a variable which is disregarded in all present analyses.
We thus find that the technical part of the statistical analysis is overall well-done. However, considering the theoretical underpinnings, i.e. which variable structures and which variables to work with, we find room for improvement.
The overall rating is the mean of both other ratings. We chose ‘publishable with minor revisions’ as we do not find any insurmountable issues with the statistical analysis, but would advise for creating further models to see whether the aforementioned structures and variables bring about changes in the model estimates. It goes without saying that such results are to be presented as exploratory post hoc findings; they are not to be confused with confirmatory analyses.","As mentioned above, we cannot speak in detail towards the path model part. Yet, after consulting relevant sources, we find no issues with the statistical analysis type. Similarly, using mixed-effects regression, we find no issue with the choice of statistical technique.","The authors make use of several dependent variables: duration of the determiner and an adjective, intensity mean, intensity range, pitch mean, pitch range. These measures are motivated by previous research; thus, we have no issues to raise at this point.
As independent variables, typicality and focus are used. Typicality being the predictor variable of interest, including it is straightforward and requires no further motivation. Focus was included to investigate acoustics when the speaker is not faced with a need to facilitate disambiguation even in untypical adjective-noun combinations.
As briefly mentioned above, random effects are not considered in the main path model. Even though a random effect for speaker is explored in further analyses, we would have wished for an inclusion in the main model as well.","We find typicality and focus to be meaningful predictors to answer the present research question as these variables are directly related to the target items. However, we would have liked to see some sort of random effect variable for adjective and/or noun combinations, as well as the use of the continuous typicality variable.
Condition was used as variable as well, even though the authors missed mentioning it in the relevant part of the questionnaire. This is probably due to condition being used in a post hoc analysis. Nonetheless, it is used in a meaningful way as the aim was to compare the three different conditions (ANF, NF, AF).","We find the structure of the presented models to be overall suitable for answering the present research question. As mentioned before, a more sophisticated random effect structure is preferable.","Excluding any observations that had comments in the “Notes” tier of the TextGrids is a reasonable choice. If such observations were without any issues to begin with, there would be no comments. Additionally, trials with issues during the acoustic analyses were disregarded; this also is a meaningful choice.","Typicality and condition were treatment-coded; a straightforward choice we have no issues with.
One point we wish to make is that we assume that working with semitones instead of Hz is the better operationalisation for working with pitch data. Thus, we would have liked this team to work with semitones as unit instead of Hz.","We find the overall analysis well done. However, it feels as if only the first part, i.e. the main path model, was planned beforehand. All further analyses, i.e. which we call ‘post hoc’ in our review, appear to be highly explorative and results of chance/circumstances rather than prior planning. Of course, exploring data further in post hoc analyses is not to dismiss; however, we feel that it must be more clearly stated at which point the main analysis ends and the additional (post hoc) analyses begin. It might be the case that we got the wrong impression here and all presented analyses were indeed planned beforehand; if this is true, the presentation of analyses and results and its underlying structure should be phrased in a more comprehensible way."
2022-05-13 02:03:31,2022-05-13 02:12:48,IP Address,194.254.61.42,100,557,True,2022-05-13 02:12:49,R_8hM8lraapYfvyKt,,,,,48.8323,2.4075,anonymous,EN,linckia_nattereri,ctenosaura_limax,80,70,75,publishable with minor revision.,"The analysis was overall convincing, and the statistical model used was satisfactory, however, there were some issues of replicability. The description of the analysis and the scripts lacked details and could have benefitted from more clarification. ","The authors chose to use a multinomial logistic regression model which seems appropriate for predicting multiclass models such as the current one with three possible discrete outcomes (atypical, medium, and typical). ","A subset of the full 88 features was selected based on their relation to voice source, but the full features were also kept for a separate model to provide a source of comparison which appears to be a good idea. Because multinomial logistic regression assumes that collinearity is relatively low between variables, it seems wise that the authors inspected the variables through a correlation matrix and chose to keep only 10 features in the models. 
Even if a reference is cited for further detail on the measures that were used, it might be beneficial to at least have a full list of measures obtained in the paper itself.  ","The 10 variables that ended up being included seems to be highly suitable for the analysis as they are deemed more interpretable and familiar to the linguistics community. 
The choice to exclude MFCCs as variables in the main model because they are less easily interpretable is debatable. Solid interpretations can also be drawn from MFCCs.  
Variables were suitable but only the most important one (mean F1 relative energy) was discussed in the results, there was no interpretation drawn from other variables which may also have contributed to the classification. It would be interesting to see if other variables had the same or different weights across the 30 iterations of the model. It would be helpful to show the inspection of the results within the script. 
","Cross-validation to prevent overfitting the model so each model group has 30 trained models for each speaker. Structure is satisfactory but the correlation matrix should have been included. ","The authors chose to focus specifically on the noun focus condition of the provided data, which is a satisfactory choice but was not justified in the paper. ","Multiple transformations (mean, variance etc.) but in the end, the variables that were chosen were based on their non-correlation and ease of interpretation. ","The scripts could benefit from more annotations and visualization. They lack the required package information, for example. We weren’t able to run the code for forced alignment or for the measures obtained with OpenSMILE – in other words only the statistical analysis itself was replicable. 
Furthermore, the stats script is lacking details as well, it would be helpful if it were annotated. It’s hard to understand was “pos_variables” and “neg_variables” are and they are not explained in the paper. The correlation matrix is not included and the multinomial model did not converge, so there is a potential interpretation problem. 
"
2022-05-11 09:20:53,2022-05-13 05:14:14,IP Address,80.135.123.109,100,158001,True,2022-05-13 05:14:15,R_1j9XgAceK14OkGQ,,,,,48.1836,11.5754,anonymous,EN,procambarus_maculosus,ceratophrys_elephantotus,25,55,40,publishable with major revision.,"The idea of using machine learning for classification is certainly interesting and is the main asset of this study. The main issues with the analysis are (a) data processing/selection and (b) a general lack of information that would have provided the necessary insights in order to understand the author’s intentions.
Regarding (a): 
- ASR introduced more errors than the original orthographic transcription of the sentences and the clean-up of the errors was probably incomplete
- Since MAUS is based on the ASR-computed orthography, many of the segment boundaries placed by MAUS are probably also wrong
- The selection of “phones” contains unstressed vowels even though these were supposed to be excluded according to the author (e.g. the second /a/ in “Paprika”)
- Why was /i/ amongst the phonemes selected as a phone in “Möhre”?
- The data selection included both the colours and fruits, i.e. both adjective and noun, even though only the noun was focused (NF condition)
- What exactly was difficult about extracting f0 automatically? Why are there (according to the first point on page 4 of the result) so many missing f0 values? If f0 was extracted as a signal for the whole phone, which part of the signal was a variable in the analysis?
Regarding (b):
- The hypotheses specify the way in which acoustic parameters indicate stress in German. However, no connection is made between 'focalisation' and 'typicality' (e.g. is the atypical or the typical condition considered focalised?)
- Which effects of typicality on the three acoustic variables (F0, formants, duration) are expected/hypothesised? That is, are atypical or typical colour+fruit combinations marked by “longer duration, higher f0, and higher loudness” (from section 3)? Also, loudness is not measured and there are no predictions at all for the formants that were measured.
- It remains unclear whether the results reported in section 7 go in the expected direction for each of the typicality categories. It is only reported which variables seem to improve (or bias) the classification algorithm.
- In the conclusion (section 8), the influence of the unbalanced dataset (section 2) is no longer mentioned.
- Parts of the report mismatch the R script, e.g. the conclusion sounds like the measurements referred only to the adjectives, not the nouns (but both were used according to the script).
","The idea is indeed good and to some extent original, but more detailed analyses and explanations would have been necessary (see answer 9)","There is no real „structuring“ of a statistical model, since a classification algorithm decided which variables from a given list were good predictors for typicality. In this respect, the  author chose a series of acoustic parametres like f0, formants, vowel duration, but this choice was only poorly justified by the report (sometimes also admittedly by the author, see page 3 of the report, last paragraph). In the method, formants are extracted that are not mentioned in the hypotheses. ","The variables are suitable, although a longer explanation of why the variables “target_name” and “phone” introduce a bias into the classification would have been necessary, i.e. why was the classification rate 100% with these factors, but without only 56.7%?","Internally, all machine learning algorithms do a lot on their own; the external structure (i.e. the choice of variables, which was discussed before) was instead (and had to be) provided by the author. 

However, the members of this team are not very familiar classification trees and cannot really judge on this. For the rest of the evaluation, please see previous answers.","The author excluded utterances carrying annotated errors. This can be considered as a good quick choice in order to avoid noise in the data. 
It is understandable that missing values in F0 could not be manually excluded  because of matters of time, although the analysis (as the same author admits) would have benefitted from it.
It was a good choice to only examine NF, but it is unclear why both adjective and noun were included, and not just the focused noun. The analysis also included unstressed vowels even though these were supposed to be excluded (see answer to Q9 for further points on data selection).
","A sort of normalisation of F0 would have probably be useful in order to avoid possible biases due to differences between women and men.
Similarly, phone duration could be biased by overall speech rate (although this effect might be negligible). ","Just our opinion: I’d suggest the author, if needed, to re-write the report, since there are so many sentences which we found ambiguous. In particular, many parts were not described in any depth, so that the procedure, which could be also interesting to a reader who’s not familiar with it (like us), is not that clear (even when looking at the script, too, some parts were missing or not enough described in the comments).
In 2 it is stated that the data is not balanced in terms of speaker sex. In the given dataset, however, there was no information about speaker sex: Where does that classification come from? Literature references would be helpful when formulating the hypotheses. "
2022-05-13 06:59:53,2022-05-13 07:02:47,IP Address,5.71.20.168,100,174,True,2022-05-13 07:02:48,R_1pDopg6VRxLnMli,,,,,51.9367,-0.51,anonymous,EN,stygobromus_tyraica,trachurus_riukiuensis,75,30,50,publishable with major revision.,"We found the segmentation protocol to be generally rigorous, although no result is provided (qualitative or quantitative) for the inter-segmenter reliability check. The Praat script used to obtain the phonetic measurements is not provided and, thus, could not be reviewed.

While the statistical analysis followed a generally accepted framework, there were issues with how models were formed, meaning that the resulting estimate of uncertainty is very likely inflated. Our rating for this part is thus a reflection of what we consider a flawed and somewhat misleading analysis that is likely to produce an unreliable estimate of the relationship between the variables.",We consider the use of linear mixed-effects modelling to be appropriate.,"We found the process of model selection to be generally appropriate. However, the widespread issues of model convergence and singular fits suggests that another approach (e.g. Bayesian regression) might have been more appropriate for the dataset used here.","Duration and peak f0 are certainly relevant variables to include, although we believe that they are too reductive for the research question. For example, only including peak f0 ignores dynamic prosody, which might be captured by including also minimum f0, f0 range, f0 slope, or some global quantification of the overall f0 contour. Also, no measures of intensity are included, which is surprising given that the team has specified that “the effect would manifest as a prosodic difference.”

We strongly disagree with the inclusion of  “noun” as random effect, for reasons outlined in Q13.","We are of the opinion that the final models were not appropriately structured. The inclusion of noun as a random effect is inappropriate for the research question: nouns are mutually exclusive over typicality conditions, thus including a random effect for noun effectively offsets the main effect of typicality. Indeed, using a by-adjective instead of by-noun random effect as a proxy for a by-item effect results in significant main effects for the two duration models.","We consider excluding trials marked with “error” or “hesitation break” to be an appropriate choice. The exclusion of all data from speaker JW_3 is perhaps questionable, but justified clearly in the text.

The method of outlier removal, while not common in the field, appears to be justified.","No transformations were performed. We believe that this is appropriate for f0 but inappropriate for duration, which should ideally be log-transformed before statistical treatment in a linear model.",Inclusion of the Praat script used for obtaining the phonetic measurements would have been appropriate.
2022-05-13 06:24:51,2022-05-13 07:43:35,IP Address,72.75.217.214,100,4723,True,2022-05-13 07:43:36,R_1pSDZu8PEoaaIRn,,,,,42.9767,-78.7959,anonymous,EN,sphyrna_ellioti,pseudopleuronectes_assasi,80,75,77,publishable with minor revision.,"Overall this was an appropriate analysis to address the experimental question, and most of the decisions are clearly explained. There are some decisions as to phonetic measurements and statistical analyses that I have small concerns about. For example, about the necessity to include sex as a fixed effect if one can already include subject as a random effect. ","The choice to use multi-level models and model comparison via ANOVA is appropriate. While not the most cutting-edge analyses, they follow the standards of many of our fields. Some common issues with these methods arose for these analyses, such as convergence issues, but the decisions made to cope with that seem reasonable and I doubt they would change the overall conclusions.","I appreciated the clarity with which the authors presented their hypotheses and justification for their models. I think overall the models are well-justified and -reasoned. ","The dependent and independent variables make sense, although I have doubts about including Sex as a fixed effect in models that already include Speaker as a random intercept.","The structure of the statistical models are reasonable. Given that these were multi-level models, there were some compromises that had to be made for the models to converge. This is to be expected, but the choices they made to aid convergence were standard and I do not believe they impact the conclusions.","Data exclusion criteria are clearly explained and well-justified. I do wonder whether it's necessary to exclude everything but the NF condition for some of the typicality analyses. I think it's reasonable to do this given the experimental design, though.","I was initially concerned about the conversion of F0 from Hz to semitones. It seems like it might introduce room for error, but the fact that the baseline was a constant number rather than a measured value, and that the experimenters found the same overall pattern when looking at Hz addresses my concerns.

I did notice that Duration was not log transformed, which is a fairly standard transformation to perform. I think there is good reason to log transform Duration values for these types of analyses, although I expect the patterns will remain the same.",I appreciate the quality of the writing of this write-up. Presenting relevant background literature and the thought process leading to the analyses provided me with good information when evaluating the analyses.
2022-05-13 08:37:02,2022-05-13 08:38:47,IP Address,128.240.225.38,100,105,True,2022-05-13 08:38:47,R_1DoFVOe5IpCnFje,,,,,54.9836,-1.5895,anonymous,EN,haematopus_fossor,lasionycteris_altavela,40,70,60,publishable with major revision.,"Phonetic analysis: 
This analysis looked at maximum intensity over each entire sentence.
Intensity is a relatively inconsistent cue, compared to fundamental frequency or duration, to prosodic focus and stress. It is not clear from the methods document how precisely the recordings were made, but based on impressionistic auditory inspection of the recordings it appears that the participants did not necessarily have a constant distance between the mouth and the microphone. This makes any estimation of intensity inherently unreliable.
There is also no guarantee that the intensity peaks occurred during the target noun phrase, rather than elsewhere in the sentence.
Statistical analysis: 
The model overall appears to be well-constructed and well-though-out. The checking of model assumptions is an excellent inclusion. However there are some causes for concern.
The description of the model mentioned a random slope for typicality, but this does not appear to have been included in the model.
The statistical model has a random effect of “target_name”, which corresponds to the noun’s identity, but it does not have a random effect of “target_colour”, which corresponds to the adjective’s identity. Ideally this should have been justified or explained. (As it happens, inclusion of this random effect leads to a singular fit, as the adjective does not appear to have a substantial effect on intensity. Nevertheless this should have been justified.)
The statistical model appears to compile with a warning of a singular fit. This appears to be due to the random intercept of speaker:item, which has a variance of zero. Removing this random intercept leads to no convergence error, and does not substantially alter the pattern of results.
The data included was apparently only the NF trials. However, examination of the data shows that there’s a mismatch between the “label” and “condition” columns, such that the “label” column incorrectly lists every token as “NF”, even though many are actually “ANF” or “AF”. This might be a bug in the R/Praat code that generates the “label” column. (The “condition” column appears to consistently match the actual data in the textgrids.) Fixing this problem (by changing the subset code to refer to the “condition” column rather than the “label” column) does not appear to substantially alter the results.","A linear mixed effect regression model was used to analyze the data, which is appropriate.","The choice of IVs was reasonable, although more attention could have been paid to the singular fit issue. The choice of DV was questionable – as discussed above, there are better measurements (such as f0 or duration) that could have been made, and done so more carefully.",See above.,"The statistical model was fine in principle, although more attention could have been paid to the singular fit issue.","The analysis (ostensibly) included the data from the NF focus conditions, and excluded tokens marked as “error”. This is reasonable (aside from the bug which caused non-NF tokens to be included in the data.)",No transformations were made. It may have been helpful to center the intensity data around zero before entering it into the model but it is unlikely that this would have affected the conclusions in a meaningful way.,No further comments
2022-05-13 07:33:45,2022-05-13 08:39:08,IP Address,185.108.105.148,100,3922,True,2022-05-13 08:39:08,R_32LVz3GZ2DEvRJj,,,,,55.8306,-4.2593,anonymous,EN,trigonias_lachneri,trapezia_cantonensis,75,50,60,publishable with major revision.,"Overall, the phonetic analysis was done very well, and we appreciated the effort taken to complete the segmentation manually. 
However, there is some concern about the choice of GAMM analysis regarding whether it is an appropriate choice for answering the research question. The research question itself would demand an analysis that gave more absolute results, rather than how outcome variables changed over time (see below discussion on GAMM).

Another concern was the treatment of some of the variables which did not have random slopes. In addition, the choice of including ‘adjective’ as a predictor instead of as a random intercept is not clearly justified. 

A major concern is the inclusion of ‘non-food’ items as one of the levels of typicality. The reason for this is not provided and predictions about it as a variable are not provided, further questioning its inclusion. It is difficult to take the results at face value without post-hoc tests to see if any of the effects are the results of an influence of the ‘non-food’ items. Further, ‘non-food’ was not included as part of the duration lmer model and was found to be non-significant – why a difference in choosing to disregard ‘non-food’ here? 

Results for variables other than duration seemed to be confirmed from visual inspection of graphs rather than the results of the GAMMs themselves. It is also unclear how the graphs are arrived at. The method of confirming results is not consistent or reasoned. While we generally agree with the interpretations of the results, one of the author’s statements raised a question.

There appear to be editorial errors in several sections such as the F0 section suggesting that results from another section were copied but not adjusted. 

Finally, a single effect size and confidence interval was not reported, which was one of the objectives of the analysis. This is problematic as a final answer to the research question is not provided. Despite some of these issues, within each individual analysis the author had a principled approach. ","Linear mixed models seem appropriate for the continuous duration variables analysed, considering that multiple participants responded to the same stimuli and that the same words (for adjectives) occurred in different conditions.

The choice of a GAMM analysis was not well documented. We consider that its output might provide information, which is not well aligned to the research question. The research question is about the overall difference (zero, positive or negative) between the dependent variable levels, rather than the change of outcome variables over time. A GAMM analysis shows whether the dependent variable has a non-linear effect on the outcome variable, in this case with respect to normalised time, without directly addressing the author’s hypothesis of higher or lower f0/intensity/formants. Given how the author specified their hypotheses and the research question, the choice of a GAMM analysis is questionable.

In addition, this information is interpreted secondarily from the plots and the method of arriving at the plots are unclear. 

Finally, the task was to provide an effect size and CI and the author did not know of a way to do that with GAMM. Therefore, we think the choice of analysis type was not appropriate.","The choice of variables was based on the author’s prior knowledge of stress in world languages. While they were reasonable in our opinion, the choice was not motivated beyond that.

We think that duration should have been included as a random slope per speaker and token. Also, we are unsure why adjective was included as a predictor in the GAMM analyses and not as a random intercept.","While we agree with the exclusion of non-foods from the duration analysis, it is unclear what the benefits were to include them in analyses of other variables. The author explicitly states that they have no expectations for the non-food items and in the norming document the non-foods are explicitly described as agnostic to typicality (hence separate from the RQ).",We think that duration should have been included as a random slope per speaker and token. Why was adjective included as a predictor in the GAMM analyses and not as a random intercept?,"The exclusion of ‘non-food’ for the duration model but its inclusion for all the other variable models is not justified and could have had an influence on the results. However, it is unclear what influence, if any, ‘non-food’ had on the effects found as no post-hoc tests were conducted. 

We also noticed that there were no representations of atypical instances of “orange” in the plots and were wondering why that data are missing.","We believe that given that the RQ is interested in within-speaker variation, the lack of transformations is not an issue. ","In several sections the author seems to have made editorial errors in their reporting and interpretation of the results, citing an outcome variable from a different section or comparing a level of the dependent variable to itself, leading to lower clarity.

We think that not reporting a final effect size and CI is an issue, given the task.

While we agree with the interpretation that typicality having an effect on formants could be a false positive, the author’s interpretation of the results raises a question. They suggest that the results are a false positive because the direction of the effect is different from what was predicted. Would a result in the predicted direction merit being considered a true positive, given the same hypothetical power of the analysis?

We also noticed that the analysis of the previous team we reviewed resulted in lower intensity for atypical tokens, which is the opposite of what was found here. This is unexpected since everyone analysed the same dataset. "
2022-05-13 09:28:48,2022-05-13 09:30:13,IP Address,80.7.115.73,100,84,True,2022-05-13 09:30:14,R_2fIDsjBxLbzatSB,,,,,53.9573,-1.0837,anonymous,EN,chelonia_brummeri,aratinga_lugubris,15,15,15,deeply flawed and unpublishable.,"We have serious reservations about the structure of the model, including the choice of DV.",The choice of analysis type (Bayesian models) is in principle ok but the model structure is odd.,"We don’t understand the rationale of choosing typicality as the DV, when that was the factor which was deliberately manipulated in the experiment. 

Given this choice, though, we do not understand why random intercepts for speaker and adjective (i.e. colour) were included: typicality does not vary by speaker. If the model were constructed to predict speech error rate as a function of typicality as a fixed factor, the chosen random effects structure might make sense. ","No acoustic phonetic analysis was performed. The report says that a decision was made not to perform any such analysis, based on the observation after listening to the data that there were no obvious acoustic differences. Despite this, Hypothesis 1 was included in the report, though rejected without any acoustic investigation. It could perhaps be rejected on the basis of the results of the auditory phonetic analysis, but the methods and results of that analysis would need to be documented in more detail (e.g. what criteria or phonetic features were consistent with classification of adjective tokens being ‘similar’ or ‘no different from each other’)? 

The decision to look at speech errors was an interesting choice. Hypothesis 2 is formulated in a circular way, though, which undermines the analysis. We also do not have very clear criteria of what counted as an error (e.g. how long did a pause have to be to count as a ‘longer pause’; a much stronger approach would be to measure the duration of pauses as a continuous variable). The decision to view the data in this way reduces power, as there are only 81 errors across (819 non-errors) so there is not much data to look at, resulting in the very wide credible intervals.

Similarly, the criteria used to identify an adjective as plus/minus ‘emphasized’ are not described (other than in footnote 1). The project brief was to look for evidence of phonetic modulation of utterances; in principle that could be detected using auditory impressionistic analysis, but at the least some description and discussion of the types of linguistic exponents of ‘emphasis’ is needed. We do not understand how it was possible to reliably or consistently detect presence/absence of emphasis in the same tokens that had previously been described as not being hearably different from each other.","The structure of the model seems counter-intuitive. It has typicality as the DV with number of speech errors (or presence/absence of emphasis) as a predictor, rather than trying to predict the probability of a speech error or of emphasis based on the typicality condition (which would be a closer fit to the project brief, in our view). ",N/A,N/A,"There is some evidence of hypothesising after the results are known (HARK-ing). The authors are at least very transparent about switching to explore a new post hoc hypothesis, but the presentation does at times seem like they already know the answer they want (typicality should modulate *something* in speech) and are looking for the question that gets them to that answer (i.e. speech errors). The transparency is good, but it is a questionable practice, and we did not get the sense that this experiment was designed to test the hypothesis that typicality relates to speech errors. It would at least need to be followed up by a confirmatory analysis."
2022-05-13 09:19:51,2022-05-13 10:08:40,IP Address,62.92.112.130,100,2929,True,2022-05-13 10:08:41,R_RaU190KzIkgWw7v,,,,,59.955,10.859,anonymous,EN,saron_pictus,swiftia_ruber,75,75,75,publishable with minor revision.,"The analysis was well done in general, but it did seem slightly forced given the original structure of the data. It is understandable why data had to be excluded to fit the analysis decision, but it unfortunately left a very small number of items that makes conclusions slightly less generalizable that it would have otherwise been.

If these results were to be replicated, more items would be required and time points along an entire pitch contour would be preferable (although this would require sentences to be composed of sonorants.)

As the main research question was concerned with the NF condition, we will limit our comments to typical and atypical conditions within that condition.","Fo modelling pitch contours, the use of B-splines is entirely appropriate. It was unclear why they were modelled as two separate models (for the NF conditions), although we profess that we are unsure whether or not this needs to be the case in brms. In other packages (e.g., mgcv), these curves would have been fit in the same models as a by-factor smooth and the addition of random effects would have had the advantage of partial pooling.",N/A,"Plotting the fundamental frequency over time was appropriate, although controlling for changes in experimental time (trial) may have also been appropriate.","The structure of the model seems appropriate, although it is generally better to fit the data in a single model to take advantage of partial pooling. We understand the computation time issue, but the authors should note that committing pseudo-replication makes us overconfident in conclusions that we draw. There was no difference in this instance, however (we fit a different b-spline model using penalized maximum likelihood with maximal random effects and the interpretation was similar.)","It was unfortunate that the majority of items were excluded, and it would have been nice to see an analysis that took advantage of more of the data. Based on the analysis chosen, however, the exclusion seems appropriate and necessary. ",The data seem to have been transformed in the appropriate way to facilitate comparison across speakers.,
2022-05-13 11:09:17,2022-05-13 12:12:47,IP Address,141.201.219.162,100,3809,True,2022-05-13 12:12:48,R_3PhMstSa3pbMB5d,,,,,47.8008,13.0443,anonymous,EN,anomalocaris_ornata,comanthina_maculatus,65,35,50,publishable with major revision.,"We rated this project with 50 as we do not believe this is enough data in the subset used to substantiate the claims made in this report. The phonetic analysis seems fine, but we have reservations about the statistical analysis. The methods seem rather vague. The data visualisation was however very well done. What also needs to be more explicitly stated is how these variables were actually measured, was it at mid-point? means? ","T-tests were used as the data was found to be non-normal, these are however also parametric tests and are bound by the same assumptions of normality. While t-tests may be fairly robust, there are other more appropriate tests available which would also allow for the inclusion of more variables. The non-normality is essentially not addressed here regarding the use of t-tests.","Variable choice is not addressed in detail, referencing for these variables is not sufficient and it is unclear which references (and what parts thereof) have informed these decisions. Variable selection was limited by the choice of t-tests.",See above,"Again the structure was heavily restricted by the use of t-tests. The T-tests do seem to have been carried out correctly, with the correct structures, but see our previous comment about normality.","Limiting the data to a subset is understandable given the work it takes to code by hand, but it is problematic here. In our experience, even 30 speakers was on the lower side of what we felt was necessary for this kind of analysis. At a minimum, it would be necessary to state how these 15 were selected. It might have been interesting to include the colour adjectives too (it is understandable that this was not included because of time). ","Great that residuals of the model were checked, however transformations that are able to deal with non-normality might have been a better route than opting for t-tests. With transformed data the assumptions of your model might have been better met.
","The methods need to be clearer and more explicit. Even with the material that has been provided, it would be very difficult to replicate this study in the way that it has been done here. For example, how did you run your statistical tests? Was it with R? SPSS? Something else? What normality test was conducted?

The visualisation was a clever way of showing the results of the variables!"
2022-05-09 10:51:16,2022-05-13 12:14:50,IP Address,184.171.61.220,100,350613,True,2022-05-13 12:14:51,R_1oulepeK7Iziv5c,,,,,44.0197,-123.1008,anonymous,EN,epinephelus_aztecus,varanus_eulophotes,70,90,80,publishable with minor revision.,"Analyzing mean f0 as an indicator of effect of typicality was an appropriate approach. The use of Praat scripts to analyze the pitch contour of each observation followed by use of the Get mean function is a good method for determining mean f0.One issue is that the use of the Montreal Forced Aligner may have led to several observations containing acoustic information beyond the desired word boundaries. The team found 4 instances of errors when manually aligning 50 random utterances to check the accuracy of the aligner. The analysis does not make it clear whether utterances from each speaker were manually checked for alignment. It is possible that if 4 out of 50 utterances (around 8%) included extra acoustic information, several of the total 895 observations may have also included extra acoustic information. This extra acoustic information  may not be problematic since the outcome being measured was pitch and not duration, but it is unclear what effect this analysis may have had on the outcome.
As for their overall statistical analysis, there weren’t any major problems. They also provided diagnostics of their data before interpreting the model, which is a good important step. However, it would be better if they also include a random slope in addition to the random intercept for speaker that they did include in their linear mixed model. 
","Their choice of linear mixed effects models for statistical analysis was appropriate considering they only looked at 1 continuous dependent variable. ","They provided a good explanation with theoretical background as to why they chose to measure f0 as a cue to stress in German. They also explained well why they chose typicality as the only predictor variable. ","Based on the information from Kohler (2012) that pitch is the main cue for listeners to perceive stress in German, using mean f0 as the only outcome variable is a good choice. The research question is focused on typicality’s effect on speaker productions, so this team’s use of typicality as the sole predictor variable is also well reasoned.","Using linear regression mixed effects models for a single continuous outcome variable and single predictor variable is an appropriate statistical analysis. The fixed effect of typicality is appropriate. However, this team included random intercepts for speaker, but did not include random slopes, which are the norm in these models. If their team decides not to include random slopes on purpose, it is important to justify why that is.","This team mentioned excluding trials that did not have typicality ratings. This seems like an appropriate step since they were only focusing on typicality. ","The team provided a good explanation of log and inverse transforming the raw f0 values and how they still resulted in significant Shapiro-Wilks results. Since the transformed values were still not normally distributed, their choice of using the raw f0 values for their model was appropriate. However, it might also be useful to run a linear model with the centered variables, instead of the raw variables, to see if the output would be different.",
2022-05-13 13:18:36,2022-05-13 13:36:30,IP Address,77.161.90.97,100,1074,True,2022-05-13 13:36:31,R_s89697TGPHu2dFL,,,,,51.4361,5.4958,anonymous,EN,neosilurus_omanensis,haematopus_fossor,80,80,80,publishable with minor revision.,"We like that the authors chose to include several different dependent variables for their analysis, and also their attention to individual variation (as depicted in graphs) was appreciated.

It is somewhat unclear how the authors reached the conclusion that there is no strong evidence for an effect of typicality on speech production -- even though we agree that no strong evidence was found in the reported analyses. After all, they report one or two significant differences (albeit with p-values relatively close to .05).

On a related note, there does not seem to be any correction made for the evaluation of multiple different models to test the same hypothesis (e.g., FDR or Bonferroni). If such corrections were made, the conclusion that there is no strong evidence for an (overall) effect of typicality would gain more support.

Interestingly, one of the significant effects is in the opposite direction to what we would predict (typical adjectives being longer). Could that be a ""sign-error"", in Gelman's terms?","The LME model is suitable for the task at hand. (We used the same model.)
","It is not entirely clear why certain dependent variables are chosen (although most do make intuitve sense); we are also not entirely sure why two analyses were conducted, with two levels of typicality each.",See previous comment.,Structure strikes us as suitable (module earlier comments),"The authors note that “This hand-correction was only carried out for target trials, i.e.trials in the NF condition. Trials marked with “error” or “hesitation break” in the notes tier were not corrected and were excluded from further analysis.” This makes sense, but we should note that typicality data can be derived for the other conditions as well (not just NF).","The transformations that are applied (e.g., centering) are common practice.","The segmentation process is explained very clearly. We appreciate the fact that the authors manually corrected the forced alignments, and carried out a reliability analysis of their corrections. It is good that they also included instructions for each annotator."
2022-05-13 14:46:50,2022-05-13 15:04:11,IP Address,186.223.222.75,100,1041,True,2022-05-13 15:04:12,R_3dYaLc1EVeEaY8n,,,,,-21.9065,-47.8747,anonymous,EN,swiftia_ruber,pseudodax_euryzona,90,90,90,publishable as is.,The analysis is sound. Most methodological decisions regardind the phonetic and statistical analysis make sense given the problem at hand.,"The use of Functional Principal Component Analysis seems a good fit, given the problem at hand. The fact that the test material consisted of phrases differing in the number of words and the words differed in terms of metrical makeup, FPCA is a way of quantifying differences when there is no fine-grained alignment between contour and individual syllables.","Given the instruction to choose one dependent variable, analysing the contours using Functional Principal Component Analysis is a good fit, because the PCA's encapsulate overall changes in the shape of contours. Independent variables were a more obvious choice, given that teams were instructed to explicitly look for the effect of typicality on the f0 contours.","Given the choice of using Functional Principal Component Analysis, the inclusion of PCAs as variables in the statistical models are more or less obvious and suitable.","The model chosen was pretty straightforward: linear mixed-effects with the second PC score as the response variable, typicality as the fixed effect and a by-speaker random intercept. It is very appropriate for the problem at hand.","F0 contours were normalized using z-score and contours excluded when data points in it exceeded a threshold of z > |3|. The rationale for it is reasonable and the use of a threshold may have avoided the inclusion of f0 extraction errors. ","
F0 contours were normalized using z-score. This transformation is appropriate, given that the dataset was comprised of contours produced by different speakers and it would be a problem to aggregate all contours without any type of transformation. Z-score is a good choice to minimize f0 effects due to differences in speaker sex and other physiological factors that affect f0.",
2022-05-13 15:07:41,2022-05-13 16:27:20,IP Address,35.46.102.78,100,4778,True,2022-05-13 16:27:21,R_1KrpqNJzacbM5My,,,,,42.9178,-85.6995,anonymous,EN,arapaima_modularis,linckia_nattereri,20,70,40,deeply flawed and unpublishable.,"The main issue in this analysis is the phonetic analysis. It is probably irrelevant how good or bad the statistical analysis is, as it is based on unreliable data. There are at least three major problems with the phonetic analysis:
1) According to the report, the identification of word boundaries in NF trials was done entirely automatically. There is no report of visual corroborations or manual corrections. This is highly problematic, as automatic detection of boundaries is not always reliable. In fact, I randomly chose 10 NF trials from this analysis and I disagreed with boundaries in at least 2 cases. One example of erroneous boundaries is trial 23 in participant CG. All measurements derived from the analysis are compromised based on potentially incorrect word-level boundaries.
2) From the report, it seems that all trials were included. How did authors handle trials with errors, as reported in tier 5 by MSA coordinators? Also, it is expected that sometimes certain measurements cannot be used, as they are errors in detections. This is common, for example, when extracting f0. One way to safeguard against this is to visually inspect results, or use a quantitative method that singles out anomalous values. 
3) It is not clear why authors decided to include so many predictors. It really looks like a case of data fishing. Some phonetic predictors seem intuitively appealing, such as intensity, but others do not. For example, what is the theoretical or acoustic motivation to include F3 as a predictor? Authors should have defended their choices as to why they hypothesized each phonetic cue could serve as a predictor for typicality.  

","Appropriate, but used on a potentially unreliable set of data. ","The outcome variable (typicality, with three levels) is correctly chosen. 
As explained above, however, the set of predictor variables is a massive amount of acoustic features (n = 44) which are not appropriately accounted for. ","Predictor variables are not adequately introduced. It is not clear to this reviewer why authors believed these 44 predictors could explain typicality. ","Suitable. ","All data points were included, which I find problematic. At the very least, trails labeled as containing errors should have been treated differently. ",N/A,"For those predictors that did reach significant, it would have been useful to read an interpretation, even if brief, of what the results really mean. Why would typically between noun and adjective be predicted by these four (out of 44) phonetic cues, and not by the other 40? What does it mean in terms of speech perception, for example? Is this really relevant? Otherwise, the results may look like a mere statistical artifact (i.e., out of 44 variables, something could randomly be significant). Again, this problem goes back to the lack of explanations as to why these predictor variables were chosen in the first place. "
2022-05-13 18:02:04,2022-05-13 18:16:19,IP Address,104.28.50.170,100,855,True,2022-05-13 18:16:19,R_2XoN8OreLPIv1By,,,,,33.5716,-101.8545,anonymous,EN,anthracoceros_coronata,lycodes_bradfieldi,90,90,90,publishable with minor revision.,"The analysis overall is sound, and uses interesting independent variables.",The statistical analysis is sound.,"I am not very familiar with the AM and Mi measures, but they seem to be appropriate.","I am not very familiar with the AM and Mi measures, but they seem to be appropriate.",The model structure is sound.,"Only data with a value for typ_cat were included.  That means that data in condition NF was analyzed, but not condition AF or ANF, since typ_cat were not provided for trials in those conditions.","A Hilbert transform was applied, which seems appropriate.",
2022-05-13 15:20:19,2022-05-13 21:01:34,IP Address,71.104.44.228,100,20474,True,2022-05-13 21:01:35,R_2qEDRmg8BgaTOaG,,,,,40.4992,-74.4996,anonymous,EN,lasionycteris_altavela,gymnothorax_spinulosus,100,90,90,publishable with minor revision.,"Overall, their handling and analysis of the data were exemplary (both preprocessing of the audio files and the statistical analysis). Their pipeline was straightforward and I believe they made informed decisions. 

","I have no issues with their choice of analysis. ","Pitch and intensity were chosen according to the hypothesis and prior literature, but only intensity was ultimately included due to project stipulations. Typicality was included as a continuous variable, and speaker and target color were included as random factors—accounting for all possible sources of variance. 

The authors used a backward selection process to determine the best model, which is a reasonable thing to do. ","The variables seem appropriate for the hypothesis, and all relevant sources of variance seem to be accounted for in the model.","The structure of the model was suitable. They used the best fitting model found through backward selection. ","
The authors excluded measurements +/- 1.5 standard deviations from the median as outliers (108 measurements, or 5% of the data). I think this is a fair criterion, and the total number of excluded measurements seems reasonable.
","
Mean intensity over the critical vowel was normalized by subtracting it from the mean intensity over the entire stimulus sentence. This follows previous literature on phrasal prominence in German.

These transformations of the data seem appropriate. I think it was a good decision to segment the audio files and isolate the measurements to the vowel of the accented syllable of the critical adjective.
","Would have been better if they did did everything in one program instead of multiple programs. "
2022-05-13 20:59:32,2022-05-13 21:03:09,IP Address,73.217.118.234,100,216,True,2022-05-13 21:03:09,R_56cay170jCcXodj,,,,,39.9834,-105.143,anonymous,EN,nestor_idahoensis,chelonia_brummeri,80,100,90,publishable with minor revision.,"I’d like a missing data analysis for the files not surviving MFA or Fast Track, and the heuristics used to flag implausible MFA boundary placement (e.g., implausibly short) or acoustic measurements (e.g., implausibly high/low formant estimations). I also am hesitant to analyze intensity in this dataset, even when values are relativised at the level of the utterance, but these methods are better than analyzing untransformed intensity. A minor note that did not impact my rating: using a F0 range from 0-600 for Praat F0 extraction and then, later, the default FastTrack settings seems incongruent to me - either you expect high pitch in the target or you don’t. 
","The Bayesian linear regression was appropriate for the analysis because it allowed for the maximal random effects structure. The priors were appropriate as well. It would be helpful to include a line about how the priors were appropriate based on prior predictive checks. I also appreciate how a detailed decision criteria was included for determining a reliable effect. My only critique is to provide reasoning for the .025 criteria when evaluating whether the CI overlaps with zero. ","The authors describe that the acoustic variables were chosen because they are suprasegmental features. They use relative intensity because the recording conditions are unknown. More detail could be provided for the choice of variables. 
","I appreciated that they included syllable structure as a fixed effect. Using a relative measure of intensity allowed its inclusion. The formant analysis was appropriate.
",he structure of the statistical model was appropriate.," Focusing on NF is appropriate.","See above re: intensity - I also am hesitant to analyze intensity in this dataset, even when values are relativised at the level of the utterance, but these methods are better than analyzing untransformed intensity. 
",
2022-05-13 19:46:13,2022-05-14 01:01:19,IP Address,70.95.160.71,100,18906,True,2022-05-14 01:01:20,R_3njXsYhz15HHV9S,,,,,32.8595,-117.2124,anonymous,EN,genyonemus_evotis,dermatolepis_aculeatus,30,20,25,publishable with major revision.,"Phonetic analysis: the word boundaries and the Praat script seem to work. But it lacks details behind the f0 calculation - why choose parabolic? Why not restrict speakers' vocal range to get a more accurate calculation? How did they confirm that the f0 calculations were correct? Any irregularities seen in the data?
Statistics: The linear mixed-effects models seem fine for a single f0 point, but still, is f0 distributed normally? Why a linear assumption is warranted for this analysis? The scaling of f0 was also problematic.","The statistics method might be valid, however, the authors failed to offer enough justification and explanation on why the model they used would be able to address the questions that they posted.","What are the unobservable or unobserved confounders might affect/bias the results that are reported here? The author also didn't explain how the dependent variables are treated in the model: since they measured three different f0 values, what's the reason that they chose the maximum f0 at the end?","The author failed to mention, or elaborate on most of the important aspects of the measurements that they've taken. The author didn't explain what algorithm they picked, and what might be the problem with the choice, rendering their results unreliable or unable to be evaluated. Even if we could assume that the F0 measurements are valid, the authors didn't explain how maximum, minimum and mean values are obtained. Did they exclude any outliers? How did they deal with irregularities in the signal (low F0 regions where F0 is intractable)? How did they do normalization and why? (It is far from sufficient to just name the R function!) Without addressing these questions thoroughly, the measurements and analysis are likely to be invalid.",The authors didn't discuss the variable selection process and didn't do any exploratory data analysis to justify their model.,"It was not entirely clear why the authors didn't exclude the errors included in the original textgrids. When there's hesitation, it is likely that the prosody may be influenced by the pausing or artifacts, which in turn affects the f0 of the target phrase.
Also, no mention of excluding any outliers in the dependent variable makes the analysis less credible.","The f0 was scaled, but only subtracted from the mean. This is not standard practice to scale f0, given that f0 typically does not have a linear assumption and is not normally distributed. For example, they could have taken the log transform and then z-score it to normalize individual differences.",
2022-05-10 09:29:04,2022-05-10 09:44:08,IP Address,173.76.251.40,40,904,False,2022-05-17 09:44:13,R_3sstLbhhvwf8pHH,,,,,,,anonymous,EN,trachyphyllia_lappa,dunkleosteus_inscriptus,69,93,93,publishable with minor revision.,,,,,,,,
2022-05-13 09:01:26,2022-05-13 09:30:49,IP Address,80.7.115.73,40,1763,False,2022-05-20 09:30:54,R_2BrZXuiyNtLcJyA,,,,,,,anonymous,EN,chelonia_brummeri,aratinga_lugubris,15,15,15,publishable with major revision.,,,,,,,,
2022-05-06 22:43:29,2022-05-13 12:40:52,IP Address,128.223.174.30,20,568642,False,2022-05-20 12:40:55,R_26aHmtNxwe0zpkC,,,,,,,anonymous,EN,epinephelus_aztecus,varanus_eulophotes,,,,,,,,,,,,
