Q3,Q5_2,Q5_3,Q5_4,Q6,Q8,Q9,Q10,Q11,Q12,Q13,Q14,Q15,q6
dermatolepis_aculeatus,75,85,80,publishable with major revision.,"In general, we found this analysis to be appropriate, although we have issues with the use of the word 'normalization' with respect to the transformation of the values of the fundamental frequency. We would have expected a transformation that improved the comparability between speakers with differing vocal tract lengths, as is typically done in studies of vowels.

Additionally, while the use of linear mixed models with a Gaussian likelihood would be the default for the field, checking the assumptions of these fitted models shows they were misspecified, and that a different model may have allowed us to be more confident in any conclusions drawn (or not drawn).","The choice of mixed-effect regression is appropriate for the structure of the data. However, checking the residuals of the fitted models shows that they depart strongly from normality, indicating that the model is misspecified and interpreting the model parameters may not be wise.

The authors should have transformed the data or employed a different likelihood distribution instead of centring the dependent variable and using the default Gaussian likelihood.","In general, the variable was selected that directly answered the research question, which was appropriate.","The inclusion of typicality was important, given the research question.

It may also have been useful to account for the time course of the experiment by including Trial as a covariate along with a random slope for trial by participant.

Not including a random slope for typicality by speaker weakens confidence in the results of the model.","The model structure, in terms of the random and fixed effects that were included, seems appropriate. However, the authors describe random slopes that were fit that does not appear anywhere in the R code.",The criteria for exclusion were both reasonable and clearly delineated.,"Based on the code provided, the pitch was not scaled but only centred. This was also done for all observations together. While centring in this way can help with model interpretability, it is a linear transformation that should have no impact on the interpretation of the model or the findings.

The problem with this is that we are comparing speakers across a variable that is likely to differ from person to person, and because the data was not normalized within each speaker comparing pitch values across different participants may not be appropriate or meaningful.",NA,1
dermatolepis_aculeatus,30,20,25,publishable with major revision.,"Phonetic analysis: the word boundaries and the Praat script seem to work. But it lacks details behind the f0 calculation - why choose parabolic? Why not restrict speakers' vocal range to get a more accurate calculation? How did they confirm that the f0 calculations were correct? Any irregularities seen in the data?
Statistics: The linear mixed-effects models seem fine for a single f0 point, but still, is f0 distributed normally? Why a linear assumption is warranted for this analysis? The scaling of f0 was also problematic.","The statistics method might be valid, however, the authors failed to offer enough justification and explanation on why the model they used would be able to address the questions that they posted.","What are the unobservable or unobserved confounders might affect/bias the results that are reported here? The author also didn't explain how the dependent variables are treated in the model: since they measured three different f0 values, what's the reason that they chose the maximum f0 at the end?","The author failed to mention, or elaborate on most of the important aspects of the measurements that they've taken. The author didn't explain what algorithm they picked, and what might be the problem with the choice, rendering their results unreliable or unable to be evaluated. Even if we could assume that the F0 measurements are valid, the authors didn't explain how maximum, minimum and mean values are obtained. Did they exclude any outliers? How did they deal with irregularities in the signal (low F0 regions where F0 is intractable)? How did they do normalization and why? (It is far from sufficient to just name the R function!) Without addressing these questions thoroughly, the measurements and analysis are likely to be invalid.",The authors didn't discuss the variable selection process and didn't do any exploratory data analysis to justify their model.,"It was not entirely clear why the authors didn't exclude the errors included in the original textgrids. When there's hesitation, it is likely that the prosody may be influenced by the pausing or artifacts, which in turn affects the f0 of the target phrase.
Also, no mention of excluding any outliers in the dependent variable makes the analysis less credible.","The f0 was scaled, but only subtracted from the mean. This is not standard practice to scale f0, given that f0 typically does not have a linear assumption and is not normally distributed. For example, they could have taken the log transform and then z-score it to normalize individual differences.",NA,1
dermatolepis_aculeatus,92,93,93,publishable with minor revision.,"The authors used a linear mixed effects model with condition and typicality as independent factors, random slopes for speakers, and random intercepts for either noun, adjective, or overall phrase. This seems like an appropriate and concise statistical measurement.",The linear mixed effects model is appropriate.,The variables and structure are appropriate.,The variables of condition and typicality as independent variables are suitable since this is what the overall project is examining.,The structure of the model seems suitable as it included independent factors as well as random slopes to account for differences among speakers.,"They excluded data points that because of mispronunciations, false starts, and recasts as they might have affected pitch with regards to typicality. However, they did not exclude data points with hesitation breaks, as this would not affect pitch, nor did they exclude trials with mistakes unrelated to the noun-adjective pair. This seems to be an appropriate approach to data cleaning, though it might be nice to add a post hoc analysis verifying that the trials with hesitation breaks do not systematically differ in pitch.","There was no report of data transformation. This seems okay with the given model, and would not likely affect the outcome of the analysis.",This was a straightforward and concise write-up and analysis. A slightly more extensive conclusion that explains what the density plots show and a brief explanation of the statistical results in plain language would be beneficial to the reader.,2
dermatolepis_aculeatus,65,65,65,publishable with minor revision.,"First of all, the manual correction of the noun and adjective boundaries is a remarkable effort that should be noticed. However, some methodological choices were not sufficiently supported by arguments, which limits the understanding of the analyses. These points are discussed in the following sections.","Mixed-effects linear models are a good tool to limit the influence of extreme observations and to take into account inter-individual or inter-item variability. However, it remains unclear whether the assumptions of these models were met (e.g. ,normality of residuals distribution, homoscedasticity)  and the script available on GitHub does not help in this regard. Also, regarding the choice of the fixed effects: could have other predictors than typicality had an influence on the pitch, such as the sex of the participants?","As mentioned above, the choice of variables is not sufficiently justified. For example, we can ask whether other factors than typicality could have been added as predictors and thus improved the model’s fit: word length, sex of participants, order of items (in co-variable for example), etc. In any case, it would have been preferable to test these predictors, in a forward stepwise model selection procedure (for example) to select fixed and random effects of the model according the Akaike criterion (Akaike, 1973), and using the lmerTest package (Kuznetsova et al., 2017) for model comparison. Therefore, fixed effects, random effects, and random slopes could be only included if they significantly improved the model’s fit. The report gives the impression that none of these prerequisites have been implemented and this is why it seems to lack content and precision.","Regarding the choice of variables, while the authors describe having detected delay and state that this delay can be an indicator of the typicality effect, one wonders whether this measure could not have been tested in the analyses.","To complete what we said about the choice of model, a word about the random structures. Indeed, all the random structures estimated here are ""intercept-only"" (1|speaker) and (1|noun). This could be problematic; indeed it is one of the points of agreement between the series of papers by Barr (2013; keep it maximal) and Bates (2015; parsimonious models) on how to specify random structure. The argument is that this type of model is a source of both false positives and false negatives (depending on the covariance matrix of the different predictors and VDs).","This ties in with our various comments about the lack of justification. Indeed, the authors seem to have detected and excluded some errors but decide nevertheless to keep the detected errors in the TextGrid. While these choices may be quite justifiable, it would be relevant to document why certain tests have been excluded or kept.",We have no comment to make on this point.,We have no further comment.,2
