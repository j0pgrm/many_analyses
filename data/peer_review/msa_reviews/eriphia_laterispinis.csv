Q3,Q5_2,Q5_3,Q5_4,Q6,Q8,Q9,Q10,Q11,Q12,Q13,Q14,Q15,q6
eriphia_laterispinis,90,100,98,publishable as is.,"My ratings were based on replicability, motivation of the study from previous research, appropriateness of the analysis given the data and hypotheses, and interpretation of results.",The analysis type was appropriate for the data,"Variable selection was appropriate and well motivated. Model specification was nearly flawless; my only question is why such long chains were used when sampling the posterior. If not warranted by the data, doing so uses more time and resources to arrive to the same conclusion.","Use of the average values across the sentence, while easier than isolating average values within regions of interest, may not index the intended phenomenon. Several out-of-the-box tools currently exist to automate the process of alignment, so this could have easily been done.",The model structure is gorgeous. The fitting procedure is excellent. The assessment of fit is spot-on. Every piece of the analysis is accountable and replicable (up to random noise that could have been avoided by setting the random seed).,"The others did not exclude errored trials, which may have biased their findings.",The choice not to transform data was justified.,The statistics are beautifully done; this could be a gold standard for peer review.,3
eriphia_laterispinis,70,95,85,publishable with minor revision.,"We find that concerning the phonetic analysis, not much has been done. However, this is not meant to be a negative statement. The authors explained that they lacked time and skill to provide alignment at the word level, i.e. as they had planned originally. Instead, they used the provided sentence level information and extracted their measures on sentence level, respectively.
Concerning the statistical analysis, we have little experience with running Bayesian models ourselves. However, the report provided by the team does a very good job in explaining their statistical procedures. We only find one minor issue with the statistical analysis, that is the authors did not incorporate an effect of item. As they did not motivate their decision, we are left wondering why this might be.
On a more general note, we are not sure whether taking into account the complete sentence to analyse the phonetic nature of a rather specific part of that sentence is sensible. While we do not deny that later parts of sentences may very well influence earlier parts, we nonetheless find that earlier parts can also be different due to very different factors, e.g. due to speakers’ motivation, outer influences, etc. That is, while the analysis presented is very thorough and well executed, we are uncertain whether the broad scope of incorporated data points per measure per sentence really is meaningful.
The overall rating is the mean of both other ratings. We chose ‘publishable with minor revisions’ as we do not find any insurmountable issues with the statistical analysis, but wish for a discussion of the sentence-scope measurement computation.","As mentioned above, we cannot speak in detail towards the Bayesian statistics part. Yet, after consulting relevant sources, we find no issues with the statistical analysis type. The only minor concern regards the incorporation of item as a variable, as already mentioned above.","The authors make use of several dependent variables: average F0, intensity, and duration of the whole sentence. The authors do not provide motivation for their choice of dependent variables.
As independent variables, typicality and speaker are used. Typicality being the predictor variable of interest, including it is straightforward and requires no further motivation. Speaker was included as random effect to account for inter-speaker differences. Comparing a model with random slopes and intercepts to a model with only random intercepts, the authors found the latter to be most parsimonious. 
As briefly mentioned above, item was not included in the modelling process.","Typicality is the predictor of interest; thus it should be part of any model trying to answer the present research question. However, we would have liked to see some sort of random effect variable for adjective and/or noun combinations, as well as the use of the continuous typicality variable.","We find the structure of the presented models to be overall suitable for answering the present research question. As mentioned before, a more sophisticated random effect structure – even if it is found that is does not improve the model and is to be removed – is preferable.",No subsets of the data were excluded. We find that trials with pertinent comments in the ‘Notes’ tier of the provided TextGrids should have been checked and removed if necessary.,"The three dependent variables, i.e. average F0, intensity, and duration of the whole sentence, were standardised by subtracting the mean and dividing by the standard deviation to account for inter-speaker differences. We find this method appropriate.","We would like to stress again that, regarding the Bayesian part of the statistical analysis, we are laypersons. One group member has basic theoretical knowledge of Bayesian statistics from a linguistics perspective, another member as theoretical knowledge of Bayesian statistics from a mathematical perspective. However, no group member has ever worked with Bayesian methods from a practical point of view.",2
eriphia_laterispinis,30,70,40,deeply flawed and unpublishable.,"We have major concerns with the phonetic analysis.  We appreciate that in the roadblocks question of the questionnaire the team were forthright about their lack of skill in doing the word level segmentation–that's a nice honest disclosure, and we understand this group may have been expecting going into this project to only be doing the statistics part.  Their decision to take measurements over the entire sentence is deeply problematic, and we think it makes most of the results unreliable.  While using the entire sentence might have been workable for some measures, it’s nonsensical for others.  This is particularly true for the total duration, given that the start and end points of the labelled utterance are not themselves aligned with the audio, and so the duration of the sentence includes silence on either side; moreover, the duration of this silence is not uniform (varying by a few hundred milliseconds), and so this will certainly introduce a large amount of error.  We are also very concerned with the fact that no observations were excluded, even those that were specifically marked in the comments tier as having been produced with errors; in some cases, the errors were repetitions or even pauses of upwards of 100ms, and so again should not have been included.

The use of average intensity is inappropriate, as there is no evidence that mic-to-mouth distance was held constant.  From the low quality of many of the recordings, with echoes in them, we strongly suspect that head-mounted mics were not used.  Therefore intensity is useless unless normed within each utterance, which this analysis didn't do.

Finally, with respect to average F0, this is the only measure that could in principle work, though it’s so imprecise as to be uninformative.  By taking the average over the whole utterance, there may well be unknown confounds, or it may average away effects that are realised at a particular point of the utterance.  Additionally, we found creaky voice to be particularly prevalent throughout some speakers' utterances, and prevalent at onset of vowel-initial words for many speakers.  Since the pitch tracker rarely does well with creak, and often gives an extremely low value for f0 during creak (pitch halving), or no value at all, unanalyzed creak is likely to throw off the average f0 dependent variable.  We noticed that, to their credit, the authors use a maximum pitch of 400 Hz (therefore not the default of 500 Hz), thereby eliminating some of the spurious pitch measures taken during voiceless aspiration noise or frication.  But it is therefore puzzling that there is no discussion of the effect that creak might have on the results, since creak is very prominent (both audible and visually evident in the spectrograms and waveforms).

Finally, removing the middle typicality is an odd choice, and makes it difficult to interpret the results (to the extent that they are interpretable): for example, suppose medium typicality utterances had a higher average f0 than both the atypical and typical.  What would we make of that?  Perhaps the authors have a hypothesis that adequately addresses this possibility, but if so, it’s not made sufficiently clear in the report.
","A multivariate multilevel Bayesian regression model is a good choice here.  It allows the researchers to explore the influence of typicality on all 3 dependent variables without running 3 separate models. This approach also allows the researcher to take into account any potential correlation between the 3 outcomes. The actual Bayesian analysis is solid, we believe. The R code is thorough, well documented, and the models seem correctly specified. In a vacuum, the analysis would score a 100. Our score of 70 reflects the problems with the data itself and the authors' failure to consider these problems as they relate to the dependent variable. Here are a few of our concerns:

The authors did all of their acoustic analysis within R with the help of the ‘PraatR’ package (and others). This itself is fine, but it assumes that the files are ready for analysis. As we note above in Q9, this was not the case. Utterances had errors and hesitations, along with NA values from Praat and values that reflected creaky voice. A back-of-the-envelope calculation of our own data shows that the adjective duration was, on average, 100 ms longer in trials marked as ‘error’ compared to the unmarked trials. We found that participant JW_3 was especially problematic and removed them entirely from our own analysis. 

Related to the NA values, the authors note in their code in line 179 that NAs were replaced by the preceding value (time gets normalized so as to fix this). Although we see no problem with this approach, per se, NAs via Praat should always be further inspected. Were the NAs due to a pitch tracker error? Were they due to problematic settings? The authors appear to simply accept that NAs are inevitable. It is possible that the authors did not listen or look at the utterances in Praat, and only accessed the files through R. We see this as bad practice and strongly advise the authors to always first examine the speech in Praat before analyzing the speech in R. (It is possible that the authors did this, but given the failure to note errors or further explore the nature of the NA values, we believe this was not the case.)

We recognize that the creators of the “Many Speech Analyses” project are partly to blame here as we suspect this group, “eriphia_laterispinis,” believed little to no phonetics knowledge was necessary to take part in this project, and that the data were ready for analysis. (The group has also acknowledged this as much.)","The choice of weakly informative priors seems appropriate here given that we do not know which direction any effect of typicality might go.
","As discussed above, the dependent variables are highly problematic: the duration measure is confounded by the variable silence on either end of the utterance; intensity is not normalized within the utterance; and the f0 measure does not take creak into account. No errors or hesitations were removed from the data and NA values were replaced by the preceding value without proper consideration as to why they appear.
","The statistical model seems fine.
","The medium typicality condition was excluded in order to focus on the 2 levels of typicality that would most likely show a difference: typical vs. atypical.  Otherwise, no subsets of data were excluded.  Given that some of the trials marked as errors and hesitations, this presents a problem for the current analysis, especially for the duration measure, since sentences with hesitations would necessarily be longer than those without. It is possible that trials with hesitations are more likely to be of one type of typicality, although we have not investigated this.
","All three measures were standardized at the individual level, which seems prudent, but the intensity measure does not take into account the possibility of head-to-mic placement, and so ought to be normalized within the utterance.  For example, since all utterances used the same frame sentence (""Und jetzt sollst Du…""), one could normalize peak intensity during the adjective relative to peak intensity of ""jetzt"" or some other stressed syllable of the frame sentence.  This would fall beyond the authors'  acknowledged plans for omitting hand-labeling.
",NA,0
