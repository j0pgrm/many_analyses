Q3,Q5_2,Q5_3,Q5_4,Q6,Q8,Q9,Q10,Q11,Q12,Q13,Q14,Q15,q6
polymetme_brevirostrum,90,70,80,publishable as is.,"We find that concerning the phonetic analysis and the resulting F0 values, one is commonly well-advised to use semitones instead of Hz values. However, this is the only minor comment we have on the phonetic analysis part.
Concerning the statistical analysis, we have little experience with running Bayesian models ourselves. However, having a look at relevant literature we find that the statistical analysis appears to be conducted in a meaningful manner without any issues that come to our (laypersons’) minds. Concerning the linear-mixed effects regression, we find that using rather complex effect structures as done in the present case, one should not neglect potential issues of collinearity. The authors of the present analysis do not check for such issues (or they do not mention doing so). As collinearity can lead to several problems regarding the model estimates, we find this point worth mentioning. 
The overall rating is the mean of both other ratings.","As mentioned above, we cannot speak in detail towards the Bayesian statistics part. However, regarding the mixed-effects regression part, we find no issue with the choice of model. Whether one applies such regression in a frequentist or Bayesian framework is a ‘matter of faith’, we believe. Thus, we have no negative points to raise.","Using pitch range as dependent variable is motivated by other research; in fact, we used a rather similar approach to the question in our own analysis. 
The choice of independent variables is very straightforward; Condition, Typicality, and Category are meaningful predictors to answer the research question. However, the team does not provide a rational for their choice, i.e. they choice is not motivated by previous research or theoretical approaches.
The operationalising of all variables appears to be sophisticated as well. Using treatment coding for Condition and custom contrasts for Typicality seems reasonable to us. Fitting a maximal model to begin the modelling process with is one of the two prominent model building approaches (top-down/backwards vs. bottom-up/forwards) and aligns with our choice as well. Using rather elaborate random effect structures to begin modelling with, the present analysis tries to account for all potential inter-variable influences.","We find Condition, Typicality, and Category to be meaningful predictors to answer the present research question as these variables are directly related to the target items.","We find the structure of the linear mixed-effects regression model to be suitable for answering the present research question. Citing from Q11: Fitting a maximal model to begin the modelling process with is one of the two prominent model building approaches (top-down/backwards vs. bottom-up/forwards) and aligns with our choice as well. Using rather elaborate random effect structures to begin with, the present analysis tries to account for all potential inter-variable influences.","Excluding any observations that had comments in the “Notes” tier of the TextGrids is a reasonable choice. If such observations were without any issues to begin with, there would be no comments.","As mentioned in Q9, we assume that working with semitones instead of Hz is the better operationalisation for working with pitch data. Thus, we would have liked this team to work with semitones as dependent variable instead of Hz values.","We would like to stress again that, regarding the Bayesian part of the statistical analysis, we are laypersons. One group member has basic theoretical knowledge of Bayesian statistics from a linguistics perspective, another member as theoretical knowledge of Bayesian statistics from a mathematical perspective. However, no group member has ever worked with Bayesian methods from a practical point of view.",3
polymetme_brevirostrum,50,60,55,publishable with minor revision.,"We have concerns about both the phonetic analysis and the statistical analysis, both of which could be revised to make this analysis publishable.  

First, the phonetic analysis seems to use entirely automatic methods both for determining where the adjective is and for taking measurements from it, without requiring the researcher to ever listen to the recordings or look at the spectrograms (although the authors may, of course, have done so!).  We understand that this is both more expedient and more replicable than methods that require hand-labeling, however, we think completely automatic methods may lead to missing crucial things about the speech that would inform the choice of dependent variable and choice of exclusions.  The main issue in this case is creaky voice, explained below. 

Second, for the statistical analysis, we like the approach, but are concerned about the choice to include the AF and ANF conditions alongside the NF condition (see Q12).  Also, although typicality for the AF and ANF conditions were coded as NA in the ratings file, the authors recoded this using the binning procedure discussed in the pdf of the methodological details, however, they used the median typicality ratings, where it appears that the original researchers used the mean for the binning (NOTE: in the methodological details it does not specify that the mean was used, but this appears to be the case based on inspection of the participants’ trials files). This leads to 4 noun phrases being categorized as ‘atypical’ because their median typicality rating is below 25, while their mean typicality rating is above 25.  Because of this miscategorization, the ANF condition appears to have all 3 levels of Typicality, when it should only have 2 (medium and typical). Ultimately, we don’t think the experiment was designed to test the effect of Typicality in the AF and ANF conditions. 
","This method seems fine. A minor note that the R code read in two of the three data files provided, but there was a missing file (or the third one was mislabeled). We *think* we figured it out, but we aren’t 100% sure.","We are not very familiar with the process of using ELPD values to determine which random effects to remove from the model, but it seems ok.","Gaps in experimental design: 
We are surprised by the choice to include the AF and ANF conditions in the analysis, and to include both the Condition (AF, ANF, NF) factor and an additional factor Category (FOOD/NON-FOOD).  The AF and ANF conditions do not seem to be designed to test the question about Typicality, so the AF and ANF items are not well-distributed among the Typicality conditions.  There are AF items with atypical and medium Typicality, but not with typical Typicality, while there are ANF items with medium and typical Typicality, but not with atypical.  Only the NF condition has data in all three levels of Typicality.  It follows that only the NF condition has each adjective appearing with each of the three Typicality levels (matching of the adjectives).  That is, the cells AF-typical and ANF-atypical have no data.  It gets worse when Category (FOOD/NON-FOOD) is included:  The AF condition has only food items in the atypical condition and only non-food in the medium typicality condition.  The ANF condition is populated exclusively by food items.  So out of the 18 possible combinations of levels, the 7 marked 'x' are the only ones represented:
	     Typ	   Med	Atyp
AF
  food			           x
  non-f.		      x

ANF
  food	 x	      x
  non-f.

NF
  food	 x	      x	   x
  non-f.

We think the authors' analysis is creative in attempting to expand the test of the Typicality question to the AF and ANF conditions and to the non-food items, but with so many gaps in the distribution of items among combinations of conditions, we think the model will be rank deficient and wonder whether it will be able to provide an accurate reflection of the effects of any factor, let alone the interactions.  It seems like the experimental design wasn't built to test anything other than the NF condition, and trying to do so is likely to lead to inaccurate estimates of effects.

There are also problems with the Category factor (food/nonfood).  First, the category was not correctly defined, with 2 of the food items not included in the food list (trauben and paprika were coded as non-foods).  Second, determining the effect of Category will be problematic, especially when considering its interaction with other factors. For example, among the non-food items, almost all of the Category*Typicality cells are empty (8/9). This seems like the kind of situation where even if the computer will return a mathematical solution for what the interactions of Category x Condition and Category x typicality and Category x Condition x Typicality are, one should not ask the computer for a solution to this model, because the information about interaction of Category with either other factor, and even about the main effect of Category, simply isn't there in the data.

Dependent variable: 
NOTE: the following discussion is based on the idea that the authors used f0 range within the adjective as the dependent variable as stated in their analysis summary, but according to the R code (bluebanana.R), in fact, adjective duration was used as the dependent variable.

The authors (maybe) use f0 range within the adjective (max-min) as the dependent variable, over the span of the entire adjective, defined by automatic alignment.  We like the authors' effort at checking the literature on German intonation to determine what patterns one might expect on these adjectives, and in principle, the choice of f0 range on the adjective seems fine.  Since the adjectives are matched across typicality conditions (within NF), the adjective is a good choice for the dependent variable.  However, we noticed substantial creaky voice in many of the tokens.  It is especially prevalent in vowel-initial adjectives, especially after vowel-final ""der"", as in ""der orangen"", but can also occur in ""den orangen"". Also, a few speakers just seem to use a lot of creaky voice in many locations.  The pitch tracker is not likely to do well with creak and will often give an extremely low value for f0 during creak (pitch halving).  Since usually part of the adjective is in modal voice, this will give an extremely large f0 range for most tokens that contain creak.  The authors' automatic alignment and measurement method doesn't seem to have led to noticing the creak or finding a way to deal with it.

Secondly, the authors seem to have left the pitch analysis range at Praat's default of 75 Hz - 600 Hz.  When the max is 600 Hz, the pitch tracker is extremely likely to have some pitch tracker errors that give very high ""pitch"" during voiceless aspiration noise or frication noise, where there is just enough of a repeating pattern for a brief bit for the pitch tracker to think there's voicing.  A method of automatically taking max - min over the span of a whole word is almost sure to get quite a few of the pitch maxima falling in these pitch tracker errors.  Restricting the determination of max pitch to voiced portions at least 30 ms from the onset or offset of voicing would help with this, or hand measurement of max and min would, or setting a narrower range for pitch analysis, perhaps with the max determined for each speaker's actual pitch range.  A process of checking outliers could also help.

We did not see any information in the analysis write up or R code about a verification step to ensure that the f0 ranges were reasonable.
","As noted above, we are surprised by the inclusion of the Condition (ANF/AF/NF) factor and the Category (FOOD/NONFOOD) factor, and concerned about the large number of completely missing cells.  Beyond that, the structure of the statistical model seems fine. 

However, we were surprised to find no statistical model for adjective f0 range in the R file (bluebanana.R).
","We agree with the decision to exclude items with labels filled in on the notes/error tier, as most of these contain production errors. However, we would like to note that we don’t think that information about errors/hesitation breaks/etc. were annotated in the AF and ANF conditions. For example, in JW_3, who had 9/30 NF trials marked as an error/hesitation break, none of the AF or ANF trials for JW_3 were so marked.  It seems unlikely to us that JW_3 only produced errors on the NF trials.  This introduces an inconsistency in how the different conditions were treated with respect to errors and points to the problems with including the AF and ANF conditions.

One might also want to exclude speaker JW_3 entirely, as this speaker had a very high proportion of stimuli marked as errors and seems to have struggled with the task.  However, since the actual error tokens were excluded, this seems like more a matter of preference, it could be fine either way.

We would also look for some method of excluding creaky voice from the f0 range measurements.  Hand-placement of measurement points could avoid this, or one could use a measure other than range, to avoid including f0 minima that reflect creak. 
","There are none, and we're fine with that.","The many blank cells in the design (as analyzed by these authors) and the possible inclusion of very large pitch ranges reflecting doubling from creak to modal voice are large concerns that make us worry about whether the findings are reliable, although we’d like to note again that the results do not specify if they are talking about adjective duration or f0 range.  Based on the contents of the R code (bluebanana.R), we think the results must only pertain to duration, which is surprising, given the explanation in the Analysis_summary.txt file, which focuses on pitch excursions and does not mention duration.

In addition, the problems with the categorization of the food/non-foods and with the assigning of typicality bins based on median instead of mean results in several differences between how we determine the data to be distributed among the cells and how the authors did.  Whereas we find this:

	     Typ	   Med	Atyp
AF
  food			          x
  non-f.		       x

ANF
  food	  x	       x
  non-f.

NF
  food	  x	       x	   x
  non-f.

The authors found this (which is incorrect):
	    Typ	  Med	Atyp
AF
  food			          x
  non-f.		     x	          x

ANF
  food	x	     x	          x
  non-f.

NF
  food	x	     x	          x
  non-f.			          x",2
polymetme_brevirostrum,80,75,60,publishable with minor revision.,"We found ourselves having to go back to the team's Qualtrics responses for many of the answers to the questions we had as we were reading the very brief write-up. We are concerned at the inclusion of typicality for all three conditions (i.e. NF, AF, ANF), since we understood that typicality ratings outside of the NF condition were unreliable. We would also liked to have observed the following: a) more information and/or verification of the alignment using the Montreal Forced Aligner b) more information (tables, figures, etc.) about the results and c) a clear response to the research question in the write-up and clearer supporting evidence. 

We did appreciate the brief review of previous literature as motivation for the dependent variable to be used in the analysis.","We often used linear mixed effects regression, but found that the explanation of the statistical analysis left out some important information and included some information that was not clearly motivated. For instance, we are still uncertain as to what the dependent variable was-- pitch range could have been explained more clearly, what were the units of the DV, etc. The end result was confusing.","Again, we're a bit confused as to why the Typicality ratings were included across conditions, since those were marked as NA in the ratings sheets. Otherwise, the variables look fine (random variables for object, color, and speaker made sense).","'- We don't think there was a strong enough motivation for the inclusion of Category and Condition
- We don't think there was a strong enough motivation for beginning with a ""kitchen sink"" model by including all possible interactions among the main effects. In fact, to our understanding, interactions should only be included in analyses when there is a) theoretical motivation to include them or b) data exploration has suggested an interaction that helps better explain the model.",Answered above,"The researchers excluded any observations that had comments in the ""notes"" tier of the textgrids, this is fine. No mention was made of any other exclusions, though we might wonder if there were any outliers or undefined pitch regions. We also observed many instances of creaky voice, but no mention was made of this.","To our understanding, no transformations were needed given that pitch range relies on each speaker (and is therefore normalized by speaker). Confirmation of this in the write-up would have been helpful.","Organizers-- We were not sure whether we *could* use the survey responses in addition to the write-up provided. If you would prefer that we solely relied on the write-up (called ""Analysis_Summary"" within the ""Runbook"" folder), we are happy to re-do this analysis. Please let us know (we've also emailed). Thanks!
",2
polymetme_brevirostrum,70,80,5,deeply flawed and unpublishable.,"This analysis is extremely difficult to rate. On the one hand, the choice to investigate F0 range is well argued, the phonetic analysis itself is sound, and the statistical treatment is careful, considered, and appropriate. On the other hand, the actual analysis provided uses duration (and not F0 range) as a dependent variable, and no results (other than no effect) are given. Thus, the analysis that is provided does not match the report that is given, and there is no effect size or CI given for the analysis that was carried out (only that there is no significant effect within the ROPE). Based on these discrepancies alone, the analysis that is given, as it currently stands, is unpublishable.","We consider the use of Bayesian mixed effects regression and 89% ROPE to be appropriate.

However, the summary document polymetme_brevirostrum.txt states that the criterion used to answer the research question was “the posterior distribution and 95% credible intervals”, which is not correct:  the 89% ROPE was used as the criterion. Furthermore, there is no effect or CI values provided for the ROPE, which was supposed to be provided.","The choice to consider F0 range in order to capture previously reported prosodic marking in German is theoretically sound; however, the analysis provided does not include F0 range, but only duration. Model selection using LOO cross-validation and ELPD comparison is appropriate.","The predictor variables are appropriate. The inclusion of a “FOOD vs. NONFOOD” variable is certainly interesting and seems to be argued well, but this effectively reduplicates (or even overrides) the typicality rating that is already provided; that being the case, we are not sure the effect that this may have on the final result. The suitability of the response variable cannot be evaluated, since the DV included in the analysis provide (duration) is not the same as the DV described in the report (F0 range).","The structure of the statistical model is sound, but the script could be annotated much more clearly to understand which model was retained in the end.",We consider the choice to observations with comments in “notes” to be appropriate.,N/A,"We find that there is a severe lack of clarity in both the R script provided and in the details of the results. Because of this lack of clarity, combined with the fact that the reported variable does not match the variable used in the script, we find that the analysis is nearly impossible to review properly.",0
