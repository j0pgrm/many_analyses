Q3,Q5_2,Q5_3,Q5_4,Q6,Q8,Q9,Q10,Q11,Q12,Q13,Q14,Q15,q6
trachyphyllia_lappa,75,70,75,publishable with major revision.,"The analyses reported by this team are overall appropriate for this data and design. In particular, we agree with the team’s choice to analyze word-level acoustic features (pitch, duration) that correlate with focus using linear mixed effects models, which allows the team to control for individual differences across speakers. We also agree with the choice to only analyze NF trials, as trials in other conditions did not present a well-balanced typicality manipulation.
However, we have questions regarding some of the choices made by the team, which could have been justified more thoroughly in the report. These include the choice to only analyze half of the data rather than including “repetition” as a covariate in the models; the choice to include “phrase” as a random effect, and to include “color” as a fixed effect even for the models on the nouns; the choice to ignore singular fit in the lmer model; the difference between min/max f0 and f0 range and the parameters used to extract f0. We elaborate on these and other issues in response to the following questions.",Linear mixed effects models are an appropriate choice of analysis.,"In response to this question, we address concerns with the selection of independent variables and random effects. 

1. The choice to include “phrase” as a random effect was not well justified. First, it was unclear what was meant by “phrase” in the report; by looking at the R script provided, we found that “phrase” referred to the specific color-noun combination. However, that is equivalent to using the target noun as a random effect, since each noun was only paired with a single color in the NF condition. A potential issue with including noun/phrase as a random effect is that nouns were not crossed with typicality (each noun only occurrent in one typicality condition); this may lead the models to attribute variance due to typicality to word specific effects, which may have contributed to the generally null effects of typicality in these models. Therefore, we would recommend that the team provides a more thorough justification for why the noun/phrase random effect should be included. 

2. While looking at the R code, we also detected a minor issue with the generation of the phrase variable: in two cases (“braunen-Paprika” and “orangen-Trauben”) an extra space was added at the end of the color in some trials (“braunen -Paprika” and “orangen -Trauben”) leading to two spurious extra phrases with just 2 observations each. If the team keeps phrase as a random effect, they should address this minor issue so that phrase is coded correctly.

3. There could also be more justification for the choice to include color as a fixed effect in all models, including the models on the nouns.

4. We appreciated that the team raised the issue of repetition in shaping typicality effects. We suggest that the team could have added “repetition” (first or second time each noun was uttered in the NF condition) as a covariate in the models to address any potential repetition effects. 
","In response to this question, we address concerns with the selection of dependent variables.

1. Pitch and duration, measured at the word-level, are relevant measures of prosodic prominence. The team could also have measured intensity, a similarly relevant measure that has consistently been associated with focus, or explained whether there were any reasons why they did not think this measure was appropriate.

2. The report should have included more details about the parameters used in the Praat script to extract f0. In particular, we have the following questions: given that pitch range is usually measured as the difference between min and max f0, why was range used as a separate variable in addition to min and max f0? What parameters were used to determine plausible f0 measurements, and were they the same across speaker genders? While looking at the f0 data collected by the team, we found that a large percentage of words led to “undefined” f0 measurements (presumably outside the preselected bounds); a different set of parameters may have led to fewer data exclusions and, therefore, greater statistical power.
","1. When looking at the R code provided, we noticed that the team suppressed the “singular fit” warnings produced by lme4 by default. By running the models without this suppression, we found that all models led to a singular fit, indicating that the random effects structure may have been too complex for the small dataset used in these analyses. While there is some debate as to whether random effects should be simplified to avoid singular fit or not, the developers of lme4 (the package used in these analyses) recommend avoiding singular fit (see the lme4 documentation, https://search.r-project.org/CRAN/refmans/lme4/html/isSingular.html). In particular, they warn that Wald statistics (e.g. chi square) and likelihood ratio tests may be inappropriate when applied to models with singular fit. As the team relied on these types of tests to assess significance, we recommend that they simplify the random effects structure to avoid singular fit; alternatively, if they wish to retain the maximal structure even with singular fit, we suggest that they reconsider the type of significance test being used. More generally, we recommend that the team address and justify their reasons for ignoring the singular fit warning.

2. In both the R code and the report, the team did not specify what type of contrasts were being applied to the fixed effects in the models. As the contrasts were not defined in R, the models presumably used the default dummy coding, with “atypical” as the default baseline for typicality (being first alphabetically). Contrasts may not have been relevant to this team, as they did not report any of the model estimates but rather tested significance using chi-squared, likelihood ratio tests, and eta squared effect sizes. However, we would recommend that the team still define the fixed effects contrasts in order to produce interpretable estimates relevant to the hypothesis (for a review on the importance of setting and reporting contrasts, see Brehm & Alday 2022, JML, https://osf.io/9648f/). Reporting individual contrasts (e.g., atypical vs typical, atypical vs medium, etc…) may also reveal more nuanced effects of typicality that may not have been captured by the effect size for the overall effect of typicality across all levels.","1. We agree with the team’s choice to exclude error trials, including the additional errors identified by the team and not by the study coordinators.

2. We do not agree with the choice to only analyze the first mention of each color-noun phrase for each speaker, which is equivalent to only analyzing half of the data. This undoubtedly led to lower power in the models, which were already too complex for this small dataset. The team raised a valid issue by noting that each color-noun phrase was repeated twice for each speaker, and that this repetition may have influenced prosodic prominence; however, rather than eliminating half of the data, we suggest that the team could have added repetition as a covariate in the models (which we would find more justified than adding color as a fixed effect).
","It is debatable whether linear models are appropriate to analyze non-transformed pitch in Hz, which is not normally distributed. We would recommend performing a transformation on Hz to render it more linear (e.g., log or semitones), or to justify the choice not to transform Hz.
","The team used an overall solid approach for data originating from a limited dataset with some design flaws. We agree with the major steps taken by the team in the analysis, including the choice of DV and the type of models used. We raise some concerns regarding some of the team’s choices, which we believe could be addressed by modifying some aspects of the data analysis and/or by further justifying those choices in the report.

Another minor comment: the report and figures did not specify which units were used to measure duration (e.g., seconds or milliseconds). This could be inferred by the unit size on the Fig. 1 y-axis, but we recommend that the team include the units more explicitly.
",1
trachyphyllia_lappa,95,100,97,publishable as is.,"Solid methods, even if manual corrections of the TG are not easily replicable.",Linear mixed effects models that were put into competition against each other.,Well chosen and explained in a clear matter.,Very good.,Very good.,Well documented and argued. Exclusions were based on the annotators experience of the data sets.,No data transformation.,NA,3
trachyphyllia_lappa,90,90,90,publishable as is.,The authors explained well what they did and how they went about completing analyses. It is easy to follow.,We believe the model that was chosen fits the data well.,The only issue we have with the process of choosing variables is that the authors could have used typicality as a continuous variable instead of categorical one.,"Duration and pitch are suitable variables for answering the research question, we wonder if they would consider exploring intensity as well.",The structure of the statistical model is suitable for the data and data frame.,Authors excluded data based on errors in speech production and justified why they were excluded well.,N/A,"We didn't fully understand the plots, particularly the right hand side of the plot that focused on nouns.",3
trachyphyllia_lappa,60,40,50,publishable with major revision.,"The phonetic analysis was overall good. The two main points of concern are that hypotheses regarding the effect of typicality on the acoustic measurements were missing and it was unclear why the measurements were taken for the whole words and not just e.g. the stressed vowel.
The statistical analysis had some major flaws both in the construction of the LMERs and the reporting of the results.","Linear Mixed Effect Regressions (using standard R packages) are an appropriate choice of statistical analysis for the derived acoustic data and chosen variables. Also appropriately, type II chi-square tests were conducted to assess significance, and eta squared were used for effect sizes. Given that sometimes the interaction between fixed effects in the LMER was significant, pairwise comparisons would have been interesting to compute (e.g. using emmeans).","While we do not understand the inclusion of target_colour as a fixed effect in the LMER, it was correctly tested by means of BIC whether models with or without interaction between the fixed effects provided a better fit. Otherwise, we cannot evaluate the process of choosing variables for and structuring the statistical model because the process was not described in more detail.","The acoustic variables – duration, minimum pitch, maximum pitch, pitch range – are likely to be associated with the phonetic realisation of typicality (or, rather, stress or prominence). However, none of these choices are explained in the report, and no hypotheses for directional effects of typicality on these variables are provided (but would have been necessary for a scientific report). It also remains unclear, why these measurements were taken for whole (and phonetically very diverse) words, and not just for e.g. the stressed vowels.","Overall, we think there are major flaws in how the LMERs were constructed. First, it remains completely opaque why target_colour was used as a fixed effect in the LMERs. It would have made  much more sense to include it as a random intercept or as a random slope (typicality | target_colour), if the authors think that the target_colour has some sort of random influence on the acoustic measurements – even though it is unclear what this influence would look like. Second, given that target_colour was included as a fixed effect, it is unusual that no statistical results are reported for it (even though the models sometimes find a statistical effect of target_colour on an acoustic measurement). Third, it is not very clear from the report what is meant by “phrase” which was included as a random intercept; the scripts reveal that “phrase” refers to the combination of colour and noun (but there was an error in the creation of the “phrase” categories, resulting in 17 instead of 15 categories). Anyway, there is no explanation for why this intercept was necessary. Our proposal would be to construct slightly different LMERs for nouns and adjectives:
measurement_noun ~ typicality + (typicality | speaker) + (1 | noun)
measurement_adj ~ typicality + (typicality | speaker) + (typicality | adjective)
Lastly, we have run the models with duration as dependent variable and found that some of them threw errors which were ignored in the reporting; therefore, none of the reported results (for this dependent variable – but possibly also for the others) are reliable.
In summary, we think that the report would have benefited from a conceptual explanation of the statistical model with clear statements about the expected effects.","In addition to the tokens that were flagged as erroneous by the MSA coordinators, 8 further tokens were excluded due to hesitation breaks between adjective and noun. However, it was not clear how such breaks would interfere with the analysis, i.e. why it was necessary to exclude them (and the other flagged tokens).
Trials with numbers higher than 35 were excluded because uttering the same atypical word combination for the second time might decrease phonetic effects of typicality. This is okay in principle, but it leaves only 15 trials per speaker. To justify this procedure, a pretest with a statistical comparison would have been useful to show that the acoustic parameters in the first repetition differ significantly from those in the second repetition. Following the authors' idea, this would have to be the case in particular with atypical word combinations, since the first occurrence seems more unusual to the speaker than when he/she sees it the second time. If this was not the case, both repetitions could have been included in the main analysis.
Both nouns and adjectives of the NF condition were analysed, though it remains unclear why effects of typicality should show up in adjectives when the noun is focused.","The TextGrids were manually corrected; more specifically, it seems like the word boundaries were corrected, although from the report we first thought that the phonetic segments had been corrected (i.e. the term “segments” should have been defined).
Duration was not normalised, neither for individual speech rate nor for length of word. The latter would have been especially necessary, because the target nouns consist of between two and four syllables. This length variation is not necessarily accounted for by including a by-phrase intercept in the LMER.
Pitch was not normalised for sex; the speakers’ sex could have therefore impacted the statistical analysis without having been taken into account by e.g. adding sex as a fixed effect.","From the report: “overall, atypical nouns had shorter durations than other nouns, except for nouns preceded by the colors orange and brown” – this is neither visible in Fig. 1 nor does that become apparent from the stats.
From the report: “As shown in Figure 2, this effect may indicate a slightly higher pitch range for nouns preceded by medium typical adjectives, compared to nouns preceded by atypical and typical adjectives.” – this sentence is wrong: adjectives are not divided into typicality groups; and from the plot it cannot be seen by which adjective the noun was preceded. What the authors probably meant to write was: “As shown in Fig. 2, this effect may indicate a slightly higher pitch range for medium typical as compared to atypical or typical nouns.”",1
