Q3,Q5_2,Q5_3,Q5_4,Q6,Q8,Q9,Q10,Q11,Q12,Q13,Q14,Q15,q6
linckia_nattereri,66,70,23,publishable with major revision.,NA,NA,NA,NA,NA,NA,NA,NA,1
linckia_nattereri,66,70,23,publishable with major revision.,"Team 'linckia_nattereri' took a maximal approach, measuring various segmental, prosodic and voice quality acoustic markers. We rated 66 for the phonetic analysis for the team’s effort and thoroughness. See below for detailed comments on the problems that we have identified. 
 
For the statistical analysis, the ‘linckia_nattereri’ team provided a summary of their variables, correlations between the variables, and the outputs of a clustering analysis (Principal Component Analysis [PCA], Random Forest) and Bayesian modeling. The choice of the statistical analyses is appropriate, targeting a multidimensional approach in order to explore which variables would be of primary interest.

The overall rating reflects our concerns about how to link the research question, data analysis, and interpretations. As alluded to in our review of the suitability of the acoustic variables below, the choice to measure whole words has unfortunate implications. The problem is more general than the contested choice to analyze segmental variables, which affects a subset of the variables. This problem affects all the variables as it appears to blur distinctions across the board. This is so because we expect to find relevant effects within mostly syllable-size portions of speech but in this study, they were measured together with (i.e. they were affected by) all the other syllable(s) in the same word.

Perhaps not less problematic is the fact that in order to reduce scores to a single scale, Team 'linckia_nattereri' chose to present the values as the difference between the values for the noun and the adjective (Adjective minus Noun), essentially preventing the possibility to interpret any of the findings in terms of actual acoustic events. We believe that this interpretability problem at the heart of this study greatly undermines an otherwise worthy effort by Team 'linckia_nattereri’.
","All measurements (means, medians, standard deviations, etc.,) were transformed to a difference between the obtained value on the Noun minus the obtained value on the Adjective, providing a derived estimate of the amplitude of change of each estimate from the adjective to the noun. Correlations were used to identify co-modulated variables before running multidimensional classification analyses. Predictive modeling was performed by running a PCA and then a Random Forest classification in order to examine whether and how the dependent variables formed clusters depending on the typicality; the Bayesian modeling tested the predictive power of the ‘typicality’ factor for various measurements. The PCA was used both as a dimensional reduction technique and as a clustering one. Factor loadings from the PCA were used to identify main dimensions and their relation to corresponding variables. Following these results, the authors selected 6 main dimensions on the basis that each dimension accounted for more than 5% of the variance. The total variance explained for these 6 dimensions was 64%. According to the PCA results and based on these dimensions, the 3 typicality categories were clustered separately. The authors describe relations between each dimension and the separation between typicality levels. The random forest technique was applied to evaluate the contribution of each predictor to the clustering in terms of typicality after removing predictors with high correlation levels (R^2 > 0.9), reducing the total number of predictors from 44 to 33. A confusion matrix was used to estimate the goodness of the obtained model. 

The quantitative results seem fair, showing that there’s a relatively good classification performance on the basis of the 33 predictors. The confusion matrix seems to have been color-coded with various grey shades but we could not make sense of what different shades indicate. It is a serious issue for information transfer to the reader. The authors then focused on the 10 best predictors deduced from this clustering technique. Following this section, these 10 best predictors were entered into a Bayesian logistic mixed model in order to estimate both the posterior probability of each associated effect and the corresponding effect sizes. 

Overall the analysis is sound, with justifications concerning the relations between each step of the analysis.
","Team 'linckia_nattereri' do not provide an explanation as to why they chose to look at segmental markers. This is problematic given the design of the data, in which each noun is combined with a particular adjective. We therefore did not understand why segmental markers were chosen. However, as the modeled variables were always expressed in terms of a difference between the adjective and the noun, this issue may be partly attenuated, but not fully, as each typicality level was associated with different adjective-noun pairs. Each difference within a typicality level was associated with different word pairs with different segmental contents. It is therefore very likely that any pattern associated with the segmental measures would reflect segmental differences rather than typicality variations.

Under prosodic measures, Team 'linckia_nattereri' looked at intensity and F0 measurements. The procedure for the error reduction in F0 is sound. We also think that the choice of 'Rise' measurements makes good sense, but note that the F0 trajectory in the data seems to be mostly falling throughout the noun phrase for the vast majority of speakers, so in hindsight, it's not that clear what these measurements capture.

Team 'linckia_nattereri' also chose a set of ""voice quality measures"". They measured the ""Harmonics to noise ratio"" (HNR) in Praat, which they also present among other variables (they used HNR to also compute other related variables). Note that these measurements, which are very sensitive to physiological differences between different speakers, are also quite reflective of differences in the segmental makeup (e.g. voiceless vs. vocalic material), such that they may be quite problematic (especially raw HNR) in the context of this data which is not lexically balanced by design.


","The different variables that were analyzed could be justified under certain contexts, but we found the given contexts unsuitable. For example, we'd expect segmental measurements like formants and even HNR (see the previous answer) to make sense if the segmental materials were controlled for, or if the nouns and adjectives in the MSA data were counterbalanced. However, they are not and there does not seem to be an appropriate context to look at segmental markers in this case.

Furthermore, we expect prosodic and (for that matter also) voice quality differences to play a role at specific positions in prosodic structures. We would therefore expect that such measurements would at least pay attention to final positions and/or stressed positions of structures in their targets. Team 'linckia_nattereri' measured all the variables within entire word intervals, blurring links between potential post-lexical events and their position in prosodic word structures (e.g. word-final rise/fall would be diminished given the number of preceding syllables and their F0 trajectory).

We find the choice to measure whole words unsuitable in the context of this task and we believe that this choice has some problematic implications.
","The succession of phases in the analysis (correlational analysis, PCA, Random Forest, Bayesian modelling) is clearly argued. The two main issues in the analyses relate to (1) the color-coding scheme of the confusion matrix which does not make sense in relation to the model’s performance, and (2) more critically, the decision that the authors assume concerning the random-effects structure in the Bayesian modeling part: they state that ‘The model included speaker as random effect. No other random effects were included as this led to R crashing.‘ It is not satisfactory to make a choice based on software limitations, it should be argued in terms of data structure. Indeed, ‘item’ may clearly not be treated as a random factor in the Bayesian analysis as the same adjective was used in all typicality conditions (atypical, medium and typical) but different nouns appeared across the typicality conditions. Except for this limitation, the structure of the model is well-conceived. In line with this issue, The exact random structure of the model is not stated in the report, though it is accessible from the scripts provided. In the end, only a random-intercept by speaker was included in the model. After all other solutions failed in terms of computation.

Overall, the authors choose to model typicality as the outcome with each of the ten continuous variables as a predictor, fitting a logistic model. The results are compared with the random forest ones and seem to be mostly comparable. The main predictors, according to their analysis, would be F2 (it should be transcribed as ‘F2’, not ‘f2’), intensity, tilt, glottal excitation, and energy. Some are associated with only a small effect-size but these are not provided in the report. F1 does not stand out from the Bayes model though it is the best predictor in the Random Forest. The report lacks a perspective on the very different status of the outstanding predictors: indeed, at least some of them are clearly related to spectral content that will be associated with segments. Therefore, they would necessarily correspond to categorical associations that would imply a confusion between typicality and word sequence.

",Only NF tokens were analysed. This choice is justifiable given that we were interested in the ‘typicality’ as a predictor within the NF condition.,"We were not certain whether this question concerned treatments of the phonetic data (e.g., speaker normalisation) or those for the variables in data modelling. If the question concerned the latter, no transformation was done. It is justifiable in the given analysis, but see our comments about the selection and treatment of the variables.",NA,1
linckia_nattereri,20,70,40,deeply flawed and unpublishable.,"The main issue in this analysis is the phonetic analysis. It is probably irrelevant how good or bad the statistical analysis is, as it is based on unreliable data. There are at least three major problems with the phonetic analysis:
1) According to the report, the identification of word boundaries in NF trials was done entirely automatically. There is no report of visual corroborations or manual corrections. This is highly problematic, as automatic detection of boundaries is not always reliable. In fact, I randomly chose 10 NF trials from this analysis and I disagreed with boundaries in at least 2 cases. One example of erroneous boundaries is trial 23 in participant CG. All measurements derived from the analysis are compromised based on potentially incorrect word-level boundaries.
2) From the report, it seems that all trials were included. How did authors handle trials with errors, as reported in tier 5 by MSA coordinators? Also, it is expected that sometimes certain measurements cannot be used, as they are errors in detections. This is common, for example, when extracting f0. One way to safeguard against this is to visually inspect results, or use a quantitative method that singles out anomalous values. 
3) It is not clear why authors decided to include so many predictors. It really looks like a case of data fishing. Some phonetic predictors seem intuitively appealing, such as intensity, but others do not. For example, what is the theoretical or acoustic motivation to include F3 as a predictor? Authors should have defended their choices as to why they hypothesized each phonetic cue could serve as a predictor for typicality.  

","Appropriate, but used on a potentially unreliable set of data.","The outcome variable (typicality, with three levels) is correctly chosen. 
As explained above, however, the set of predictor variables is a massive amount of acoustic features (n = 44) which are not appropriately accounted for.",Predictor variables are not adequately introduced. It is not clear to this reviewer why authors believed these 44 predictors could explain typicality.,Suitable.,"All data points were included, which I find problematic. At the very least, trails labeled as containing errors should have been treated differently.",N/A,"For those predictors that did reach significant, it would have been useful to read an interpretation, even if brief, of what the results really mean. Why would typically between noun and adjective be predicted by these four (out of 44) phonetic cues, and not by the other 40? What does it mean in terms of speech perception, for example? Is this really relevant? Otherwise, the results may look like a mere statistical artifact (i.e., out of 44 variables, something could randomly be significant). Again, this problem goes back to the lack of explanations as to why these predictor variables were chosen in the first place.",0
linckia_nattereri,100,100,90,publishable with minor revision.,"While the approach and statistical analysis with the presentation of the statistical results seem excellent, the overall conclusion is masked by data.","Out of my league, but seemingly appropriate.",The removal of correlated variables was good.,"The number of variables included was high, perhaps too high for a targeted analysis.",Seemingly very appropriate.,Focus on NF condition appropriate for the RQ; there is no mention of how the tokens marked as 'Error' were treated.,NA,NA,2
linckia_nattereri,100,100,100,publishable as is.,"This was an extremely thorough analysis, with detailed documentation of the many acoustic measures and statistical tests used.","PCA was a good choice for the many variables they extracted. I'm not familiar with Bayesian models and how they work for many variables, but it seems to have been alright.","Variable selection was good. They included several variables for formants, prosody, F0, and voice quality. They cite many sources for how these acoustic measures were extracted. The statistical model itself appeared to be straightforward: put everything in and see what happens. For an exploratory analysis like this, it seems good enough to me.","This team chose many variables representing a variety of cues in the acoustic signal and spanning different aspects of language (formants, prosody, voice quality, etc). At explaining them with the brief justification in their hypothesis paragraph.","Again, for an exploratory process like this, dumping them all in and seeing what comes out at the end seems fine.","It appears there were no exclusions, not even the ones marked by the lead researchers. Perhaps this could have been addressed in the write-up.","There did not appear to be any transformations, as far as I could tell. Some things could have been transformed, such as log-transforming formants. I'm not familiar with all the variables they used in this study, so I'm not sure what the distributions typically are.",This was extremely thorough and I enjoyed reading it. I'd like to refer back to this (including the scripts and supplementary materials) in case I need to do a similar analysis or use those acoustic measures.,3
