Q3,Q5_2,Q5_3,Q5_4,Q6,Q8,Q9,Q10,Q11,Q12,Q13,Q14,Q15,q6
stygobromus_tyraica,60,70,70,publishable with minor revision.,Please see next comment,"To be honest I could not follow what statistical test they used. They produced confidence intervals, etc, but this writeup is too computational for me to really understand it enough to be able to evaluate it in detail.",They used supervised machine learning for the whole analysis.,"It looks like their predictor was Typicality, which is valid for the research question.",I do not have enough understanding of their approach to answer this effectively.,"They excluded all errors and hesitations, which is reasonable. They also only examined the adjectives, which I think is also reasonable based on the fact that the adjectives are repeated in each typicality condition.",discrete cosine transform,"As noted above, I do apologise but I don't have enough knowledge of computational approaches to evaluate this submission effectively. However, I did note that they did not choose one acoustic measure, as we were instructed to do, but instead trained a model to pick out the most relevant acoustic features. However, even in the results, which shows a barplot with the most important features, many of the terms are opaque to me.",2
stygobromus_tyraica,75,75,80,publishable with minor revision.,"The current study doesn't control for co-articulatory affects. For example, the authors only focussed on the adjective, but they haven't controlled for the noun. Furthermore, the analysis doesn't connect with the signal and how this might affected by linguistic environment. There is opaqueness in the analysis as we don't know if this is an environmental affect or typicality affect. The acoustic measures were included without research justification. We would like to see more details on variable selection.","We like the training and test set-up, but it doesn't control for variable selection. They could've held out noun categories or created a subset so that we can control for typicality and create a baseline. Classification problem that tests for typicality that accounts for the noun condition (which isn't balanced).","They didn't choose specific acoustic variables, even the top 10 variables are quite similar.",They didn't explain the acoustic qualities and didn't relate this to phonetics. They didn't exclude any variables. The variable importance is misleading. No justification to use a one tailed binomial test there's no existing hypothesis.,We don't think it's suitable based on our review.,"The authors removed segments with notes and known errors. They did not hand correct alignments; however, this wouldn't be necessary if they removed outliers.",They scaled and centred each variable by speaker. We believe this is suitable.,"We appreciate the presentation of their analysis in the markdown, and we are confident we could reproduce their analysis. It would be nice to summarise the results or relate their analysis back to the research question or hypothesis. It would be useful if they contextualised the results.",2
stygobromus_tyraica,90,80,85,publishable with minor revision.,This is an extremely comprehensive and robust analysis of the intervals of the speech dataset that are directly comparable (color adjectives in NF condition trials). The high dimensionality of the acoustic analysis is able to tease out differences between the utterances corresponding to changes in typicality. The major limitation of this approach is the restricted interval of speech which can be analyzed.,"The statistical analysis used in this study is entirely appropriate for the machine-learning methods deployed, although difficult to interpret.","The methods used to structure, evaluate and interpret the statistical models are state-of-the-art for this type of analysis.","The variables chosen for this analysis are generally appropriate, although the dataset was originally developed to analyze affective differences in speech, rather than prosodic differences. Nevertheless, one of the great advantages of this study is the use of high-dimensional acoustic parameterization, which may be able to detect differences between utterances not apparent with more simplistic acoustic characterizations.","The statistical models are structured in a way that is standard for this type of analysis, and entirely appropriate for this approach.","The exclusion of the 54 trials with notes is well motivated, as this removes utterances compromised by errors and hesitation break.  The analysis is restricted to the short interval of time corresponding to adjective production in NF trials; this selection is well explained and motivated as this technique requires directly comparable utterances, but it does mean that only a tiny subset of the data have been analyzed, so the results should be interpreted with some caution as this approach tells us nothing about overall patterns of production of the larger consistuents containing the adjectives.","The use of DCTs to parameterize the acoustic profile of the adjectives is entirely appropriate, and an effective way to efficiently represent the temporal acoustic dynamics of the speech intervals being compared.","This team have analyzed the acoustic profiles of adjectives in the NF condition utterances using a parameterized form of a set of 25 low level acoustic features originally developed for analysis of affective states in speech (GeMAPS). This is a comprehensive analysis of the way that the acoustic properties change over the speech intervals corresponding to the color words, and the data are interpreted by comparing the way that these dynamic feature sets differ with typicality, with respect to machine-learnt default characterizations.

Although this is a state-of-the art approach for affective computing, the analysis could use some further unpacking to make it more accessible to phoneticians less familiar with machine learning techniques. In particular, although the study is extremely well documented overall, it is not entirely clear how to interpret the significance of the findings, especially since the feature set used was designed to detect emotional, rather than linguistic differences between utterances.

Nevertheless, these data suggest that the utterances do differ with typicality, primarily in the spectral flux, loudness, spectral balance above and below 1 kHz, first three MFCCs, and F0 trajectories over directly comparable adjectives. It is not surprising that differences between utterances are encoded as complex combinations of acoustic properties, so this is an important contribution to the meta-study, as most teams will not have investigated multiple integrated phonetic properties, where subtle speech differences can be encoded.",2
