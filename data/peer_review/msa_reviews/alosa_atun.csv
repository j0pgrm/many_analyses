Q3,Q5_2,Q5_3,Q5_4,Q6,Q8,Q9,Q10,Q11,Q12,Q13,Q14,Q15,q6
alosa_atun,70,90,79,publishable with major revision.,"The statistical modeling used in this analysis is carefully considered, well-motivated, comprehensive - taking into account all variables of interest to the research question, systematically deployed and generally well interpreted.",Linear Mixed Effects models are an appropriate choice of statistic to examine the effects of interests in these data.,"The models used are well desgined and systematically refined following a standard process. The authors clearly motivate each version of the model, the tests used to evaluate them, and the process by which they have been refined.",The variables included in the models are appropriate to the data and research question. It is not clear whether the use of mean typicality ratings as a continuous variable is appropriate in place of the 3-level categorical typicality variable that informed the study design.,"The final models chosen are appropriately structured with respect to the phenomena being examined, the nature of the dataset, and the metrics dervied from it. The authors clearly motivated all elements of the models, and the process by which they have been refined.","The primary concern with this team's approach is the decision to restrict the analysis to the subset of phrases with the singular dative article. This decision appear to be necessary for aspects of their analysis that wouldn't extend to 'den' phrases, but this unfortunately means that approximately 25% of the data is not analyzed, including five entire noun phrases (""den gelben Erbsen"", ""den gruenen Bohnen"", ""den gruenen Socken"", ""den orangen Kartoffeln"", ""den orangen Trauben"") that were carefully included in the balanced study design (Coretta et al. 2021 ""Methodological Details of the norming and production studies"": Sec. 2.3). This is a substantial and possibly critical subset of the experimental data that has been omitted and which has skewed the balance of the data unevenly across typicality categories. All analysis presented therefore either assumes that the phenomena reported extend across plural NPs, or that any phonetic differences can be neglected, but the authors might wish to better motivate the decision and address this issue in more detail.","No data transformations have been directly applied to the modeled data, but given the large findings reported for speech rate differences and the lack of difference in local duration effects, some kind of temporal transformation/normalization might have been applied to examine duration effects more closely.
","This team have planned, executed and presented a comprehensive analysis of the data that thoughtfully addresses the research question and offers some insights into the phonetic properties of these utterances and how they might differ with typicality.

Some key metrics which are central to the analysis are not adequately explained. In particular, 'DiphDur', and 'DurRat' -- ""the ratio of the duration between the nucleus and the offglide"" -- will depend critically on the accuracy and consistency with which the segment boundary is located between the /e/ vowel and the coda of the article, yet the details by which this was measured and validated are not properly explained. (In the first place, this also assumes that all speakers produce a comparable vocalized 'rhotic' in the definite article, but this is not addressed either - do all participants speak a sufficiently comparable variety of German in this important respect, and how was this acertained?). Vowel-approximant transitions of this nature do not typically contain a clear segment boundary, so how robustly was this boundary located in this data by the automatic segmentation tools used, and how was this checked and validated?

The segmentation of the vowel-coda sequence in the definite article is tied up with another methodological issue about which there is insufficient detail: formant tracking across the rime. No details are provided about how LaBB-CAT tracks formants (window size/type/overlap), max. number of formants, etc., yet this is also a key metric: the first variable claimed to differ significantly with typicality in the article. While the statistical analysis is compelling, and appears to be robust, it is difficult to assess without having a better idea of the data. A couple of illustrative spectrograms aligned with waveforms showing how formants track over the course of a determiner, where the segment boundries lie, and how the nuclear vowel is delineated from the coda schwa would greatly aid interpretability here, especially since no significant differences are reported for either vowel duration or F2: it is perhaps surprising that typicality is flagged through vowel height with no commensurate change in fronting or duration, and some phonetically detailed illustrations of this phenomenon would help to understand the effect reported.

Other key variables which require more explanation are: 'maxIntensity', 'meanPitch', and 'ED' - few details are given to properly understand how these were measured, calculated and validated.

>> ""The alosa_atun_data.rds file contains 10,913 observations"".
What is the breakdown of this dataset (by type/condition/speaker)? 70*30 test trials were provided, and this team has also analyzed the additional trigger trials, but that should result in no more than 4,200 observations?

The finding that ""(AF) typical trials have a significantly faster speech rate"" is not adequately unpacked. Since no significant durational effects were reported at the segment, rime, or lexical level in the determiner, utterance-level duration/speech rate differences cannot arise from local lengthening, so this is inconsistent with the claim that ""This may be an effect of the overall trial, or may be carried by local duration effects, such as in the adjective itself)"". If this were thought to be the case, duration should have been examined in other constituents in the target NP. If these differences are global, as the data suggest, what drives this phenomenon? 
",1
alosa_atun,100,100,100,publishable as is.,"The analysis is performed very thorough. Each step of preprocessing and of the analysis are well described. The analyses are described in detail in the supplementary data. 

Regarding preprocessing: the automatic forced alignment was complemented with manual segmention (I would have wished to see how segmenters dealt with problematic cases of the [e] and [@]  boundary, but this is just complaining at a high level).

Regarding the analysis: The authors pre-registered their analysis and reported the results of the preregistered analysis. However, since the preregistered model would not converge, they opted for a more exploratory, backwards fitting procedure. This step is fully justified.","The authors used lmer which, given the nature of the  dependent variable (point-wise measurements of formants and F0, durations) and predictors (numeric predictor of typicality, categorical predictor of focus) is fully justified.","Choosing dependent variables: 
The authors chose to investigate the direct article preceding the adjective+noun phrase. Given the large phonetic variability in the phrase, this was a very elegant move to controll for phonetic variability . 
Accordingly, the authors opted to investigate literally all possible acoustic measures (formant values, pitch, word duration, speaking rate). Given the task at hand, this is justified. 

Choosing predictors: The authors chose good control variables. As for the predictors of interest, it was a good move to chose average typicality ratings from the rating experiment to get a broader distribution of the ""typicality"" predictor (instead of using a categorical predictor).",All variables are suitable. See above,"The  structure of the final, presented model, is fully sensible. However, I am not surprised that the preregistered model with a three-way interaction in the fixed effects and a three-way interaction of slopes with three (!!!!)  random intercepts did not work. This Barr et al. approach surpasses overfitting.","Given that the authors have decided to use numerical typicality predictors, it is acceptable that they did not exclude the ""adjective focus"" condition. 
Regarding the exclusion of ""den"": this is totally acceptable, given that the [@] woud systematically differ in the [n] context (and that it was not part of the target phrases).
The exclusion of formants is completely sensible.",The authors have not reported any transformations. It is very likely that the speaking rate predictor and the word duration variable would have needed a transformation. But I do not think that this would have changed the results.,"I am very impressed with the thoroughness of the analysis. One minor point that I have is the separation of [e] and [@] in ""der"". In my experience, finding a boundary between these two may be very hard. The authors could have used a non-linear method to investigate the entire formant trajectory of the entire vowel complex [e@] or what ever there is. 

Another aspect concerns the interpretation of the statistical analysis of F1. The authors made predictions for all the nine analyzed measures. However, only one turned out to yield a significant effect with a very low t-value (2.5). Under normal circumstances, I would be very careful to state that there is a significant effect of typicality. 

But again, these two points do not diminuish the accomplishments and thoroughness of the authors.",3
alosa_atun,83,90,87,publishable with minor revision.,"Positives
- Pre-registration
- Well-organized report, most choices were well justified
- Statistical analyses are appropriate
- Well-defined procedure for arriving at the mixed effects model structure (starting maximal and pairing down via backward elimination)

Negatives
- Some of the pre-registered predictions are a little vague (e.g. for f0 and amplitude)
- Would have been useful to have p-levels for the model outputs (e.g., with lmerTest) as significance is hard to determine with t values alone
- Could have reported the model structure even for models that led to no significant effects
- Unclear if the fixed effects were treatment coded or not - if so, it might make more sense to use “T” as the baseline
",Mixed effects models are an appropriate analysis.,"1. There were a lot of dependent variables, some of which could be seen as redundant (e.g., both F1 and F2 and the euclidean distance between the two; several measures of duration). We would recommend narrowing down to fewer justified measures, even if the analysis is exploratory.

2. Including speech rate as a predictor is a good idea.

3. The team also nicely dealt with the uneven design by using typicality ratings rather than the typicality category. However since the ratings are not distributed equally across focus conditions (fig. 3) that might still lead to problems in the models (e.g. might be why there were convergence issues). A possible solution would be to run a separate model on each focus condition.
",The team mentions using the intensity/f0 from “den” as a control variable but it doesn’t appear to be included in any of the model structures reported,The overall model structure was well justified and there was a clear process for determining it.,"1. It would be useful to know how many trials included “den” and were therefore excluded from the analyses. The team could have included both for the /e/ analyses, adding phonetic context (/r/ or /n/) as a predictor.

2. How did the team identify “misleading or incomplete” observations, and how many were removed?
","1. We agree with the choice to center/scale trial number and speech rate (though we would suggest to use syllables x seconds rather than x minute)

2. Since the data included both male and female speakers, it would be appropriate to transform F0 and F1/F2 so that they’re both on linear scales (e.g., logging), rather than using an arbitrary 800 Hz upper limit.
",NA,2
