Q3,Q5_2,Q5_3,Q5_4,Q6,Q8,Q9,Q10,Q11,Q12,Q13,Q14,Q15,q6
aratinga_lugubris,10,35,25,deeply flawed and unpublishable.,"In their analysis, there are major critical points to be considered. We go through these critical issues in this questionnaire. First, the authors state: 

“…since there are no minimal pairs, it is impossible to answer this question with the data provided by the organizers, so we decided to formulate the question differently:
Is there any difference in the speakers’ production, when they produce atypical/medium/typical word combinations with the same color adjective? (e. g. green carrots vs. green tomato vs. green beans)” (p.17).

 Because the experiment is set up in such a way so as to avoid minimal colour pairs in the testing phase (e.g. yellow banana vs. blue banana), the authors choose to re-formulate the research question and develop their own research question. This research question again asks about the differences in productions, but these differences are non-acoustic ones. This is unfortunately not the topic of the study and so the entire analysis is considerably removed from the primary research question and the goal of the project.

Our argument is supported on page 6, where the authors state the following.
“We listened to utterances of a particular color by all speakers in order to find out whether there were any differences between typical and atypical utterances. We did not manage to find any consistent acoustic differences, so we decided to drop this search and focus on non-acoustic cues” (p.6). 

The authors offer an impressionistic perception account of whether the acoustic differences exist or do not exist without even specifying whether they have any German language background. As such, their analysis not only overlooks the importance of the acoustic cues, but it is also flawed in that it does not offer specific parameters through which the acoustic differences do or do not exist.","The choice of the statistical analysis was the Bayesian mixed effects ordinal 
regression. Given the outcome variable (typicality), which has three factors that have natural ordering: typical, medium, atypical, we believe that the choice of the statistical analysis is on target. Still, because of the way the Bayesian analysis has been conducted, i.e. with default priors and without posterior checks, the results of the simple ordinal regression would be, if not the same, similar.","The outcome variable was the typicality. The first predictor variable was the speech error, which had two factors: speech errors and no speech errors. 

Another variable was the adjective emphasization, which was broken down into 2 factors: non-emphasized and emphasized. 

The authors developed 2 models with each predictor variable separately, and each model consisted of random and fixed effects (by-participant and by-colour, respectively)
Considering the choice of analysis and the authors’ hypotheses, the statistical models were on target.",All the variables included in the statistical analyses were  suitable for testing the postulated hypotheses.,"The Bayesian statistical models were not thoroughly developed.  First, the authors could have developed the prior distribution based on the previous research on speech errors and accentedness. We understand that building the priors is most certainly not an easy task, but priors can largely affect the model output and the posterior checks. The brm model itself was well structured as the model converged and the authors presumably checked the Rhat values. However, there were no posterior predictive checks to show the probability distributions of plausible values of the parameters, given the data and the model. In addition to credible intervals, the posterior checks would indicate whether the models fitted the given data. Lastly, the authors justifiably selected the cumulative link logit family type for their models which automatically transformed the estimates into log odds. While log odds are useful, it would have been more useful to interpret the results with the transformation of log odds into simple odds.","The authors did not analyze the ANF and AF items. They only used the NF condition 
in their analysis, which was an understandable choice given the structure of the original study.

For the error analysis, the authors did not exclude any items in the NF condition, which was understandable given their analysis, however, they should have excluded items with errors when they were analyzing accentedness (emphasis vs. non-emphasis) because the items with errors could have affected their results.","The typicality variable was transformed. Initially, it consisted of 3 categories: hesitations, errors, pauses. 
However, when developing the models, the authors decided to include two factors in this variable: errors and no errors. This choice strengthens the model’s fit and its convergence.","We believe that the statistical modelling is somewhat suitable given the hypotheses and the choices authors made, but that the overal analysis is flawed because:

1) it does not deal with the question the study asks
2) the impressionistic way the stimuli are analyzed
3) terms like accentedness and speech errors are not well defined
4) there is too much code in the report which is a problem because it is not accessible to people who are not used to R programming language
5) no language background of the raters is provided
6) visualization of the norming part of the study was redundant
",0
aratinga_lugubris,15,15,15,deeply flawed and unpublishable.,"We have serious reservations about the structure of the model, including the choice of DV.",The choice of analysis type (Bayesian models) is in principle ok but the model structure is odd.,"We don’t understand the rationale of choosing typicality as the DV, when that was the factor which was deliberately manipulated in the experiment. 

Given this choice, though, we do not understand why random intercepts for speaker and adjective (i.e. colour) were included: typicality does not vary by speaker. If the model were constructed to predict speech error rate as a function of typicality as a fixed factor, the chosen random effects structure might make sense.","No acoustic phonetic analysis was performed. The report says that a decision was made not to perform any such analysis, based on the observation after listening to the data that there were no obvious acoustic differences. Despite this, Hypothesis 1 was included in the report, though rejected without any acoustic investigation. It could perhaps be rejected on the basis of the results of the auditory phonetic analysis, but the methods and results of that analysis would need to be documented in more detail (e.g. what criteria or phonetic features were consistent with classification of adjective tokens being ‘similar’ or ‘no different from each other’)? 

The decision to look at speech errors was an interesting choice. Hypothesis 2 is formulated in a circular way, though, which undermines the analysis. We also do not have very clear criteria of what counted as an error (e.g. how long did a pause have to be to count as a ‘longer pause’; a much stronger approach would be to measure the duration of pauses as a continuous variable). The decision to view the data in this way reduces power, as there are only 81 errors across (819 non-errors) so there is not much data to look at, resulting in the very wide credible intervals.

Similarly, the criteria used to identify an adjective as plus/minus ‘emphasized’ are not described (other than in footnote 1). The project brief was to look for evidence of phonetic modulation of utterances; in principle that could be detected using auditory impressionistic analysis, but at the least some description and discussion of the types of linguistic exponents of ‘emphasis’ is needed. We do not understand how it was possible to reliably or consistently detect presence/absence of emphasis in the same tokens that had previously been described as not being hearably different from each other.","The structure of the model seems counter-intuitive. It has typicality as the DV with number of speech errors (or presence/absence of emphasis) as a predictor, rather than trying to predict the probability of a speech error or of emphasis based on the typicality condition (which would be a closer fit to the project brief, in our view).",N/A,N/A,"There is some evidence of hypothesising after the results are known (HARK-ing). The authors are at least very transparent about switching to explore a new post hoc hypothesis, but the presentation does at times seem like they already know the answer they want (typicality should modulate *something* in speech) and are looking for the question that gets them to that answer (i.e. speech errors). The transparency is good, but it is a questionable practice, and we did not get the sense that this experiment was designed to test the hypothesis that typicality relates to speech errors. It would at least need to be followed up by a confirmatory analysis.",0
aratinga_lugubris,70,65,67,publishable with major revision.,"Given that many of the analyses depended on listener judgments, I would want more detail on what the listeners were listening for or what they were expecting when listening to the recordings. I have some questions about how listeners could not notice an acoustic difference but could notice a difference in accentedness. I think being clearer about what types of acoustic differences were being listened for would help. Additionally, I would like more information on why the models had typicality as a dependent variable and things like error rates as independent variables. It's not that I think this choice is wrong, but I would like to understand the decision better.",The statistical analysis type itself seems sound and appropriate.,I'm not sure why the random effects structure only included independent intercepts. I'd also want more information behind the decision to include typicality as the dependent variable rather than the independent variable.,The chosen variables seem suitable for the questions the analysts posed.,The structures of the models seem suitable. I agree that going with default priors seems justified in the current situation.,"I think it was clever to look at error rates and hesitations as part of the data itself, therefore not having to exclude that data.",I didn't notice any transformations or lack of them that seemed inappropriate.,I think the questions and analyses these analysts pursued are very original and interesting! I'd just like more information or justification for some of the choices.,1
aratinga_lugubris,75,90,80,publishable with minor revision.,"They first did a perceptual analysis and agreed that they couldn’t hear any acoustic differences between productions of the same color for different typicality rates so they decided to exame non-acoustic cues to see if they differentiate typicality. The results of the preliminary perceptual experiment - that the team couldn’t meaningfully perceive any acoustic differences between conditions - and Hypothesis 4 - that the team could perceive meaningful differences in emphasis between conditions - are incongruent. The authors addressed this in a footnote at the end of the paper but additional discussion of the acoustic cues that the authors believed might vary individually is necessary to justify their assumptions. The authors do not present inter-rater and intra-rater agreement data for perception of emphasis. This should be included in a minor revision. I would also like to see discussion of how raters were masked to condition information while rating the tokens in order to control for subconscious rater bias. 

They then moved beyond a phonetic analysis and instead assessed accentedness of nouns and adjectives and the number of errors/hesitations/longer pauses. For accentedness, they did another perceptual analysis where team members marked the colors for accentedness for the noun or adjective pair.

For errors/hesitations, the tokens were marked for error type. A list of the “mistakes” included in this analysis would have been helpful – were they consonant substitutions/omissions, or were these just prosodic (hesitation) errors? 
","They did a reliability analysis using Cohen’s Kappa to compare their annotations of errors with the organizer’s annotations and found high reliability. This was a thorough and appropriate analysis. However, it was unclear whether the identified disagreements were resolved or ignored.
To analyze if speech errors were concentrated at the beginning vs. end of the experiment, they made a distribution figure to see how distributed the errors were across the experiment. Based on visual analysis, the errors were generally equally distributed. A visual analysis seems appropriate for this. 
Then to look at the relationship between typicality and speech error, and typicality and accentedness, they built two Bayesian mixed effects regression models, both of which had appropriate structure. My only critique is that they included random intercepts by participant and color, but not random slopes. It would have been nice to have used a maximal random effects structure. They also originally included color as a random effect but then excluded it because they didn’t get model convergence; so they only included speaker as a random effect which seems appropriate. 
A big critique is that they used default priors in their Bayesian models which are rarely appropriate. It would have been better to use minimally informative priors that assumed no effect instead of the flat priors used in the defaults.
","The process of variable selection was well-described. The preliminary perceptual analysis informed their variable selection. It was interesting that the authors did not include acoustic variables in the current analysis (e.g., f0, duration, etc.), but the choice not to include these variables was likely appropriate, since the listeners did not note differences here during the perceptual analysis. However, there are often differences that are not perceptible (e.g., covert contrast), so we would recommend also examining acoustic variables with this dataset.","They ran two Bayesian mixed effects models both with typicality as the dependent variable and with random intercepts by speaker and color. They said that the models didn’t converge with color included as a random effect but then color is included as a random effect in the model code. This is confusing and needs to be explained further. The predictors for each model were speech errors and accentedness, which both seem appropriate. 
They excluded acoustic variables from the model because their original perceptual analysis did not reveal any differences. This is appropriate, although it is possible that there are undetectable acoustic differences. 
",The structure of the model is appropriate. I do recommend including random slopes instead of just random intercepts. They could have also run one big model with the predictors together instead of separate.,"Including only the NF condition (and excluding the AF condition) here was appropriate. At the end, the authors note that certain speakers may have been appropriate to exclude from analysis based on their speech characteristics. While the authors did not exclude these speakers from the current analysis, it does demonstrate the benefits of the preliminary perceptual analysis, to identify whether certain speakers might be using a different register that might impact the results.",N/A,Duration of hesitations/pauses may have been informative in the current analysis.,2
