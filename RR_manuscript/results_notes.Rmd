---
title: "results notes"
author: "Timo Roettger"
date: "2022-08-13"
output:
  html_document:
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      warning = FALSE,
                      message = FALSE)

library(brms)
library(faux)
library(dplyr)
library(tidyr)
library(ggplot2)
library(tidybayes)
library(here)
library(googlesheets4)

```


```{r load-data}

msa_models <- readRDS(here("./data/analyses/msa_models.rds")) %>%
  mutate(
    outcome = as.factor(outcome),
    typicality = as.factor(typicality),
    temporal_window = as.factor(temporal_window)
  ) %>%
  droplevels() %>%
  mutate(
    outcome = contr_code_sum(outcome),
    typicality = contr_code_sum(typicality),
    temporal_window = contr_code_sum(temporal_window)
  )

# average phd experience
phd_mean <- msa_models %>% 
  select(team, years_from_phd) %>% 
  distinct(team, years_from_phd) %>% 
  summarise(mean_phd = mean(years_from_phd, na.rm = TRUE),
            sd_phd = sd(years_from_phd, na.rm = TRUE),
            min_phd = min(years_from_phd, na.rm = TRUE),
            max_phd = max(years_from_phd, na.rm = TRUE)) 

prior_mean <- msa_models %>% 
  select(team, prior_belief) %>% 
  distinct(team, prior_belief) %>% 
  summarise(mean_prior = mean(prior_belief, na.rm = TRUE),
            sd_prior = sd(prior_belief, na.rm = TRUE),
            min_prior = min(prior_belief, na.rm = TRUE),
            max_prior = max(prior_belief, na.rm = TRUE)) 

# number of teams (final)
no_teams = length(unique(msa_models$team))

# Helper function to print table data easier
split_to_list <- function(.data, ...) {
  dots <- rlang::enquos(...)
  for (var in seq_along(dots)) {
    var_name <- rlang::as_name(dots[[var]])
    .data <- purrr::map_depth(
      .x = .data,
      .depth = var - 1,
      .f = function(xs) split(xs, xs[var_name])
    )
  }
  .data
}

team_sheet <- "https://docs.google.com/spreadsheets/d/1w39rU7O5ijzPmCpClVt5LEDIhzaLBDNrwfdMJUP86q8/edit?usp=sharing"
teams <- read_sheet(team_sheet) 

# Get DF of all members with leader names
team_leaders <- readr::read_csv(
  here::here("data", "questionnaires", "msa_intake_form.csv")) %>% 
  select(ResponseId, Q6) %>% # Questions asking for team leader
  filter(!(row_number() %in% c(1, 2))) %>% 
  mutate(leader = Q6, 
         leader = case_when(
           Q6 %in% c("Ali A-Hoorie", "Ali Al-Hoorie", "Ali H. Al-Hoorie") ~ "Ali H. Al-Hoorie", 
           Q6 %in% c("Alicea Megan Brown", "Alicia Brown") ~ "Alicia M Brown", 
           Q6 %in% c("Dr Rory Turnbull", "Rory Turnbull") ~ "Rory Turnbull", 
           Q6 %in% c("Dr. Reenu Punnoose", "Reenu Punnoose") ~ "Reenu Punnoose", 
           Q6 %in% c("Dušan Nickolić", "Dušan Nikolić", "Dusan Nikolic", "Dusan Nikolic (dusan.nikolic1@ucalgary.ca)") ~ "Dušan Nikolić", 
           Q6 %in% c("Michael Gradoville (Arizona State Univ.)", "Michael Gradoville") ~ "Michael Gradoville", 
           Q6 %in% c("Nicole Rodriquez", "Nicole Rodriguez") ~ "Nicole Rodriguez", 
           Q6 %in% c("Scott James Perru", "Scott Perry") ~ "Scott James Perry", 
           Q6 %in% c("Tiphaine Caudrelier", "Tiphaine Cordelier") ~ "Tiphaine Caudrelier", 
           Q6 %in% c("University of Birmingham team under the team coordinator Zlatomira IIchovska", "Zlatomira IIchovska", "Zlatomira Ilchovska") ~ "Zlatomira Ilchovska", 
           Q6 %in% c("Vincent Hughes", "Vince Hughes") ~ "Vince Hughes", 
           Q6 == "Joey Stanley" ~ "Joseph A. Stanley",
           TRUE ~ leader
         )) %>% 
  select(ResponseId, leader) 

# Teams on OSF
teams_on_osf <- teams %>% 
  filter(added_on_osf == "yes") %>% 
  pull(coordinator)

# Teams with submitted analyses
teams_used_in_analysis <- teams %>% 
  filter(animal %in% msa_models$animal) %>% 
  pull(coordinator)

# Team descriptives
#  - number of teams (initially)
#  - Avg size of each team
team_desc_df <- bind_rows(
  team_leaders %>% 
    group_by(leader) %>% 
    mutate(member_num = seq_along(ResponseId)) %>% 
    group_by(leader) %>% 
    mutate(n_members = max(member_num)) %>% 
    distinct(leader, n_members) %>% 
    ungroup() %>% 
    summarize(n_teams = nrow(.), 
              mean_size = mean(n_members), 
              sd_size = sd(n_members), 
              median_size = median(n_members)) %>% 
    mutate(stage = "Initial"), 
  team_leaders %>% 
    filter(leader %in% teams_on_osf) %>% 
    group_by(leader) %>% 
    mutate(member_num = seq_along(ResponseId)) %>% 
    group_by(leader) %>% 
    mutate(n_members = max(member_num)) %>% 
    distinct(leader, n_members) %>% 
    ungroup() %>% 
    summarize(n_teams = nrow(.), 
              mean_size = mean(n_members), 
              sd_size = sd(n_members), 
              median_size = median(n_members)) %>% 
    mutate(stage = "Submitted"), 
  team_leaders %>% 
    filter(leader %in% teams_used_in_analysis) %>% 
    group_by(leader) %>% 
    mutate(member_num = seq_along(ResponseId)) %>% 
    group_by(leader) %>% 
    mutate(n_members = max(member_num)) %>% 
    distinct(leader, n_members) %>% 
    ungroup() %>% 
    summarize(n_teams = nrow(.), 
              mean_size = mean(n_members), 
              sd_size = sd(n_members), 
              median_size = median(n_members)) %>% 
    mutate(stage = "Final")
  ) %>% 
  mutate_if(is.numeric, round, digits = 2)


team_desc_list <- team_desc_df %>% 
  tidyr::pivot_longer(cols = n_teams:median_size, names_to = "metric", values_to = "val") %>% 
  split_to_list(stage, metric)

# Example usage
# team_desc_list$Initial$n_teams$val

# number of analyses
no_analyses = nrow(msa_models)

# acoustic operationalizations
# Gross operationalization of acoustic operationalization
unique_acou_oper <- msa_models %>%
  unite("unique_oper", outcome, operationalisation, operationalisation_notes, na.rm = TRUE) %>%
  count(unique_oper)

# average analyses
mean_analyses <- msa_models %>% 
  group_by(team) %>% 
  summarise(max_models = max(model_n)) %>% 
  group_by() %>% 
  summarise(mean_models = mean(max_models))

# outcomes
outcome_n <- msa_models %>% 
  count(outcome) %>% 
  summarise(prop = n / sum(n))

# windows
window_n <- msa_models %>% 
  count(temporal_window) %>% 
  summarise(prop = n / sum(n))

# predictors
predictor_n <- msa_models %>% 
  summarise(mean_predictor = mean(n_predictors, na.rm = TRUE)) 

# typicality op
typicality_n <- msa_models %>% 
  count(typicality_operationalization) %>% 
  summarise(prop = n / sum(n))

# framework
framework_n <- msa_models %>% 
  count(framework) %>% 
  summarise(prop = n / sum(n))

# model
model_n <- msa_models %>% 
  count(model) %>% 
  summarise(prop = n / sum(n))

# random
random_n <- msa_models %>% 
  filter(model == "linear model") %>% 
  count(random_effects) %>% 
  summarise(prop = n / sum(n))

# random terms
rterms_n <- msa_models %>% 
  filter(n_random != 0) %>% 
  summarise(mean_rterms = mean(n_random, na.rm = TRUE)) 

# unique model specs, discarding random effects
unique_model_specs <- msa_models %>%
  select(framework:outcome, pop_pred) %>%
  unnest(pop_pred) %>%
  distinct()

# review ratings
review_mean <- msa_models %>% 
  summarise(mean_prating = mean(phon_rating, na.rm = TRUE),
            sd_prating = sd(phon_rating, na.rm = TRUE),
            mean_srating = mean(stat_rating, na.rm = TRUE),
            sd_srating = sd(stat_rating, na.rm = TRUE)) 


```

# Sample characteristics

In the following, we will describe the characteristics of our analyst teams and their models in order to give an overview over the sample and the large variation across analysis strategies.  

<!-- TR: some of this or most of this should go into a nice overview table -->

## Team properties

`r team_desc_list$Initial$n_teams$val %>% xfun::numbers_to_words() %>% stringr::str_to_title()` teams initially signed up to participate. 
`r (team_desc_list$Initial$n_teams$val - team_desc_list$Final$n_teams$val) %>% xfun::numbers_to_words() %>% stringr::str_to_title()` of the proposed teams dropped out of the project during the analysis phase^[ADD REASONS]. 
The remaining `r team_desc_list$Final$n_teams$val` teams submitted their analysis in time and were coded by the three initiating authors (SC, JC, TR). 

Submitting teams consisted of an average of `r team_desc_list$Final$mean_size$val` analysts (sd = `r team_desc_list$Final$sd_size$val`). 
Analysts had on average `r round(phd_mean[[1]],1)` years (sd = `r round(phd_mean[[2]],1)`) post PhD experience, ranging from `r round(phd_mean[[3]],1)` years, i.e. PhD students (or younger) to `r round(phd_mean[[4]],1)` years.


```{r}
#| label: team-descriptives-table
#| echo: false
#| eval: false
team_desc_df %>% 
  mutate(Mean = glue::glue("{mean_size} ({sd_size})")) %>% 
  select(Stage = stage, n = n_teams, Mean, Median = median_size) %>% 
  knitr::kable(align = c("l", "r", "r", "r"), 
               caption = "Caption test", 
               label = "team-descriptives-table")
```


Analysts prior belief in the effect under investigation ranged from `r round(prior_mean[[3]],1)`  to `r round(prior_mean[[4]],1)` with an average of `r round(prior_mean[[1]],1)` (sd = `r round(prior_mean[[2]],1)`), indicating that analysts had an overall rather high prior plausibility of the investigated relationship between prosody and typicality (scale ranged from 0 to 100).  

The teams have submitted `r no_analyses` models to answer the research question. 
On average, teams submitted `r round(mean_analyses[[1]], 1)` models. 
Crucially, analytic approaches widely varied in how they acoustically analysed the speech signal and how the statistically analysed the extracted values.


## Acoustic analysis 
Teams differed in what and how they measured the acoustic signal, including choosing different aspects of the acoustic signal, the temporal window over which they measured, and the concrete operationalization of how they measured those acoustic properties.
`r round(outcome_n[[1,1]],2) * 100`\% of models used a duration measure as the outcome variable, `r round(outcome_n[[2,1]],2) * 100`\% used a f0 measure, `r round(outcome_n[[3,1]],2) * 100`\% used a formant measure, `r round(outcome_n[[4,1]],2) * 100`\% used an intensity measure, and `r round(outcome_n[[5,1]],2) * 100`\% used a different measure. <!-- TR: Do we want to list some of them?-->

`r round(window_n[[1,1]],2) * 100`\% of models measured acoustic properties at the level of the segment (e.g. comparing the acoustic profile of a vowel), `r round(window_n[[2,1]],2) * 100`\% at the level of the word (e.g. comparing the acoustic profile of "banana"), `r round(window_n[[3,1]],2) * 100`\% at the level of the phrase (e.g. the noun phrase including determiner and adjective, e.g. "the green banana"), `r round(window_n[[4,1]],2) * 100`\% at the level of the whole sentence, and `r round(window_n[[5,1]],2) * 100`\% used a different time window. Based on a coarse coding of how acoustic measures were operationalized, we find a total of `r nrow(unique_acou_oper)` different measurement specifications.

<!-- SC: I have added the number of unique meas. specs but we might want to discuss this. -->
<!-- TR: How did you do this?-->

## Statistical analysis

The multiverse of acoustic measurement choices is exponentiated with choices during the statistical analysis, including the predictors and their operationalization, the chosen inferential framework, the type of model, and their model architecture, specifically how they accounted for dependencies in the data via random effect specifications in their multilevel models.

On average, models included `r round(predictor_n,1)` different predictors (counting only conceptually different predictors), i.e. in addition to the critical predictor, typicality of the adjective noun combinations, most teams included additional predictors in their models. Predictors included the information structure of the sentence, trial number, semantic dimensions of the referent, part of speech, or gender of the speaker.

The original dataset allowed to operationalize the most relevant predictor, typicality, in different ways. 
It was operationalized as categorical (e.g. typical vs. atypical) in `r round(typicality_n[[1,1]],2) * 100`\% of models, continuous (on a scale from 0-100) with the mean typicality rating in `r round(typicality_n[[2,1]],2) * 100`\% of models, and continuous with the median typicality rating in `r round(typicality_n[[3,1]],2) * 100`\% of models.

The majority of models were rooted in the frequentist framework (`r round(framework_n[[2,1]],2) * 100`\%). `r round(framework_n[[1,1]],2) * 100`\% were operating in a Bayesian framework. 
While teams almost exclusively used linear models to analyze their data (`r round(model_n[[2,1]],2) * 100`\%, with a few exception using machine learning techniques or GAMs as a special case of a linear model), teams differed drastically in how they accounted for dependencies within the data. 
The data contains several dependencies between data points, with multiple data points coming from the same subject and with multiple data points being associated with the same adjective or noun. 
The appropriate way to account for this non-independence is by using multi-level models and specifying so-called random effects [e.g., @gelman2006data; @schielzeth2009conclusions]. 
`r round(random_n[[3,1]],2) * 100`\% of linear models specified no random effects at all (without pooling their data), so effectively ignoring these non-independences [@hurlbert1984pseudoreplication]. <!-- TODO: We need to check if these model pooled -->
`r round(random_n[[1,1]],2) * 100`\% specified random intercepts only, and `r round(random_n[[2,1]],2) * 100`\% specified random intercepts and random slopes to account for the non-independence.
On average, teams that specified random effects, included `r round(rterms_n,1)` random terms in their models. 
Based on statistical framework, type of model, distribution family, fixed terms, and discarding random effects, there were a total of `r nrow(unique_model_specs)` different model specifications. 
<!-- TR: If done on team level did every team have a different mdoel?  -->

## Review ratings

Teams reviewed each others' analyses for both the acoustic analysis and the statistical analysis. 
The mean rating of the quality of the acoustic analyses was `r round(review_mean[[1]],1)` (sd = `r round(review_mean[[2]],1)`); the mean rating of the quality of the statistical analysis was `r round(review_mean[[3]],1)` (sd = `r round(review_mean[[4]],1)`). For reference, the scale of the rating anchored a point grade of 75 as "an imperfect analysis but the needed changes are unlikely to dramatically alter final interpretation", indicating a rather high confidence of reviewers that the provided analyses yield appropriate (yet "imperfect") answers to the research question. 

# Meta analysis

```{r load-meta-analysis}
meta_bm <- readRDS(here("./data/meta_analysis/meta_bm.rds"))

meta_intercept <- fixef(meta_bm)
meta_random <- ranef(meta_bm)

# wrangle by hand 
post_meta <- meta_bm %>%
  spread_draws(b_Intercept, r_model_id[model,]) %>%
  mutate(model_mean = b_Intercept + r_model_id) %>% 
  group_by(model) %>% 
  dplyr::summarise(post_mean = mean(model_mean),
            lower95 = quantile(model_mean, probs = .025),
            higher95 = quantile(model_mean, probs = .975)
            ) %>% 
  # this basically needs to be fed by whether or not the authors considered there to be an effect
  mutate(compelling = as.factor(case_when(lower95 > 0 ~ "negative",
                                          higher95 < 0 ~ "positive",
                                          TRUE ~ "not compelling"))) 

compelling_results <- post_meta %>% 
  count(compelling) %>% 
  summarise(prop = n / sum(n))

compelling_count <- table(post_meta$compelling)
```

## Between-team variability

<!-- TODO: discuss between-team variability -->
<!-- TODO: This is more like between-model variability -->

As explained in the **Methods** section, the primary aim of this is to assess the variability of the reported effects or, in other words, the degree of between-team variability.
As a measure of between-team variability, we chose to use the meta-analytic group-level standard deviation $\sigma_{\alpha_{\text{t}}}$), i.e. the standard deviation of the group-level effect of team `(!!! we have now model_id)` returned by the meta-analytic model.

According to the meta-analytic model, the group-level standard deviation is between 0.11 and 0.15 standard units, at 95% credibility.
This means that the deviations of any individual model from the meta-analytic effect estimate range between ±0.22 to ±0.3 (0.11 * 2, 0.15 * 2) in standard units.
We illustrate what this means in actual acoustic measures by means of examples, taking the mean sample standard deviation of each measure from the sample data.

<!-- TODO: Add table with different outcomes, means (?) and SDs plus estimated deviations based on ±0.22/0.33 SDs. -->

## Estimating the crowd sourced effect size

The Bayesian random effects model estimates the range of probable values of the standardized effect size between `r round(meta_intercept[3],3)` and `r round(meta_intercept[4],3)` (95\% CrI, mean = `r round(meta_intercept[1],3)`)
In other words, if we were to assume that there is a true underlying effect of typicality and we consider each analysis randomly drawn from a population of possible analysis attempts, our best guess about the underlying effect is that it is `r round(meta_intercept[1],2)` standard deviations above zero. This outcome thus estimates that atypical word combinations have `r round(meta_intercept[1],2)` standard deviations higher acoustic values (e.g. duration, f0 etc,) than typical word combinations. 

<!-- TR: CONTEXTUALIZE IN CONCRETE ACOUSTIC VALUES (ALTERNATIVELY IN IQ VALUES).--> 

This is not only an extremely small effect <!-- TR: cannot really talk about tiny effects because we have the continuous typicality included and that looks at a difference of one unit (which is 1 in a 1-100 scale -->, but there is also much uncertainty around this estimate and the range of probably values includes zero. 
Thus, given the data, the model, and our prior assumptions, we cannot be very certain that there actually is an effect that is not zero to begin with. 
Since we do not know the true value of the underlying effect, we cannot conclude anything from this finding, but if there is an effect of typicality, it is very small.
Moreover, this population estimate is half the size of the estimated standard deviation between different models <!-- TR: EXTRACT BETA AND CRIS).--> 

(ref:meta_plot1) illustrates the estimated model outputs for all submitted models according to size. 
Given the nature and wide variety of acoustic operationalizations, there is not always a natural interpretation of the scale, but in most cases a positive effect corresponds to atypical word combinations eliciting higher acoustic values (e.g. longer duration, higher f0, etc.).
Notably, while the majority of models yielded inconclusive results, there are `r compelling_count[[1]] + compelling_count[[3]]` model estimates for which the 95\% credible interval does not contain zero (`r round((compelling_results[[1,1]] + compelling_results[[3,1]]), 2) * 100`\%).

<!-- TR: ADD PARAGRAPH ON HOW MANY CLAIMED THERE TO BE AN EFFECT.--> 

```{r 'plot_meta1', echo = FALSE, fig.cap="(ref:meta_plot1)", out.width="100%"}

#knitr::include_graphics(here("./plots/meta_plot1.png"))

```

## Can we predict the estimate?
After assessing the variability across models, we now turn toward estimating the impact of a series of predictors on the analysts' model estimates.
There is a lot of variation, raising the question as to whether we can explain some of this variation or whether it is idiosyncratic [@breznau2021observing]?

(ref:meta_plot1), panel C, displays the coefficients alongside 80% and 95% credible intervals for all model predictors. 

<!-- TR: ADD TABLE HERE TOO?--> 

The model suggests that most team-specific predictors yield very small deviations from the meta-analytical estimate and their 95% credible intervals include zero, leaving us highly uncertain about their direction. 
Neither analysts' prior beliefs in the phenomenon (ADD EST + 95CrI), nor their seniority in terms of years after completing their PhD (ADD EST + 95CrI) seem to compellingly affect model estimates.
Similarly, the evaluation of the quality of the analysis from their peers yielded a rather small effect magnitude, again characterized by large uncertainty (ADD EST + 95CrI). 
Interestingly, the model uniqueness, i.e. how unique the choice and combination of predictors is, affects the analysts estimate, with more unique models producing higher positive estimates. 
This suggests that the analysts' results very much depend on their choice of predictors. 
Looking at the most important choices during measurement, both the acoustic parameter under investigation (e.g. duration or f0) and the choice of temporal window affected the results. 
(ref:meta_plot1) displays the posterior estimates for the measurement outcome (i.e. what acoustic dimension was measured, panel A) and measurement window (i.e. what is the unit over which was measured, panel B). 
If an acoustic dimension related to f0 was measured, estimates are lower than the meta analytic estimate.
If on the other hand, vowel formants were measured, estimates are higher than the meta analytic estimate.
Similarly, if acoustic parameters were measured within individual segments (e.g. the vowel of the noun), estimates are lower than the meta analytic estimate, and if acoustic parameters were measured across whole phrases (e.g. "the blue banana"), the estimates are generally higher.
In other words, depending on the choice of measurement, analysts arrived at opposite conclusions about how and if typicality is expressed acoustically. 

```{r 'plot_meta2', echo = FALSE, fig.cap="(ref:meta_plot1)", out.width="100%"}

#knitr::include_graphics(here("./plots/alltogether.png"))

```

# Discussion drafty

## Summary

We gave XX analyst teams the same speech dataset to answer the same research question: *Do speakers acoustically modify utterances to signal atypical word combinations?*.
In order to answer this question, teams had to operationalize latent variables within a multidimensional signal, operationalize and chose appropriate predictors, and construct an appropriate statistical model to answer the research question.
As shown by the results of this meta-analytic study, such complex process has led to a wide "garden of forking paths", i.e. to a wide range of combinations of possible analytical decisions.
Every individual analyst team has chosen unique paths to acoustically measure, operationalize and statistical analyse the data.
Interestingly, the observed variation in reported effect sizes was not predicted by the analysts' prior expectations about the phenomenon.
In fact, teams on average rated the plausibility of the effect as rather high before receiving access to the data.
The variation in reported effect sizes was neither predicted by the analysts' experience in the field nor by the perceived quality of the analysis as judged by other teams. 
Analyses received overall high peer ratings for both the acoustic and the statistical analysis, suggesting that reviewers were generally satisfied with the other teams' approaches.
These findings are very much in line with previous crowd-sourced projects that suggest variation between teams is neither driven by perceived quality of the analysis nor by analysts biases or experience [e.g., @silberzahn2018many, @breznau2021observing].
Given the mounting evidence, @breznau2021observing conclude that "[...] we are left to believe that idiosyncratic uncertainty is a fundamental feature of the scientific process that is not easily explained by typically observed researcher characteristics or analytic decisions."
Idiosyncratic variation across researchers might be a fact of life which researchers have to acknowledge and integrate into how they evaluate and present evidence.

While properties of the teams did not seem to systematically affect results, teams' estimates seem to depend on certain measurement choices. 
Human speech is a complex multidimensional signal.
Researchers need to make choices about what to measure, how to measure it and which temporal unit to measure it in. 
Some outcome choices seem to bias the estimates in our data in one direction while others seem to bias estimates into another. 
For example, measurements related to fundamental frequency tended to result in lower estimates while measurements related to vowel formants tended to yield higher estimates.
This asymmetry can have several causes.
First, there could be a true underlying relationship between typicality and the speech signal that manifests itself in some measures but not others and/or manifests itself negatively in one acoustic measure but positively in another.
Second, certain measurement choices might be associated with stronger expectations relative to the research question, which might lead to strong biases.
Many researchers targeted measures related to voice fundamental frequency (f0) since similar functional relationships like information structure and predictability can be expressed via f0 [e.g. @grice2017integrating; @turnbull2017role].
Third, ADD REASONS. 
Regardless of its cause, we have to conclude that depending on the choice of how the speech signal is operationalized, researchers might find evidence for or against a theoretically relevant effect.

It particularly struck us that teams did not follow our instructions to only submit a single effect size. 
Teams submitted up to 16 different models to test for a possible relationship between typicality and the speech signal.
Obviously, the complexity of the speech signal lends itself to multiple approaches, but this plurality of hypothesis tests invites bias and can dramatically increase the rate of falsely claiming the presence of an effect [@roettger2019researcher]. 
When operating within the frequentist inferential framework, testing the same hypothesis with different dependent variables, increases the Type-I error rate if not corrected for (Tukey 1954, Benjamini & Hochberg 1995). 
It also invites bias, leading to the selective reporting of those tests that yield a desirable outcome (Kerr 1998, John et al. 2012, Simmon et al. 2011) while null results remain unreported (Sterling 1959, REF). 
Our data suggest that fields that use highly complex raw data such as speech should be particularly cautious when analyzing their data. 

## Lessons for the methodological reform movement

The current results point to important barriers for successful accumulation of knowledge.
The replication crisis has brought attention to scientific practices that lead to unreliable and biased claims in the literature (REFS).
One of the suggested paths forward is for researchers to directly replicate previous study more often [@open2015estimating; REFS, REFS].
While we agree with the importance of direct replications, our study (and similar crowd-sourced analyses before us) suggest that replicating more is simply not enough.
There is only limited value in learning that a particular procedure is replicable if the idiosyncratic nature of the procedure itself might not yield a representative result relative to all possible procedures that could have been applied to the research question.
Well-trained and experienced speech researchers in this study not only applied completely different approaches to the same research question, they also seemed to consider all these alternative approaches acceptable, as the peer-ratings suggest.
Being aware of this idiosyncratic variation between analysts should lead to more nuanced claims and what Breznau et al. refers to as "epistemic humility". 

A desired outcome of knowing that different but reasonable measurement choices or statistical approaches might lead to entirely different interpretations of research data is to calibrate our (un)certainty in the strength of the collected evidence and, in turn, communicate that (un)certainty appropriately.
The fact that the choice of measurement, measurement window, and predictor choice affect the answer to the research question further suggests that research assumptions and hypotheses should be formulated with much greater detail, particularly so in regards to how measurement systems (here, the acoustic signal) and underlying conceptual constructs (here, the phonetic expression of typicality) relate to each other.
<!-- TODO: add ref to "process and statistical model" in McElreath -->
We should ideally specify the link between conceptual construct and quantitative system---the "derivation chain" [@dubin1970theory; @meehl1990summaries]---prior to data collection and analysis, including defining constructs and their relationship within the quantitative system, specifying auxiliary assumptions and boundary conditions, and defining target measurements, statistical expectations and possible (and impossible) effect magnitudes.
Without well defined derivation chains, we "are not even wrong" [@scheel2022most] because falsified expectations cannot tell us much about the conceptual constructs they are based on when the relationship between the two is underspecified.

In light of the observed analytic flexibility, what can we do to appropriately calibrate our confidence in our claims?
First of all, through sharing of materials, data and statistical protocols, we can make our idiosyncratic choices transparent to others (REF).
This enables critical evaluation and robustness checks by other fellow researchers (REF).
Given that minor procedural changes can sometimes drastically affect the final interpretation of the results [@breznau2021observing], we should ideally share a detailed documentation of the data collection procedure, the measurement choices, the data extraction, and statistical analyses.
<!-- TODO: cite Praat, EMU, MFA, Formant tracking etc -->
Within fields that deal with speech data, open source software that allows to extract acoustic parameters via reproducible scripts can help other researchers to trace back seemingly inconsequential choices during the measurement process [e.g., CITE Praat, EMU CITE)]].

ADD

Second, making analytic pathways completely re-traceable does not change the fact that analysts apply different analytic approaches.
Crowd-source projects such as the current one can shed light on the range of degrees of freedom during analysis and help producing a consensual estimated effect using meta-analytic techniques.
This is obviously not always feasible in terms of required resources and time, but could be a consideration for claims that have large epistemological or practical consequences.

Third, if researchers have a good understanding of relevant analytic degrees of freedom, they could apply all conceivable analytic strategies and compare the results across all combinations of these choices.
This approach is called "multiverse analysis" [e.g, @steegen2016increasing; @harder2020multiverse] and has recently gained popularity across disciplines.
Finally, neither crowd-sourcing nor multiverse analyses will guarantee that all relevant pathways are explored. 
Crowd-sourcing is limited by the sampled analysts and their biases.
Multiverse analysis is limited even further by the one group of researchers who define possible analytic pathways.
Eventually, a mature scientific discipline should develop a set of detailed quantitative hypotheses of how conceptual constructs manifest themselves in the measured system, i.e. in the present case how communicative pressures of functions are expressed in the acoustic signal.
More recently, researchers across the cognitive sciences have also raised awareness of a general lack of detailed quantitative hypotheses, calling for establishing appropriate derivation chains and formalizing verbal expectations mathematically [e.g., @van2020formalizing; @guest2021computational; @scheel2021hypothesis; @devezer2021case].
The presented data clearly support a call of improving the link between our theoretical constructs and our quantitative predictions.  

## Caveats

Our study has several limitations that need to be considered when evaluating our results. 
First of all, our sample is an opportunity sample. 
We have advertised the project through online platforms which might have led to the exclusion of certain potential researcher groups.
Moreover, while the number of participating teams is larger than most earlier crowdsourcing projects, it is likely to be too small to estimate meta-analytic estimates reliably.
This is particularly important for the predictor evaluation.
Since predictor levels were not systematically distributed across teams, our estimates are characterized by large uncertainty. 

ADD

<!-- Brainstorm of ideas -->

- Analytic flexibility applies to us (as principal investigators) too.
- Research question was vague enough to possibily inflate between-team variability (REF, Hannah Fraser; although it is a typical kind of question, as found in the literature). We do relate analytical flexibility with underspecified hypothesis in the previous section.
- This should be replicated with a different question/data set.
- With this study, we can suggest that at a minimum this is the between-team variability, but need more studies like this one.
- Generalisability to linguistics/language sciences tout-court is limited. Need to do same with other fields, like language development, historical data, sociolinguistics, etc.
- The focus is on quantitative data analysis, so that we expect the issues raised here to apply differently or not at all to qualitative data analysis.
- We have not investigated researcher positionality, which is known to be an important factor.
- Meta-analytic model has model_id instead of team, so we are not accounting for the fact that teams have submitted multiple analyses (but not all of them did).

# Conclusion




